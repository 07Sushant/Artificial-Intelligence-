{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train JAX/Flax model on Vertex AI custom container and use `jax2tf` to convert to SavedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "v-kebtVqdEyV"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import aiplatform_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "W-IAEp1UqmKR"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "\n",
    "REGION = \"us-central1\"\n",
    "os.environ['REGION'] = REGION\n",
    "\n",
    "BUCKET_NAME = \"dsparing-sandbox-bucket\"\n",
    "os.environ['BUCKET_NAME'] = BUCKET_NAME\n",
    "# Use a regional bucket in the above region you have rights to.\n",
    "# Create if needed:\n",
    "# !gsutil mb -l ${REGION} gs://${BUCKET_NAME}\n",
    "\n",
    "TRAINING_APP_FOLDER = 'training_app'\n",
    "os.environ['TRAINING_APP_FOLDER'] = TRAINING_APP_FOLDER\n",
    "\n",
    "MODEL_NAME = \"jax_model_customcontainer\"\n",
    "SAVEDMODEL_BASEDIR = f\"gs://{BUCKET_NAME}/models/{MODEL_NAME}/output\"\n",
    "os.environ['SAVEDMODEL_BASEDIR'] = SAVEDMODEL_BASEDIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p $TRAINING_APP_FOLDER/trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting training_app/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/trainer/task.py\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from absl import flags\n",
    "from jax.experimental.jax2tf.examples.mnist_lib import (\n",
    "    load_mnist, FlaxMNIST\n",
    ")\n",
    "from jax.experimental.jax2tf.examples.saved_model_lib import (\n",
    "    convert_and_save_model\n",
    ")\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        help=\"GCS location to export SavedModel\",\n",
    "        default=os.getenv(\"AIP_MODEL_DIR\")\n",
    "    )\n",
    "    args = parser.parse_args().__dict__\n",
    "\n",
    "    # need to initialize flags somehow to avoid errors in load_mnist\n",
    "    flags.FLAGS(['e'])\n",
    "\n",
    "    train_batch_size = 128\n",
    "    test_batch_size = 16\n",
    "\n",
    "    flax_mnist = FlaxMNIST()\n",
    "\n",
    "    train_ds = load_mnist(tfds.Split.TRAIN, train_batch_size)\n",
    "    test_ds = load_mnist(tfds.Split.TEST, test_batch_size)\n",
    "\n",
    "    predict_fn, params = flax_mnist.train(\n",
    "        train_ds=train_ds,\n",
    "        test_ds=test_ds,\n",
    "        num_epochs=2,\n",
    "    )\n",
    "\n",
    "    image, _ = next(iter(train_ds))\n",
    "    input_signature = tf.TensorSpec.from_tensor(\n",
    "        tf.expand_dims(image[0], axis=0)\n",
    "    )\n",
    "\n",
    "    convert_and_save_model(\n",
    "        jax_fn=predict_fn,\n",
    "        params=params,\n",
    "        model_dir=args[\"output_dir\"],\n",
    "        input_signatures=[input_signature],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should be able to use [CustomContainerTrainingJob](https://googleapis.dev/python/aiplatform/latest/aiplatform.html#google.cloud.aiplatform.CustomContainerTrainingJob), but it gives an error, see the similar [CustomTrainingJob.run](https://googleapis.dev/python/aiplatform/latest/aiplatform.html#google.cloud.aiplatform.CustomTrainingJob.run) currently giving an error even when using the [official notebook](https://github.com/GoogleCloudPlatform/ai-platform-samples/blob/master/ai-platform-unified/notebooks/official/custom/sdk-custom-image-classification-online.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MACHINE_TYPE = 'n1-standard-4'\n",
    "REPLICA_COUNT = 1\n",
    "JOB_NAME = 'jax_customcontainer_training'\n",
    "\n",
    "USE_GPU = True\n",
    "if USE_GPU:\n",
    "    TRAINING_IMAGE = 'gcr.io/deeplearning-platform-release/tf2-gpu.2-5'\n",
    "    ACCELERATOR_TYPE = \"NVIDIA_TESLA_T4\"\n",
    "    ACCELERATOR_COUNT = 1\n",
    "    IMAGE_NAME = 'jax_vertex_image_gpu'\n",
    "else:\n",
    "    TRAINING_IMAGE = 'gcr.io/deeplearning-platform-release/tf2-cpu.2-5'\n",
    "    ACCELERATOR_TYPE = None\n",
    "    ACCELERATOR_COUNT = None\n",
    "    IMAGE_NAME = 'jax_vertex_image_cpu'\n",
    "\n",
    "os.environ['TRAINING_IMAGE'] = TRAINING_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting training_app/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/requirements.txt\n",
    "flax\n",
    "jax[cuda111]  # needs pip to run with `-f https://storage.googleapis.com/jax-releases/jax_releases.html`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 458,
     "status": "ok",
     "timestamp": 1617197650331,
     "user": {
      "displayName": "Daniel Sparing",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GimB4WP4BC_SgKVqyvIS8u7WFJXnJGMSuyFNS4Lww=s64",
      "userId": "02797666024167492243"
     },
     "user_tz": 300
    },
    "id": "WlzDz-6wdpNC",
    "outputId": "9404061c-e26c-42e3-9b2a-bce48767be1f",
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo > $TRAINING_APP_FOLDER/Dockerfile \"FROM $TRAINING_IMAGE\n",
    "\n",
    "COPY requirements.txt ./\n",
    "RUN python3 -m pip install --no-cache-dir -r requirements.txt -f https://storage.googleapis.com/jax-releases/jax_releases.html \n",
    "\n",
    "WORKDIR /app\n",
    "COPY trainer/task.py .\n",
    "\n",
    "ENTRYPOINT [\\\"python\\\", \\\"task.py\\\"]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "FZ7Eh2Wwfc9Y"
   },
   "outputs": [],
   "source": [
    "IMAGE_TAG = 'latest'\n",
    "IMAGE_URI = 'gcr.io/{}/{}:{}'.format(PROJECT_ID, IMAGE_NAME, IMAGE_TAG)\n",
    "os.environ['IMAGE_URI'] = IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1184,
     "status": "ok",
     "timestamp": 1617197720314,
     "user": {
      "displayName": "Daniel Sparing",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GimB4WP4BC_SgKVqyvIS8u7WFJXnJGMSuyFNS4Lww=s64",
      "userId": "02797666024167492243"
     },
     "user_tz": 300
    },
    "id": "JhHx_WUIfs6j",
    "outputId": "418935e1-58c0-4812-f820-c09244711048"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  37.38kB\n",
      "Step 1/6 : FROM gcr.io/deeplearning-platform-release/tf2-gpu.2-5\n",
      " ---> 0f998c784cd6\n",
      "Step 2/6 : COPY requirements.txt ./\n",
      " ---> Using cache\n",
      " ---> ffa2eb0f16a3\n",
      "Step 3/6 : RUN python3 -m pip install --no-cache-dir -r requirements.txt -f https://storage.googleapis.com/jax-releases/jax_releases.html\n",
      " ---> Using cache\n",
      " ---> 0403d96c9d9d\n",
      "Step 4/6 : WORKDIR /app\n",
      " ---> Using cache\n",
      " ---> d734275a2c36\n",
      "Step 5/6 : COPY trainer/task.py .\n",
      " ---> Using cache\n",
      " ---> 679ee46226c9\n",
      "Step 6/6 : ENTRYPOINT [\"python\", \"task.py\"]\n",
      " ---> Using cache\n",
      " ---> 5d45e431f3c4\n",
      "Successfully built 5d45e431f3c4\n",
      "Successfully tagged gcr.io/dsparing-sandbox/jax_vertex_image_gpu:latest\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker build -f $TRAINING_APP_FOLDER/Dockerfile \\\n",
    "--tag $IMAGE_URI $TRAINING_APP_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional local test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "executionInfo": {
     "elapsed": 367,
     "status": "error",
     "timestamp": 1617776619162,
     "user": {
      "displayName": "Daniel Sparing",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GimB4WP4BC_SgKVqyvIS8u7WFJXnJGMSuyFNS4Lww=s64",
      "userId": "02797666024167492243"
     },
     "user_tz": 300
    },
    "id": "X0rCYgF6_63Y",
    "outputId": "8d917d99-39db-4ae9-cefc-ed5b6ad74289"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-26 18:37:27.582794: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "INFO:absl:Load pre-computed DatasetInfo (eg: splits, num examples,...) from GCS: mnist/3.0.1\n",
      "INFO:absl:Load dataset info from /tmp/tmpyl1i15qytfds\n",
      "INFO:absl:Field info.citation from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.splits from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.module_name from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Generating dataset mnist (/root/tensorflow_datasets/mnist/3.0.1)\n",
      "INFO:absl:Dataset mnist is hosted on GCS. It will automatically be downloaded to your\n",
      "local data directory. If you'd instead prefer to read directly from our public\n",
      "GCS bucket (recommended if you're running on GCP), you can instead pass\n",
      "`try_gcs=True` to `tfds.load` or set `data_dir=gs://tfds-data/datasets`.\n",
      "\n",
      "Dl Completed...: 100%|██████████| 4/4 [00:00<00:00, 11.84 file/s]\n",
      "INFO:absl:Load dataset info from /root/tensorflow_datasets/mnist/3.0.1.incompleteX4NHJH\n",
      "INFO:absl:Field info.citation from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.splits from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.module_name from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Constructing tf.data.Dataset mnist for split train, from /root/tensorflow_datasets/mnist/3.0.1\n",
      "2021-06-26 18:37:32.241230: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2021-06-26 18:37:32.251741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-26 18:37:32.252639: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
      "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\n",
      "2021-06-26 18:37:32.252717: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-06-26 18:37:32.283449: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2021-06-26 18:37:32.283601: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2021-06-26 18:37:32.286103: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
      "2021-06-26 18:37:32.287063: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
      "2021-06-26 18:37:32.291863: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-06-26 18:37:32.293226: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
      "2021-06-26 18:37:32.293554: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
      "2021-06-26 18:37:32.293756: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-26 18:37:32.295108: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-26 18:37:32.296101: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
      "2021-06-26 18:37:32.296667: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-06-26 18:37:32.297196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-26 18:37:32.298029: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
      "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\n",
      "2021-06-26 18:37:32.298194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-26 18:37:32.299154: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-26 18:37:32.300028: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
      "2021-06-26 18:37:32.300103: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-06-26 18:37:32.892862: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-06-26 18:37:32.892924: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \n",
      "2021-06-26 18:37:32.892936: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \n",
      "2021-06-26 18:37:32.893273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-26 18:37:32.894103: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-26 18:37:32.894940: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-26 18:37:32.895613: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13566 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
      "INFO:absl:Load dataset info from /root/tensorflow_datasets/mnist/3.0.1\n",
      "INFO:absl:Field info.citation from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.splits from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.module_name from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Reusing dataset mnist (/root/tensorflow_datasets/mnist/3.0.1)\n",
      "INFO:absl:Constructing tf.data.Dataset mnist for split test, from /root/tensorflow_datasets/mnist/3.0.1\n",
      "INFO:absl:Starting the local TPU driver.\n",
      "INFO:absl:Unable to initialize backend 'tpu_driver': Not found: Unable to find driver in registry given worker: local://\n",
      "INFO:absl:Unable to initialize backend 'tpu': Invalid argument: TpuPlatform is not available.\n",
      "2021-06-26 18:38:01.244073: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-06-26 18:38:01.244741: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2299995000 Hz\n",
      "INFO:root:mnist_flax: Epoch 0 in 9.62 sec\n",
      "INFO:root:mnist_flax: Training set accuracy 88.33%\n",
      "INFO:root:mnist_flax: Test set accuracy 88.62%\n",
      "INFO:root:mnist_flax: Epoch 1 in 1.10 sec\n",
      "INFO:root:mnist_flax: Training set accuracy 90.98%\n",
      "INFO:root:mnist_flax: Test set accuracy 91.56%\n",
      "2021-06-26 18:38:18.269558: I tensorflow/stream_executor/cuda/cuda_driver.cc:789] failed to allocate 13.25G (14225768448 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2021-06-26 18:38:18.270770: I tensorflow/stream_executor/cuda/cuda_driver.cc:789] failed to allocate 11.92G (12803190784 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2021-06-26 18:38:18.271882: I tensorflow/stream_executor/cuda/cuda_driver.cc:789] failed to allocate 10.73G (11522871296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2021-06-26 18:38:18.273330: I tensorflow/stream_executor/cuda/cuda_driver.cc:789] failed to allocate 9.66G (10370583552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2021-06-26 18:38:18.274315: I tensorflow/stream_executor/cuda/cuda_driver.cc:789] failed to allocate 8.69G (9333524480 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2021-06-26 18:38:18.275275: I tensorflow/stream_executor/cuda/cuda_driver.cc:789] failed to allocate 7.82G (8400172032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2021-06-26 18:38:18.276525: I tensorflow/stream_executor/cuda/cuda_driver.cc:789] failed to allocate 7.04G (7560154624 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2021-06-26 18:38:18.277646: I tensorflow/stream_executor/cuda/cuda_driver.cc:789] failed to allocate 6.34G (6804139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2021-06-26 18:38:18.278796: I tensorflow/stream_executor/cuda/cuda_driver.cc:789] failed to allocate 5.70G (6123724800 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2021-06-26 18:38:18.279892: I tensorflow/stream_executor/cuda/cuda_driver.cc:789] failed to allocate 5.13G (5511352320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2021-06-26 18:38:18.280970: I tensorflow/stream_executor/cuda/cuda_driver.cc:789] failed to allocate 4.62G (4960217088 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2021-06-26 18:38:18.281950: I tensorflow/stream_executor/cuda/cuda_driver.cc:789] failed to allocate 4.16G (4464195072 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2021-06-26 18:38:18.282925: I tensorflow/stream_executor/cuda/cuda_driver.cc:789] failed to allocate 3.74G (4017775360 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2021-06-26 18:38:18.284002: I tensorflow/stream_executor/cuda/cuda_driver.cc:789] failed to allocate 3.37G (3615997696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2021-06-26 18:38:18.285011: I tensorflow/stream_executor/cuda/cuda_driver.cc:789] failed to allocate 3.03G (3254397952 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2021-06-26 18:38:18.286246: I tensorflow/stream_executor/cuda/cuda_driver.cc:789] failed to allocate 2.73G (2928957952 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2021-06-26 18:38:18.287394: I tensorflow/stream_executor/cuda/cuda_driver.cc:789] failed to allocate 2.45G (2636062208 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2021-06-26 18:38:18.288480: I tensorflow/stream_executor/cuda/cuda_driver.cc:789] failed to allocate 2.21G (2372455936 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2021-06-26 18:38:18.289512: I tensorflow/stream_executor/cuda/cuda_driver.cc:789] failed to allocate 1.99G (2135210240 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2021-06-26 18:38:18.290504: I tensorflow/stream_executor/cuda/cuda_driver.cc:789] failed to allocate 1.79G (1921689344 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2021-06-26 18:38:18.291572: I tensorflow/stream_executor/cuda/cuda_driver.cc:789] failed to allocate 1.61G (1729520384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2021-06-26 18:38:18.292518: I tensorflow/stream_executor/cuda/cuda_driver.cc:789] failed to allocate 1.45G (1556568320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2021-06-26 18:38:18.293512: I tensorflow/stream_executor/cuda/cuda_driver.cc:789] failed to allocate 1.30G (1400911616 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2021-06-26 18:38:18.294506: I tensorflow/stream_executor/cuda/cuda_driver.cc:789] failed to allocate 1.17G (1260820480 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2021-06-26 18:38:18.295555: I tensorflow/stream_executor/cuda/cuda_driver.cc:789] failed to allocate 1.06G (1134738432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2021-06-26 18:38:18.296597: I tensorflow/stream_executor/cuda/cuda_driver.cc:789] failed to allocate 973.95M (1021264640 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "WARNING:tensorflow:Mapping types may not work well with tf.nest. Prefer using MutableMapping for <class 'flax.core.frozen_dict.FrozenDict'>\n",
      "WARNING:tensorflow:Mapping types may not work well with tf.nest. Prefer using MutableMapping for <class 'flax.core.frozen_dict.FrozenDict'>\n",
      "INFO:tensorflow:Assets written to: gs://dsparing-sandbox-bucket/models/jax_model_customcontainer/output/localmodel/assets\n",
      "INFO:tensorflow:Assets written to: gs://dsparing-sandbox-bucket/models/jax_model_customcontainer/output/localmodel/assets\n",
      "\u001b[1mDownloading and preparing dataset 11.06 MiB (download: 11.06 MiB, generated: 21.00 MiB, total: 32.06 MiB) to /root/tensorflow_datasets/mnist/3.0.1...\u001b[0m\n",
      "\u001b[1mDataset mnist downloaded and prepared to /root/tensorflow_datasets/mnist/3.0.1. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!docker run --name training_jax --runtime nvidia $IMAGE_URI --output_dir=$SAVEDMODEL_BASEDIR/localmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "once the above container run finished:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_jax\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker rm -f training_jax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push image to registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The push refers to repository [gcr.io/dsparing-sandbox/jax_vertex_image_gpu]\n",
      "a88fb86cb67d: Preparing\n",
      "9d471c683180: Preparing\n",
      "2a77eb46b89c: Preparing\n",
      "2d3dc1ac256c: Preparing\n",
      "03dc40ebdbd6: Preparing\n",
      "f5d954d2bd94: Preparing\n",
      "db34a056d495: Preparing\n",
      "f9b1cb8c2687: Preparing\n",
      "e68443de6bca: Preparing\n",
      "88bb87a4088d: Preparing\n",
      "29bf522d97b4: Preparing\n",
      "d96c519f0898: Preparing\n",
      "ff0a6aeeabc0: Preparing\n",
      "8a7cebfdebb3: Preparing\n",
      "61546b863e43: Preparing\n",
      "d2b843fb2f7a: Preparing\n",
      "36c9a9d68143: Preparing\n",
      "730e84ac5c5d: Preparing\n",
      "a25ae1798c0c: Preparing\n",
      "37a19de06c8b: Preparing\n",
      "27c459f353b4: Preparing\n",
      "25d03c11e857: Preparing\n",
      "d5585264beff: Preparing\n",
      "e39414beba01: Preparing\n",
      "ff6af85bc8aa: Preparing\n",
      "98cedd6c9734: Preparing\n",
      "574aa732d388: Preparing\n",
      "686978e3bf48: Preparing\n",
      "80088b120579: Preparing\n",
      "79a187e0621d: Preparing\n",
      "72657ad6008c: Preparing\n",
      "22d4dd8ed907: Preparing\n",
      "d7e7872b888e: Preparing\n",
      "5f08512fd434: Preparing\n",
      "c7bb31fc0e08: Preparing\n",
      "50858308da3d: Preparing\n",
      "f5d954d2bd94: Waiting\n",
      "db34a056d495: Waiting\n",
      "f9b1cb8c2687: Waiting\n",
      "e68443de6bca: Waiting\n",
      "88bb87a4088d: Waiting\n",
      "29bf522d97b4: Waiting\n",
      "d96c519f0898: Waiting\n",
      "ff0a6aeeabc0: Waiting\n",
      "8a7cebfdebb3: Waiting\n",
      "61546b863e43: Waiting\n",
      "d2b843fb2f7a: Waiting\n",
      "36c9a9d68143: Waiting\n",
      "730e84ac5c5d: Waiting\n",
      "a25ae1798c0c: Waiting\n",
      "37a19de06c8b: Waiting\n",
      "27c459f353b4: Waiting\n",
      "25d03c11e857: Waiting\n",
      "d5585264beff: Waiting\n",
      "e39414beba01: Waiting\n",
      "ff6af85bc8aa: Waiting\n",
      "98cedd6c9734: Waiting\n",
      "574aa732d388: Waiting\n",
      "686978e3bf48: Waiting\n",
      "80088b120579: Waiting\n",
      "79a187e0621d: Waiting\n",
      "72657ad6008c: Waiting\n",
      "22d4dd8ed907: Waiting\n",
      "d7e7872b888e: Waiting\n",
      "50858308da3d: Waiting\n",
      "5f08512fd434: Waiting\n",
      "c7bb31fc0e08: Waiting\n",
      "9d471c683180: Layer already exists\n",
      "a88fb86cb67d: Layer already exists\n",
      "2d3dc1ac256c: Layer already exists\n",
      "2a77eb46b89c: Layer already exists\n",
      "03dc40ebdbd6: Layer already exists\n",
      "e68443de6bca: Layer already exists\n",
      "88bb87a4088d: Layer already exists\n",
      "f5d954d2bd94: Layer already exists\n",
      "f9b1cb8c2687: Layer already exists\n",
      "db34a056d495: Layer already exists\n",
      "29bf522d97b4: Layer already exists\n",
      "d96c519f0898: Layer already exists\n",
      "8a7cebfdebb3: Layer already exists\n",
      "ff0a6aeeabc0: Layer already exists\n",
      "61546b863e43: Layer already exists\n",
      "d2b843fb2f7a: Layer already exists\n",
      "730e84ac5c5d: Layer already exists\n",
      "a25ae1798c0c: Layer already exists\n",
      "37a19de06c8b: Layer already exists\n",
      "36c9a9d68143: Layer already exists\n",
      "d5585264beff: Layer already exists\n",
      "27c459f353b4: Layer already exists\n",
      "25d03c11e857: Layer already exists\n",
      "ff6af85bc8aa: Layer already exists\n",
      "e39414beba01: Layer already exists\n",
      "574aa732d388: Layer already exists\n",
      "98cedd6c9734: Layer already exists\n",
      "686978e3bf48: Layer already exists\n",
      "80088b120579: Layer already exists\n",
      "72657ad6008c: Layer already exists\n",
      "22d4dd8ed907: Layer already exists\n",
      "79a187e0621d: Layer already exists\n",
      "d7e7872b888e: Layer already exists\n",
      "5f08512fd434: Layer already exists\n",
      "50858308da3d: Layer already exists\n",
      "c7bb31fc0e08: Layer already exists\n",
      "latest: digest: sha256:e5af439b70c2d27f2e9ae6718b15413a2ddd539674bcf76c48aef77a2870969c size: 7874\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker push $IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response: name: \"projects/654544512569/locations/us-central1/customJobs/4949618718201085952\"\n",
      "display_name: \"jax_customcontainer_training\"\n",
      "job_spec {\n",
      "  worker_pool_specs {\n",
      "    machine_spec {\n",
      "      machine_type: \"n1-standard-4\"\n",
      "      accelerator_type: NVIDIA_TESLA_T4\n",
      "      accelerator_count: 1\n",
      "    }\n",
      "    replica_count: 1\n",
      "    disk_spec {\n",
      "      boot_disk_type: \"pd-ssd\"\n",
      "      boot_disk_size_gb: 100\n",
      "    }\n",
      "    container_spec {\n",
      "      image_uri: \"gcr.io/dsparing-sandbox/jax_vertex_image_gpu:latest\"\n",
      "    }\n",
      "  }\n",
      "  base_output_directory {\n",
      "    output_uri_prefix: \"gs://dsparing-sandbox-bucket/models/jax_model_customcontainer/output\"\n",
      "  }\n",
      "}\n",
      "state: JOB_STATE_PENDING\n",
      "create_time {\n",
      "  seconds: 1624732707\n",
      "  nanos: 800796000\n",
      "}\n",
      "update_time {\n",
      "  seconds: 1624732707\n",
      "  nanos: 800796000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "api_endpoint: str = f\"{REGION}-aiplatform.googleapis.com\"\n",
    "\n",
    "# The AI Platform services require regional API endpoints.\n",
    "client_options = {\"api_endpoint\": api_endpoint}\n",
    "# Initialize client that will be used to create and send requests.\n",
    "# This client only needs to be created once, and can be reused for multiple\n",
    "# requests.\n",
    "client = aiplatform.gapic.JobServiceClient(client_options=client_options)\n",
    "custom_job = {\n",
    "    \"display_name\": JOB_NAME,\n",
    "    \"job_spec\": {\n",
    "        \"worker_pool_specs\": [\n",
    "            {\n",
    "                \"machine_spec\": {\n",
    "                    \"machine_type\": MACHINE_TYPE,\n",
    "                    \"accelerator_type\": ACCELERATOR_TYPE,\n",
    "                    \"accelerator_count\": ACCELERATOR_COUNT,\n",
    "                },\n",
    "                \"replica_count\": REPLICA_COUNT,\n",
    "                \"container_spec\": {\n",
    "                    \"image_uri\": IMAGE_URI,\n",
    "                },\n",
    "            }\n",
    "        ],\n",
    "        \"base_output_directory\": {\n",
    "            \"output_uri_prefix\": SAVEDMODEL_BASEDIR\n",
    "        },\n",
    "    },\n",
    "}\n",
    "parent = f\"projects/{PROJECT_ID}/locations/{REGION}\"\n",
    "response = client.create_custom_job(parent=parent, custom_job=custom_job)\n",
    "print(\"response:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JobState.JOB_STATE_RUNNING\n",
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_RUNNING\n",
      "JobState.JOB_STATE_RUNNING\n",
      "JobState.JOB_STATE_SUCCEEDED\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    job_state = client.get_custom_job(name=response.name).state\n",
    "    print(job_state)\n",
    "    if job_state not in (\n",
    "        aiplatform_v1.JobState.JOB_STATE_QUEUED,\n",
    "        aiplatform_v1.JobState.JOB_STATE_PENDING,\n",
    "        aiplatform_v1.JobState.JOB_STATE_RUNNING\n",
    "    ):\n",
    "        break\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure we can actually predict with savedmodel (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0  2021-06-26T17:50:35Z  gs://dsparing-sandbox-bucket/models/jax_model_customcontainer/output/model/\n",
      "     54003  2021-06-26T18:52:26Z  gs://dsparing-sandbox-bucket/models/jax_model_customcontainer/output/model/saved_model.pb\n",
      "                                 gs://dsparing-sandbox-bucket/models/jax_model_customcontainer/output/model/assets/\n",
      "                                 gs://dsparing-sandbox-bucket/models/jax_model_customcontainer/output/model/variables/\n",
      "TOTAL: 2 objects, 54003 bytes (52.74 KiB)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gsutil ls -l $SAVEDMODEL_BASEDIR/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:absl:Load dataset info from /home/jupyter/tensorflow_datasets/mnist/3.0.1\n",
      "INFO:absl:Field info.citation from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.splits from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.module_name from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Reusing dataset mnist (/home/jupyter/tensorflow_datasets/mnist/3.0.1)\n",
      "INFO:absl:Constructing tf.data.Dataset mnist for split test, from /home/jupyter/tensorflow_datasets/mnist/3.0.1\n"
     ]
    }
   ],
   "source": [
    "from jax.experimental.jax2tf.examples.mnist_lib import load_mnist\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from absl import flags\n",
    "\n",
    "# need to initialize flags somehow to avoid errors in load_mnist\n",
    "flags.FLAGS(['e'])\n",
    "\n",
    "image_to_predict, _ = next(iter(load_mnist(tfds.Split.TEST, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_0': <tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
       " array([[-9.033175 , -4.895049 , -2.108976 , -0.5636912, -3.901619 ,\n",
       "         -2.4996982, -2.2915943, -6.8160973, -2.3445058, -6.4918838]],\n",
       "       dtype=float32)>}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = tf.saved_model.load(f\"{SAVEDMODEL_BASEDIR}/model\")\n",
    "loaded_model.signatures[\"serving_default\"](image_to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "last_runtime": {
    "build_target": "//learning/deepmind/public/tools/ml_python:ml_notebook",
    "kind": "private"
   },
   "name": "JAX Flax MNIST containerize",
   "provenance": [
    {
     "file_id": "1HenHxSnSqNQPqMPZNG_uhFampEPxN_Cj",
     "timestamp": 1617196427779
    }
   ]
  },
  "environment": {
   "name": "tf2-gpu.2-5.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m74"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
