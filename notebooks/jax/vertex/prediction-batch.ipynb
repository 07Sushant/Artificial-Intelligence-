{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch prediction is commonly used when you have thousands to millions of predictions. It will create a Vertex AI batch prediction job. We will put our prediction request JSONL file (multiple lines of JSON records) to GCS, and use the Python API to request the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from absl import flags\n",
    "from google.cloud import aiplatform\n",
    "from jax.experimental.jax2tf.examples import mnist_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "flake8-noqa-line-1"
    ]
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = !(gcloud config get-value project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "\n",
    "REGION = \"us-central1\"\n",
    "\n",
    "BUCKET_NAME = PROJECT_ID\n",
    "# Use a regional bucket in the above region you have rights to.\n",
    "# Create if needed:\n",
    "# !gsutil mb -l $REGION gs://$BUCKET_NAME\n",
    "\n",
    "MODEL_RESOURCENAME = None\n",
    "MODEL_DISPLAYNAME = \"jax_model_customcontainer\"\n",
    "\n",
    "USE_GPU_SERVING = False\n",
    "\n",
    "SERVING_BATCH_SIZE = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On Vertex AI, only the \"resource name\" (ending in a numerical ID) of a model is unique, not its \"display name\". Therefore while you can look up your model(s) by display_name, if you have multiple ones, you need to know the resource_name of the one you need (this is returned and printed when you upload the model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax_model_customcontainer projects/654544512569/locations/us-central1/models/653734429503520768\n"
     ]
    }
   ],
   "source": [
    "# specify either a unique MODEL_DISPLAY_NAME, or a MODEL_RESOURCE_NAME\n",
    "if MODEL_RESOURCENAME:\n",
    "    model = aiplatform.Model(MODEL_RESOURCENAME)\n",
    "else:\n",
    "    models = aiplatform.Model.list(filter=f\"display_name={MODEL_DISPLAYNAME}\")\n",
    "    if len(models) > 1:\n",
    "        for model in models:\n",
    "            print(model.resource_name, model.display_name)\n",
    "        raise Exception(\n",
    "            f\"multiple models with display_name=={MODEL_DISPLAYNAME} \"\n",
    "            \"(see above), please delete all but one, or use a resource_name\"\n",
    "        )\n",
    "    model = models[0]\n",
    "\n",
    "print(model.display_name, model.resource_name)\n",
    "\n",
    "MODEL_DISPLAYNAME = model.display_name\n",
    "MODEL_RESOURCENAME = model.resource_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:absl:Load dataset info from /home/jupyter/tensorflow_datasets/mnist/3.0.1\n",
      "INFO:absl:Field info.citation from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.splits from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.module_name from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Reusing dataset mnist (/home/jupyter/tensorflow_datasets/mnist/3.0.1)\n",
      "INFO:absl:Constructing tf.data.Dataset mnist for split test, from /home/jupyter/tensorflow_datasets/mnist/3.0.1\n"
     ]
    }
   ],
   "source": [
    "# need to initialize flags somehow to avoid errors in load_mnist\n",
    "flags.FLAGS([\"\"])\n",
    "\n",
    "images_to_predict, _ = next(\n",
    "    iter(mnist_lib.load_mnist(tfds.Split.TEST, batch_size=SERVING_BATCH_SIZE))\n",
    ")\n",
    "with open(\"inputs.jsonl\", \"w\") as file:\n",
    "    for image in images_to_predict:\n",
    "        json.dump(dict(inputs=image.numpy().tolist()), file)\n",
    "        file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://inputs.jsonl [Content-Type=application/octet-stream]...\n",
      "/ [1 files][ 21.4 KiB/ 21.4 KiB]                                                \n",
      "Operation completed over 1 objects/21.4 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp inputs.jsonl \\\n",
    "    gs://$BUCKET_NAME/batchpred/$MODEL_DISPLAYNAME/inputs.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:Creating BatchPredictionJob\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob created. Resource name: projects/654544512569/locations/us-central1/batchPredictionJobs/4116523155881721856\n",
      "INFO:google.cloud.aiplatform.jobs:To use this BatchPredictionJob in another session:\n",
      "INFO:google.cloud.aiplatform.jobs:bpj = aiplatform.BatchPredictionJob('projects/654544512569/locations/us-central1/batchPredictionJobs/4116523155881721856')\n",
      "INFO:google.cloud.aiplatform.jobs:View Batch Prediction Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/batch-predictions/4116523155881721856?project=654544512569\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/654544512569/locations/us-central1/batchPredictionJobs/4116523155881721856 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/654544512569/locations/us-central1/batchPredictionJobs/4116523155881721856 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/654544512569/locations/us-central1/batchPredictionJobs/4116523155881721856 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/654544512569/locations/us-central1/batchPredictionJobs/4116523155881721856 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/654544512569/locations/us-central1/batchPredictionJobs/4116523155881721856 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/654544512569/locations/us-central1/batchPredictionJobs/4116523155881721856 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/654544512569/locations/us-central1/batchPredictionJobs/4116523155881721856 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/654544512569/locations/us-central1/batchPredictionJobs/4116523155881721856 current state:\n",
      "JobState.JOB_STATE_SUCCEEDED\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob run completed. Resource name: projects/654544512569/locations/us-central1/batchPredictionJobs/4116523155881721856\n",
      "jax_model_customcontainer_batchprediction\n",
      "projects/654544512569/locations/us-central1/batchPredictionJobs/4116523155881721856\n",
      "JobState.JOB_STATE_SUCCEEDED\n"
     ]
    }
   ],
   "source": [
    "MACHINE_TYPE = \"n1-standard-2\"\n",
    "\n",
    "batch_prediction_job = aiplatform.Model(MODEL_RESOURCENAME).batch_predict(\n",
    "    job_display_name=f\"{MODEL_DISPLAYNAME}_batchprediction\",\n",
    "    gcs_source=(\n",
    "        f\"gs://{BUCKET_NAME}/batchpred/\" f\"{MODEL_DISPLAYNAME}/inputs.jsonl\"\n",
    "    ),\n",
    "    gcs_destination_prefix=f\"gs://{BUCKET_NAME}/batchpred/{MODEL_DISPLAYNAME}\",\n",
    "    machine_type=MACHINE_TYPE,\n",
    ")\n",
    "\n",
    "batch_prediction_job.wait()\n",
    "\n",
    "print(batch_prediction_job.display_name)\n",
    "print(batch_prediction_job.resource_name)\n",
    "print(batch_prediction_job.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "flake8-noqa-line-1"
    ]
   },
   "outputs": [],
   "source": [
    "latest_prediction_dir = !gsutil ls gs://$BUCKET_NAME/batchpred/$MODEL_DISPLAYNAME/outputs | grep prediction-$MODEL_DISPLAYNAME- | tail -1\n",
    "latest_prediction_dir = os.path.dirname(latest_prediction_dir[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below error file might be empty, if all goes well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "flake8-noqa-line-2"
    ]
   },
   "outputs": [],
   "source": [
    "# empty if no errors\n",
    "!gsutil cat $latest_prediction_dir/prediction.errors_stats-*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://dsparing-sandbox/batchpred/jax_model_customcontainer/outputs/prediction-jax_model_customcontainer-2021_07_01T08_40_52_145Z/prediction.results-00000-of-00001\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls $latest_prediction_dir/prediction.results-*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There might be multiple output files. We now only investigate the first one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "flake8-noqa-line-1"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;39m[\u001b[0;39m-11.9759874\u001b[0m\u001b[1;39m,\u001b[0;39m-14.1695709\u001b[0m\u001b[1;39m,\u001b[0;39m-11.7008791\u001b[0m\u001b[1;39m,\u001b[0;39m-7.97484255\u001b[0m\u001b[1;39m,\u001b[0;39m-5.06094456\u001b[0m\u001b[1;39m,\u001b[0;39m-9.23107433\u001b[0m\u001b[1;39m,\u001b[0;39m-13.9212132\u001b[0m\u001b[1;39m,\u001b[0;39m-3.24065161\u001b[0m\u001b[1;39m,\u001b[0;39m-6.47828436\u001b[0m\u001b[1;39m,\u001b[0;39m-0.0486364365\u001b[0m\u001b[1;39m\u001b[1;39m]\u001b[0m\n",
      "\u001b[1;39m[\u001b[0;39m-10.7667494\u001b[0m\u001b[1;39m,\u001b[0;39m-16.7841396\u001b[0m\u001b[1;39m,\u001b[0;39m-12.4569149\u001b[0m\u001b[1;39m,\u001b[0;39m-5.81106091\u001b[0m\u001b[1;39m,\u001b[0;39m-12.0771446\u001b[0m\u001b[1;39m,\u001b[0;39m-8.92639\u001b[0m\u001b[1;39m,\u001b[0;39m-17.6782799\u001b[0m\u001b[1;39m,\u001b[0;39m-0.0128059387\u001b[0m\u001b[1;39m,\u001b[0;39m-9.55736\u001b[0m\u001b[1;39m,\u001b[0;39m-4.65686941\u001b[0m\u001b[1;39m\u001b[1;39m]\u001b[0m\n",
      "\u001b[1;39m[\u001b[0;39m-10.7817202\u001b[0m\u001b[1;39m,\u001b[0;39m-13.3755789\u001b[0m\u001b[1;39m,\u001b[0;39m-8.78866291\u001b[0m\u001b[1;39m,\u001b[0;39m-11.231391\u001b[0m\u001b[1;39m,\u001b[0;39m-0.0163898468\u001b[0m\u001b[1;39m,\u001b[0;39m-7.05897188\u001b[0m\u001b[1;39m,\u001b[0;39m-7.22624874\u001b[0m\u001b[1;39m,\u001b[0;39m-5.87213516\u001b[0m\u001b[1;39m,\u001b[0;39m-5.62200928\u001b[0m\u001b[1;39m,\u001b[0;39m-4.82241917\u001b[0m\u001b[1;39m\u001b[1;39m]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!gsutil cat $latest_prediction_dir/prediction.results-00000-of-00001 | jq -c '.[\"prediction\"]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-5.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m74"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
