{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2b45880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db9f2e1",
   "metadata": {},
   "source": [
    "# Upload JAX/Flax-trained SavedModel to Vertex AI with TF Serving custom prediction container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d1cd12",
   "metadata": {},
   "source": [
    "Vertex AI Prediction supports pre-built containers, with no additional customization such as args (\"Do not specify any other subfields of containerSpec\" [source](https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers#using_a_pre-built_container)), and custom containers. As we need the `--xla_cpu_compilation_enabled` arg, we can only use custom containers. However, we can simply use tensorflow/serving from Docker Hub as our custom container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d2aeaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import requests\n",
    "import tensorflow_datasets as tfds\n",
    "from absl import flags\n",
    "from google.cloud import aiplatform\n",
    "from jax.experimental.jax2tf.examples import mnist_lib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48d6a10",
   "metadata": {},
   "source": [
    "## Create TFServing container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107d1e17",
   "metadata": {},
   "source": [
    "NOTE: serving does not work for GPU yet, the below is a CPU example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109aea55",
   "metadata": {},
   "source": [
    "Below, `MODEL_BASE_PATH/MODEL_NAME/model/MODEL_VERSION` should point to a model created in [training-prebuilt.ipynb](training-prebuilt.ipynb). Change it to any directory containing a SavedModel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67acdf33",
   "metadata": {
    "tags": [
     "flake8-noqa-line-1"
    ]
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "\n",
    "REGION = \"us-central1\"\n",
    "os.environ['REGION'] = REGION\n",
    "\n",
    "BUCKET_NAME = PROJECT_ID\n",
    "os.environ['BUCKET_NAME'] = BUCKET_NAME\n",
    "# Use a regional bucket in the above region you have rights to.\n",
    "# Create if needed:\n",
    "# !gsutil mb -l ${REGION} gs://${BUCKET_NAME}\n",
    "\n",
    "MODEL_BASE_PATH = f\"gs://{BUCKET_NAME}/savedmodels\"\n",
    "MODEL_NAME = \"jax_model_prebuilt\"\n",
    "MODEL_VERSION = 1\n",
    "\n",
    "USE_GPU_SERVING = False\n",
    "if USE_GPU_SERVING:\n",
    "    TFSERVING_TAG = \"latest-gpu\"\n",
    "else:\n",
    "    TFSERVING_TAG = \"latest\"\n",
    "\n",
    "IMAGE_TAG = 'latest'\n",
    "SERVING_IMAGE_NAME = 'jax_vertex_serving'\n",
    "SERVING_IMAGE_URI = 'gcr.io/{}/{}:{}'.format(PROJECT_ID, SERVING_IMAGE_NAME, IMAGE_TAG)\n",
    "os.environ['SERVING_IMAGE_URI'] = SERVING_IMAGE_URI\n",
    "\n",
    "os.environ[\"MODEL_BASE_PATH\"] = MODEL_BASE_PATH\n",
    "os.environ[\"MODEL_NAME\"] = MODEL_NAME\n",
    "os.environ[\"MODEL_VERSION\"] = str(MODEL_VERSION)\n",
    "os.environ[\"TFSERVING_TAG\"] = TFSERVING_TAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c6390f",
   "metadata": {},
   "source": [
    "Check that `MODEL_BASE_PATH` actually contains a SavedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36baf2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://dsparing-sandbox/savedmodels/jax_model_prebuilt/model/1/\n",
      "gs://dsparing-sandbox/savedmodels/jax_model_prebuilt/model/1/saved_model.pb\n",
      "gs://dsparing-sandbox/savedmodels/jax_model_prebuilt/model/1/assets/\n",
      "gs://dsparing-sandbox/savedmodels/jax_model_prebuilt/model/1/variables/\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls $MODEL_BASE_PATH/$MODEL_NAME/model/$MODEL_VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c299594d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVING_FOLDER = 'serving'\n",
    "os.environ['SERVING_FOLDER'] = SERVING_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44f73fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p $SERVING_FOLDER\n",
    "cat > $SERVING_FOLDER/Dockerfile << EOF\n",
    "FROM tensorflow/serving:$TFSERVING_TAG\n",
    "\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e738ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd $SERVING_FOLDER && gcloud builds submit --tag $SERVING_IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd295f06",
   "metadata": {},
   "source": [
    "## Try serving locally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e809e3",
   "metadata": {},
   "source": [
    "Get image to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da6ba5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:absl:Load dataset info from /home/jupyter/tensorflow_datasets/mnist/3.0.1\n",
      "INFO:absl:Field info.citation from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.splits from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.module_name from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Reusing dataset mnist (/home/jupyter/tensorflow_datasets/mnist/3.0.1)\n",
      "INFO:absl:Constructing tf.data.Dataset mnist for split test, from /home/jupyter/tensorflow_datasets/mnist/3.0.1\n"
     ]
    }
   ],
   "source": [
    "# need to initialize flags somehow to avoid errors in load_mnist\n",
    "flags.FLAGS([''])\n",
    "\n",
    "image_to_predict, _ = next(iter(mnist_lib.load_mnist(tfds.Split.TEST, batch_size=1)))\n",
    "instances = image_to_predict.numpy().tolist()\n",
    "image_json = json.dumps(instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd305a15",
   "metadata": {},
   "source": [
    "Start up container (for GPU it should use `--runtime=nvidia` but for now GPU prediction does not work.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c9e6891",
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latest: Pulling from dsparing-sandbox/jax_vertex_serving\n",
      "Digest: sha256:6651f4839e1124dbde75ee531825112af0a6b8ef082c88ab14ca53eb69a2e4bb\n",
      "Status: Image is up to date for gcr.io/dsparing-sandbox/jax_vertex_serving:latest\n",
      "gcr.io/dsparing-sandbox/jax_vertex_serving:latest\n",
      "f3f9698c2710909edf390a193daabee17980e31e68517661b1ee575d0f85f753\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker pull $SERVING_IMAGE_URI\n",
    "docker run -d -p 8501:8501 \\\n",
    "    --name serving_jax \\\n",
    "    $SERVING_IMAGE_URI \\\n",
    "    --xla_cpu_compilation_enabled=true \\\n",
    "    --model_base_path=$MODEL_BASE_PATH/$MODEL_NAME/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cc501c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-28 08:00:45.607151: I tensorflow_serving/model_servers/server.cc:89] Building single TensorFlow model file config:  model_name: model model_base_path: gs://dsparing-sandbox/savedmodels/jax_model_prebuilt/model\n",
      "2021-06-28 08:00:45.607508: I tensorflow_serving/model_servers/server_core.cc:465] Adding/updating models.\n",
      "2021-06-28 08:00:45.607553: I tensorflow_serving/model_servers/server_core.cc:591]  (Re-)adding model: model\n",
      "2021-06-28 08:00:47.144518: I tensorflow_serving/core/basic_manager.cc:740] Successfully reserved resources to load servable {name: model version: 1}\n",
      "2021-06-28 08:00:47.144588: I tensorflow_serving/core/loader_harness.cc:66] Approving load for servable version {name: model version: 1}\n",
      "2021-06-28 08:00:47.144632: I tensorflow_serving/core/loader_harness.cc:74] Loading servable version {name: model version: 1}\n",
      "2021-06-28 08:00:47.277616: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:38] Reading SavedModel from: gs://dsparing-sandbox/savedmodels/jax_model_prebuilt/model/1\n",
      "2021-06-28 08:00:47.482430: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:90] Reading meta graph with tags { serve }\n",
      "2021-06-28 08:00:47.482538: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: gs://dsparing-sandbox/savedmodels/jax_model_prebuilt/model/1\n",
      "2021-06-28 08:00:47.619129: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-06-28 08:00:47.659288: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:206] Restoring SavedModel bundle.\n",
      "2021-06-28 08:00:47.683991: I external/org_tensorflow/tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2299995000 Hz\n",
      "2021-06-28 08:00:48.385153: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:190] Running initialization op on SavedModel bundle at path: gs://dsparing-sandbox/savedmodels/jax_model_prebuilt/model/1\n",
      "2021-06-28 08:00:48.391377: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:277] SavedModel load for tags { serve }; Status: success: OK. Took 1113763 microseconds.\n",
      "2021-06-28 08:00:48.523124: I tensorflow_serving/servables/tensorflow/saved_model_warmup_util.cc:59] No warmup data file found at gs://dsparing-sandbox/savedmodels/jax_model_prebuilt/model/1/assets.extra/tf_serving_warmup_requests\n",
      "2021-06-28 08:00:49.209073: I tensorflow_serving/core/loader_harness.cc:87] Successfully loaded servable version {name: model version: 1}\n",
      "2021-06-28 08:00:49.210397: I tensorflow_serving/model_servers/server_core.cc:486] Finished adding/updating models\n",
      "2021-06-28 08:00:49.210478: I tensorflow_serving/model_servers/server.cc:367] Profiler service is enabled\n",
      "2021-06-28 08:00:49.211828: I tensorflow_serving/model_servers/server.cc:393] Running gRPC ModelServer at 0.0.0.0:8500 ...\n",
      "[warn] getaddrinfo: address family for nodename not supported\n",
      "2021-06-28 08:00:49.213867: I tensorflow_serving/model_servers/server.cc:414] Exporting HTTP/REST API at:localhost:8501 ...\n",
      "[evhttp_server.cc : 245] NET_LOG: Entering the event loop ...\n"
     ]
    }
   ],
   "source": [
    "!sleep 20 && docker logs serving_jax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37ac96d",
   "metadata": {},
   "source": [
    "With the `sleep`, we give it some time for TF Serving to load the model from Cloud Storage and be ready to accept requests. Verify in the log below that it is indeed ready. (Probably printing something like `\"Entering the event loop ...\"`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55befab5",
   "metadata": {},
   "source": [
    "Send prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "324f88d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-6.91256428, -21.7614384, -0.0348739624, -13.3985834, -15.6128826, -12.7912817, -3.40373135, -21.8225727, -10.63381, -19.4123306]]\n"
     ]
    }
   ],
   "source": [
    "data = json.dumps({\"instances\": image_to_predict.numpy().tolist()})\n",
    "json_response = requests.post(\n",
    "    f'http://localhost:8501/v1/models/model:predict',\n",
    "    data=data,\n",
    ")\n",
    "predictions = json.loads(json_response.text)['predictions']\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2e459cd",
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serving_jax\n"
     ]
    }
   ],
   "source": [
    "!docker rm -f serving_jax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f452af",
   "metadata": {},
   "source": [
    "## Upload model to Vertex AI using custom container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fdadad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Creating Model\n",
      "INFO:google.cloud.aiplatform.models:Create Model backing LRO: projects/654544512569/locations/us-central1/models/8976949490837618688/operations/2809439400822308864\n",
      "INFO:google.cloud.aiplatform.models:Model created. Resource name: projects/654544512569/locations/us-central1/models/8976949490837618688\n",
      "INFO:google.cloud.aiplatform.models:To use this Model in another session:\n",
      "INFO:google.cloud.aiplatform.models:model = aiplatform.Model('projects/654544512569/locations/us-central1/models/8976949490837618688')\n",
      "jax_model_prebuilt\n",
      "projects/654544512569/locations/us-central1/models/8976949490837618688\n"
     ]
    }
   ],
   "source": [
    "model = aiplatform.Model.upload(\n",
    "    display_name=MODEL_NAME,\n",
    "    serving_container_image_uri=SERVING_IMAGE_URI,\n",
    "    artifact_uri=f\"{MODEL_BASE_PATH}/{MODEL_NAME}/model\",\n",
    "    serving_container_predict_route=f\"/v1/models/model:predict\",\n",
    "    serving_container_health_route=f\"/v1/models/model\",\n",
    "    serving_container_args=[\n",
    "        '--xla_cpu_compilation_enabled=true',\n",
    "        '--model_base_path=$(AIP_STORAGE_URI)',\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(model.display_name)\n",
    "print(model.resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e42723",
   "metadata": {},
   "source": [
    "You need to note the model resource name (as a unique identifier for your model) for prediction later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4eed32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-5.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m74"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
