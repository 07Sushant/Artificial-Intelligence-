{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39c21e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d38df61",
   "metadata": {},
   "source": [
    "# Upload JAX/Flax-trained SavedModel to Vertex AI with TF Serving custom prediction container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5430460c",
   "metadata": {},
   "source": [
    "Vertex AI Prediction supports pre-built containers, with no additional customization such as args (\"Do not specify any other subfields of containerSpec\" [source](https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers#using_a_pre-built_container)), and custom containers. As we need the `--xla_cpu_compilation_enabled` arg, we can only use custom containers. However, we can simply use tensorflow/serving from Docker Hub as our custom container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ab45140",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import requests\n",
    "import tensorflow_datasets as tfds\n",
    "from absl import flags\n",
    "from google.cloud import aiplatform\n",
    "from jax.experimental.jax2tf.examples import mnist_lib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06270776",
   "metadata": {},
   "source": [
    "Below, `MODEL_BASE_PATH/model/MODEL_NAME/MODEL_VERSION` should point to a model created in [training-prebuilt.ipynb](training-prebuilt.ipynb). Change it to any directory containing a SavedModel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ffff588",
   "metadata": {
    "tags": [
     "flake8-noqa-line-1"
    ]
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "\n",
    "REGION = \"us-central1\"\n",
    "os.environ['REGION'] = REGION\n",
    "\n",
    "BUCKET_NAME = PROJECT_ID\n",
    "os.environ['BUCKET_NAME'] = BUCKET_NAME\n",
    "# Use a regional bucket in the above region you have rights to.\n",
    "# Create if needed:\n",
    "# !gsutil mb -l ${REGION} gs://${BUCKET_NAME}\n",
    "\n",
    "MODEL_BASE_PATH = f\"gs://{BUCKET_NAME}/model\"\n",
    "MODEL_NAME = \"jax_model_prebuilt\"\n",
    "MODEL_VERSION = 1\n",
    "\n",
    "USE_GPU_SERVING = False\n",
    "if USE_GPU_SERVING:\n",
    "    TFSERVING_TAG = \"latest-gpu\"\n",
    "else:\n",
    "    TFSERVING_TAG = \"latest\"\n",
    "\n",
    "IMAGE_TAG = 'latest'\n",
    "SERVING_IMAGE_NAME = 'jax_vertex_serving'\n",
    "SERVING_IMAGE_URI = f\"gcr.io/{PROJECT_ID}/{SERVING_IMAGE_NAME}:{IMAGE_TAG}\"\n",
    "os.environ['SERVING_IMAGE_URI'] = SERVING_IMAGE_URI\n",
    "\n",
    "os.environ[\"MODEL_BASE_PATH\"] = MODEL_BASE_PATH\n",
    "os.environ[\"MODEL_NAME\"] = MODEL_NAME\n",
    "os.environ[\"MODEL_VERSION\"] = str(MODEL_VERSION)\n",
    "os.environ[\"TFSERVING_TAG\"] = TFSERVING_TAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee98be2",
   "metadata": {},
   "source": [
    "Check that `MODEL_BASE_PATH/model/MODEL_NAME/MODEL_VERSION` actually contains a SavedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85a36d11",
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://dsparing-sandbox/savedmodels/jax_model_prebuilt/model/model/1/\n",
      "gs://dsparing-sandbox/savedmodels/jax_model_prebuilt/model/model/1/saved_model.pb\n",
      "gs://dsparing-sandbox/savedmodels/jax_model_prebuilt/model/model/1/assets/\n",
      "gs://dsparing-sandbox/savedmodels/jax_model_prebuilt/model/model/1/variables/\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls $MODEL_BASE_PATH/model/$MODEL_NAME/$MODEL_VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748c20fb",
   "metadata": {},
   "source": [
    "## Create TFServing container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474a27b5",
   "metadata": {},
   "source": [
    "NOTE: serving does not work for GPU yet, the below is a CPU example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25f82cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVING_FOLDER = 'serving'\n",
    "os.environ['SERVING_FOLDER'] = SERVING_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94bd7b7a",
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p $SERVING_FOLDER\n",
    "cat > $SERVING_FOLDER/Dockerfile << EOF\n",
    "FROM tensorflow/serving:$TFSERVING_TAG\n",
    "\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c426b881",
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 1 file(s) totalling 32 bytes before compression.\n",
      "Uploading tarball of [.] to [gs://dsparing-sandbox_cloudbuild/source/1624944806.096853-0e413b57b65a40618358358be0abdea7.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/dsparing-sandbox/locations/global/builds/f7a90bce-6d5e-4424-a53a-77a69d623958].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/f7a90bce-6d5e-4424-a53a-77a69d623958?project=654544512569].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"f7a90bce-6d5e-4424-a53a-77a69d623958\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://dsparing-sandbox_cloudbuild/source/1624944806.096853-0e413b57b65a40618358358be0abdea7.tgz#1624944806297629\n",
      "Copying gs://dsparing-sandbox_cloudbuild/source/1624944806.096853-0e413b57b65a40618358358be0abdea7.tgz#1624944806297629...\n",
      "/ [1 files][  199.0 B/  199.0 B]                                                \n",
      "Operation completed over 1 objects/199.0 B.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon   2.56kB\n",
      "Step 1/1 : FROM tensorflow/serving:latest\n",
      "latest: Pulling from tensorflow/serving\n",
      "01bf7da0a88c: Pulling fs layer\n",
      "f3b4a5f15c7a: Pulling fs layer\n",
      "57ffbe87baa1: Pulling fs layer\n",
      "e72e6208e893: Pulling fs layer\n",
      "6ea3f464ef73: Pulling fs layer\n",
      "01e9bf86544b: Pulling fs layer\n",
      "68f6bba3dc50: Pulling fs layer\n",
      "e72e6208e893: Waiting\n",
      "6ea3f464ef73: Waiting\n",
      "01e9bf86544b: Waiting\n",
      "68f6bba3dc50: Waiting\n",
      "f3b4a5f15c7a: Verifying Checksum\n",
      "f3b4a5f15c7a: Download complete\n",
      "57ffbe87baa1: Verifying Checksum\n",
      "57ffbe87baa1: Download complete\n",
      "e72e6208e893: Verifying Checksum\n",
      "e72e6208e893: Download complete\n",
      "01e9bf86544b: Verifying Checksum\n",
      "01e9bf86544b: Download complete\n",
      "68f6bba3dc50: Verifying Checksum\n",
      "68f6bba3dc50: Download complete\n",
      "01bf7da0a88c: Verifying Checksum\n",
      "01bf7da0a88c: Download complete\n",
      "6ea3f464ef73: Verifying Checksum\n",
      "6ea3f464ef73: Download complete\n",
      "01bf7da0a88c: Pull complete\n",
      "f3b4a5f15c7a: Pull complete\n",
      "57ffbe87baa1: Pull complete\n",
      "e72e6208e893: Pull complete\n",
      "6ea3f464ef73: Pull complete\n",
      "01e9bf86544b: Pull complete\n",
      "68f6bba3dc50: Pull complete\n",
      "Digest: sha256:6651f4839e1124dbde75ee531825112af0a6b8ef082c88ab14ca53eb69a2e4bb\n",
      "Status: Downloaded newer image for tensorflow/serving:latest\n",
      " ---> e874bf5e4700\n",
      "Successfully built e874bf5e4700\n",
      "Successfully tagged gcr.io/dsparing-sandbox/jax_vertex_serving:latest\n",
      "PUSH\n",
      "Pushing gcr.io/dsparing-sandbox/jax_vertex_serving:latest\n",
      "The push refers to repository [gcr.io/dsparing-sandbox/jax_vertex_serving]\n",
      "bb4423850a27: Preparing\n",
      "b60ba33781cd: Preparing\n",
      "547f89523b17: Preparing\n",
      "bd91f28d5f3c: Preparing\n",
      "8cafc6d2db45: Preparing\n",
      "a5d4bacb0351: Preparing\n",
      "5153e1acaabc: Preparing\n",
      "a5d4bacb0351: Waiting\n",
      "5153e1acaabc: Waiting\n",
      "b60ba33781cd: Layer already exists\n",
      "547f89523b17: Layer already exists\n",
      "8cafc6d2db45: Layer already exists\n",
      "bd91f28d5f3c: Layer already exists\n",
      "bb4423850a27: Layer already exists\n",
      "5153e1acaabc: Layer already exists\n",
      "a5d4bacb0351: Layer already exists\n",
      "latest: digest: sha256:6651f4839e1124dbde75ee531825112af0a6b8ef082c88ab14ca53eb69a2e4bb size: 1780\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                          IMAGES                                                STATUS\n",
      "f7a90bce-6d5e-4424-a53a-77a69d623958  2021-06-29T05:33:26+00:00  19S       gs://dsparing-sandbox_cloudbuild/source/1624944806.096853-0e413b57b65a40618358358be0abdea7.tgz  gcr.io/dsparing-sandbox/jax_vertex_serving (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!cd $SERVING_FOLDER && \\\n",
    "    gcloud builds submit --tag $SERVING_IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f492991d",
   "metadata": {},
   "source": [
    "## Try serving locally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c0691c",
   "metadata": {},
   "source": [
    "Get image to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd1adf5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:absl:Load dataset info from /home/jupyter/tensorflow_datasets/mnist/3.0.1\n",
      "INFO:absl:Field info.citation from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.splits from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.module_name from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Reusing dataset mnist (/home/jupyter/tensorflow_datasets/mnist/3.0.1)\n",
      "INFO:absl:Constructing tf.data.Dataset mnist for split test, from /home/jupyter/tensorflow_datasets/mnist/3.0.1\n"
     ]
    }
   ],
   "source": [
    "# need to initialize flags somehow to avoid errors in load_mnist\n",
    "flags.FLAGS([''])\n",
    "\n",
    "image_to_predict, _ = next(iter(\n",
    "    mnist_lib.load_mnist(tfds.Split.TEST, batch_size=1)\n",
    "))\n",
    "instances = image_to_predict.numpy().tolist()\n",
    "image_json = json.dumps(instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8fd0f9",
   "metadata": {},
   "source": [
    "Start up container (for GPU it should use `--runtime=nvidia` but for now GPU prediction does not work.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57f1e863",
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latest: Pulling from dsparing-sandbox/jax_vertex_serving\n",
      "Digest: sha256:6651f4839e1124dbde75ee531825112af0a6b8ef082c88ab14ca53eb69a2e4bb\n",
      "Status: Image is up to date for gcr.io/dsparing-sandbox/jax_vertex_serving:latest\n",
      "gcr.io/dsparing-sandbox/jax_vertex_serving:latest\n",
      "c25aa20a7c112c9df780d8ca9cfcf9cf0875e058f7225c438fc6233a4bf2b2ba\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker pull $SERVING_IMAGE_URI\n",
    "docker run -d -p 8501:8501 \\\n",
    "    --name serving_jax \\\n",
    "    --env MODEL_NAME=$MODEL_NAME \\\n",
    "    --env MODEL_BASE_PATH=$$MODEL_BASE_PATH \\\n",
    "    $SERVING_IMAGE_URI \\\n",
    "    --xla_cpu_compilation_enabled=true\n",
    "#    --model_name=$MODEL_NAME \\\n",
    "#    --model_base_path=$MODEL_BASE_PATH/$MODEL_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7218423e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-29 05:33:52.199259: I tensorflow_serving/model_servers/server.cc:89] Building single TensorFlow model file config:  model_name: model model_base_path: gs://dsparing-sandbox/savedmodels/jax_model_prebuilt/model/model\n",
      "2021-06-29 05:33:52.199824: I tensorflow_serving/model_servers/server_core.cc:465] Adding/updating models.\n",
      "2021-06-29 05:33:52.199850: I tensorflow_serving/model_servers/server_core.cc:591]  (Re-)adding model: model\n",
      "2021-06-29 05:33:53.773127: I tensorflow_serving/core/basic_manager.cc:740] Successfully reserved resources to load servable {name: model version: 1}\n",
      "2021-06-29 05:33:53.773172: I tensorflow_serving/core/loader_harness.cc:66] Approving load for servable version {name: model version: 1}\n",
      "2021-06-29 05:33:53.773205: I tensorflow_serving/core/loader_harness.cc:74] Loading servable version {name: model version: 1}\n",
      "2021-06-29 05:33:53.917876: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:38] Reading SavedModel from: gs://dsparing-sandbox/savedmodels/jax_model_prebuilt/model/model/1\n",
      "2021-06-29 05:33:54.172742: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:90] Reading meta graph with tags { serve }\n",
      "2021-06-29 05:33:54.172790: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: gs://dsparing-sandbox/savedmodels/jax_model_prebuilt/model/model/1\n",
      "2021-06-29 05:33:54.303320: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-06-29 05:33:54.344662: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:206] Restoring SavedModel bundle.\n",
      "2021-06-29 05:33:54.375520: I external/org_tensorflow/tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2299995000 Hz\n",
      "2021-06-29 05:33:55.297074: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:190] Running initialization op on SavedModel bundle at path: gs://dsparing-sandbox/savedmodels/jax_model_prebuilt/model/model/1\n",
      "2021-06-29 05:33:55.305267: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:277] SavedModel load for tags { serve }; Status: success: OK. Took 1387393 microseconds.\n",
      "2021-06-29 05:33:55.484422: I tensorflow_serving/servables/tensorflow/saved_model_warmup_util.cc:59] No warmup data file found at gs://dsparing-sandbox/savedmodels/jax_model_prebuilt/model/model/1/assets.extra/tf_serving_warmup_requests\n",
      "2021-06-29 05:33:56.189340: I tensorflow_serving/core/loader_harness.cc:87] Successfully loaded servable version {name: model version: 1}\n",
      "2021-06-29 05:33:56.190376: I tensorflow_serving/model_servers/server_core.cc:486] Finished adding/updating models\n",
      "2021-06-29 05:33:56.190586: I tensorflow_serving/model_servers/server.cc:367] Profiler service is enabled\n",
      "2021-06-29 05:33:56.240606: I tensorflow_serving/model_servers/server.cc:393] Running gRPC ModelServer at 0.0.0.0:8500 ...\n",
      "[warn] getaddrinfo: address family for nodename not supported\n",
      "[evhttp_server.cc : 245] NET_LOG: Entering the event loop ...\n",
      "2021-06-29 05:33:56.243051: I tensorflow_serving/model_servers/server.cc:414] Exporting HTTP/REST API at:localhost:8501 ...\n"
     ]
    }
   ],
   "source": [
    "!sleep 20 && docker logs serving_jax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a87c37",
   "metadata": {},
   "source": [
    "With the `sleep`, we give it some time for TF Serving to load the model from Cloud Storage and be ready to accept requests. Verify in the log below that it is indeed ready. (Probably printing something like `\"Entering the event loop ...\"`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8beae5f",
   "metadata": {},
   "source": [
    "Send prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "408f1c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-10.0790615, -0.0246343613, -5.78542423, -6.52261972, -9.82075691, -8.41873264, -7.13855028, -8.35187149, -3.99987411, -8.9720192]]\n"
     ]
    }
   ],
   "source": [
    "data = json.dumps({\"instances\": image_to_predict.numpy().tolist()})\n",
    "json_response = requests.post(\n",
    "    f\"http://localhost:8501/v1/models/{MODEL_NAME}:predict\",\n",
    "    data=data,\n",
    ")\n",
    "predictions = json.loads(json_response.text)['predictions']\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d0865c3",
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serving_jax\n"
     ]
    }
   ],
   "source": [
    "!docker rm -f serving_jax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b1fd39",
   "metadata": {},
   "source": [
    "## Upload model to Vertex AI using custom container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "250613ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Creating Model\n",
      "INFO:google.cloud.aiplatform.models:Create Model backing LRO: projects/654544512569/locations/us-central1/models/996570951137099776/operations/4201544266889035776\n",
      "INFO:google.cloud.aiplatform.models:Model created. Resource name: projects/654544512569/locations/us-central1/models/996570951137099776\n",
      "INFO:google.cloud.aiplatform.models:To use this Model in another session:\n",
      "INFO:google.cloud.aiplatform.models:model = aiplatform.Model('projects/654544512569/locations/us-central1/models/996570951137099776')\n",
      "jax_model_prebuilt\n",
      "projects/654544512569/locations/us-central1/models/996570951137099776\n"
     ]
    }
   ],
   "source": [
    "model = aiplatform.Model.upload(\n",
    "    display_name=MODEL_NAME,\n",
    "    serving_container_image_uri=SERVING_IMAGE_URI,\n",
    "    artifact_uri=f\"{MODEL_BASE_PATH}/{MODEL_NAME}/model\",\n",
    "    serving_container_predict_route=f\"/v1/models/{MODEL_NAME}:predict\",\n",
    "    serving_container_health_routef=\"/v1/models/{MODEL_NAME}\",\n",
    "    serving_container_args=[\n",
    "        '--xla_cpu_compilation_enabled=true',\n",
    "#        f'--model_name={MODEL_NAME}',\n",
    "#        f'--model_base_path=$(AIP_STORAGE_URI)/{MODEL_NAME}',\n",
    "    ],\n",
    "    model_serving_container_environment_variables = {\n",
    "        \"MODEL_NAME\": MODEL_NAME,\n",
    "        \"MODEL_BASE_PATH\": AIP_STORAGE_URI,\n",
    "    }\n",
    "    serving_container_ports=[8501],\n",
    ")\n",
    "\n",
    "print(model.display_name)\n",
    "print(model.resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf0fbf4",
   "metadata": {},
   "source": [
    "You need to note the model resource name (as a unique identifier for your model) for prediction later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb2a49c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-5.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m74"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
