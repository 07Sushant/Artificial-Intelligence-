{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2f7d5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c81c93",
   "metadata": {},
   "source": [
    "# Upload JAX/Flax-trained SavedModel to Vertex AI with TF Serving custom prediction container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9a09af",
   "metadata": {},
   "source": [
    "Vertex AI Prediction supports pre-built containers, with no additional customization such as args (\"Do not specify any other subfields of containerSpec\" [source](https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers#using_a_pre-built_container)), and custom containers. As we need the `--xla_cpu_compilation_enabled` arg, we can only use custom containers. However, we can simply use tensorflow/serving from Docker Hub as our custom container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c813e4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import requests\n",
    "import tensorflow_datasets as tfds\n",
    "from absl import flags\n",
    "from google.cloud import aiplatform\n",
    "from jax.experimental.jax2tf.examples import mnist_lib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2e288a",
   "metadata": {},
   "source": [
    "Below, `BASE_OUTPUT_DIR/model/MODEL_NAME/MODEL_VERSION` should point to a model created in [training-prebuilt.ipynb](training-prebuilt.ipynb). Change it to any path containing the correct SavedModel structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7550b3d8",
   "metadata": {
    "tags": [
     "flake8-noqa-line-1"
    ]
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = !(gcloud config get-value project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "\n",
    "REGION = \"us-central1\"\n",
    "\n",
    "BUCKET_NAME = PROJECT_ID\n",
    "# Use a regional bucket in the above region you have rights to.\n",
    "# Create if needed:\n",
    "# !gsutil mb -l $REGION gs://$BUCKET_NAME\n",
    "\n",
    "BASE_OUTPUT_DIR = f\"gs://{BUCKET_NAME}\"\n",
    "MODEL_NAME = \"jax_model_prebuilt\"\n",
    "MODEL_VERSION = 1\n",
    "\n",
    "SERVING_BATCH_SIZE = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a47c31a",
   "metadata": {},
   "source": [
    "Check that `BASE_OUTPUT_DIR/model/MODEL_NAME/MODEL_VERSION` actually contains a SavedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35e87641",
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://dsparing-sandbox/model/jax_model_prebuilt/1/\n",
      "gs://dsparing-sandbox/model/jax_model_prebuilt/1/saved_model.pb\n",
      "gs://dsparing-sandbox/model/jax_model_prebuilt/1/assets/\n",
      "gs://dsparing-sandbox/model/jax_model_prebuilt/1/variables/\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls $BASE_OUTPUT_DIR/model/$MODEL_NAME/$MODEL_VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57d4553",
   "metadata": {},
   "source": [
    "## Create TFServing container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27c1ce6",
   "metadata": {},
   "source": [
    "We will simply use the default TensorFlow Serving container image. We still need to build this container, because a Container Registry or Artifact Registry container is expected, so in effect we copy this from Docker Hub.\n",
    "\n",
    "NOTE: serving does not work for GPU yet, the below is a CPU example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddd553d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVING_FOLDER = \"serving\"\n",
    "SERVING_IMAGE_NAME = \"tensorflow-serving\"\n",
    "SERVING_IMAGE_URI = f\"gcr.io/{PROJECT_ID}/{SERVING_IMAGE_NAME}\"\n",
    "\n",
    "USE_GPU_SERVING = False\n",
    "if USE_GPU_SERVING:\n",
    "    TFSERVING_TAG = \"latest-gpu\"\n",
    "else:\n",
    "    TFSERVING_TAG = \"latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0fbd867",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"SERVING_FOLDER\"] = SERVING_FOLDER\n",
    "os.environ[\"TFSERVING_TAG\"] = TFSERVING_TAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac36d4aa",
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p $SERVING_FOLDER\n",
    "cat > $SERVING_FOLDER/Dockerfile << EOF\n",
    "FROM tensorflow/serving:$TFSERVING_TAG\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0290ff97",
   "metadata": {},
   "source": [
    "We only build the container if it is not already available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc1caae9",
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default tag: latest\n",
      "latest: Pulling from dsparing-sandbox/tensorflow-serving\n",
      "Digest: sha256:6651f4839e1124dbde75ee531825112af0a6b8ef082c88ab14ca53eb69a2e4bb\n",
      "Status: Image is up to date for gcr.io/dsparing-sandbox/tensorflow-serving:latest\n",
      "gcr.io/dsparing-sandbox/tensorflow-serving:latest\n"
     ]
    }
   ],
   "source": [
    "!cd $SERVING_FOLDER && docker pull $SERVING_IMAGE_URI || gcloud builds submit --tag $SERVING_IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17a1475",
   "metadata": {},
   "source": [
    "## Try serving locally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65187428",
   "metadata": {},
   "source": [
    "Get image to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbac789c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:absl:Load dataset info from /home/jupyter/tensorflow_datasets/mnist/3.0.1\n",
      "INFO:absl:Field info.citation from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.splits from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.module_name from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Reusing dataset mnist (/home/jupyter/tensorflow_datasets/mnist/3.0.1)\n",
      "INFO:absl:Constructing tf.data.Dataset mnist for split test, from /home/jupyter/tensorflow_datasets/mnist/3.0.1\n"
     ]
    }
   ],
   "source": [
    "# need to initialize flags somehow to avoid errors in load_mnist\n",
    "flags.FLAGS([\"\"])\n",
    "\n",
    "images_to_predict, _ = next(\n",
    "    iter(mnist_lib.load_mnist(tfds.Split.TEST, batch_size=SERVING_BATCH_SIZE))\n",
    ")\n",
    "instances = images_to_predict.numpy().tolist()\n",
    "image_json = json.dumps(instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ed5aaa",
   "metadata": {},
   "source": [
    "Start up container (for GPU it should use `--runtime=nvidia` but for now GPU prediction does not work.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33f726ff",
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default tag: latest\n",
      "latest: Pulling from dsparing-sandbox/tensorflow-serving\n",
      "Digest: sha256:6651f4839e1124dbde75ee531825112af0a6b8ef082c88ab14ca53eb69a2e4bb\n",
      "Status: Image is up to date for gcr.io/dsparing-sandbox/tensorflow-serving:latest\n",
      "gcr.io/dsparing-sandbox/tensorflow-serving:latest\n",
      "27abdc67652dbc1e146d4a919c65e1f38583d65c51ebbd2c44ca5f9a050415e6\n"
     ]
    }
   ],
   "source": [
    "!docker pull $SERVING_IMAGE_URI\n",
    "!docker run -d -p 8501:8501 \\\n",
    "    --name serving_jax \\\n",
    "    $SERVING_IMAGE_URI \\\n",
    "    --xla_cpu_compilation_enabled=true \\\n",
    "    --model_name=$MODEL_NAME \\\n",
    "    --model_base_path=$BASE_OUTPUT_DIR/model/$MODEL_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "765198be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-01 15:05:17.078661: I tensorflow_serving/model_servers/server.cc:89] Building single TensorFlow model file config:  model_name: jax_model_prebuilt model_base_path: gs://dsparing-sandbox/model/jax_model_prebuilt\n",
      "2021-07-01 15:05:17.078916: I tensorflow_serving/model_servers/server_core.cc:465] Adding/updating models.\n",
      "2021-07-01 15:05:17.078941: I tensorflow_serving/model_servers/server_core.cc:591]  (Re-)adding model: jax_model_prebuilt\n",
      "2021-07-01 15:05:18.515660: I tensorflow_serving/core/basic_manager.cc:740] Successfully reserved resources to load servable {name: jax_model_prebuilt version: 1}\n",
      "2021-07-01 15:05:18.515699: I tensorflow_serving/core/loader_harness.cc:66] Approving load for servable version {name: jax_model_prebuilt version: 1}\n",
      "2021-07-01 15:05:18.515741: I tensorflow_serving/core/loader_harness.cc:74] Loading servable version {name: jax_model_prebuilt version: 1}\n",
      "2021-07-01 15:05:18.728470: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:38] Reading SavedModel from: gs://dsparing-sandbox/model/jax_model_prebuilt/1\n",
      "2021-07-01 15:05:18.999227: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:90] Reading meta graph with tags { serve }\n",
      "2021-07-01 15:05:18.999280: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: gs://dsparing-sandbox/model/jax_model_prebuilt/1\n",
      "2021-07-01 15:05:19.140759: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-07-01 15:05:19.177066: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:206] Restoring SavedModel bundle.\n",
      "2021-07-01 15:05:19.217504: I external/org_tensorflow/tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2299995000 Hz\n",
      "2021-07-01 15:05:20.429353: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:190] Running initialization op on SavedModel bundle at path: gs://dsparing-sandbox/model/jax_model_prebuilt/1\n",
      "2021-07-01 15:05:20.435331: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:277] SavedModel load for tags { serve }; Status: success: OK. Took 1706864 microseconds.\n",
      "2021-07-01 15:05:20.610764: I tensorflow_serving/servables/tensorflow/saved_model_warmup_util.cc:59] No warmup data file found at gs://dsparing-sandbox/model/jax_model_prebuilt/1/assets.extra/tf_serving_warmup_requests\n",
      "2021-07-01 15:05:21.338506: I tensorflow_serving/core/loader_harness.cc:87] Successfully loaded servable version {name: jax_model_prebuilt version: 1}\n",
      "2021-07-01 15:05:21.339898: I tensorflow_serving/model_servers/server_core.cc:486] Finished adding/updating models\n",
      "2021-07-01 15:05:21.410798: I tensorflow_serving/model_servers/server.cc:367] Profiler service is enabled\n",
      "2021-07-01 15:05:21.412447: I tensorflow_serving/model_servers/server.cc:393] Running gRPC ModelServer at 0.0.0.0:8500 ...\n",
      "[warn] getaddrinfo: address family for nodename not supported\n",
      "2021-07-01 15:05:21.414983: I tensorflow_serving/model_servers/server.cc:414] Exporting HTTP/REST API at:localhost:8501 ...\n",
      "[evhttp_server.cc : 245] NET_LOG: Entering the event loop ...\n"
     ]
    }
   ],
   "source": [
    "!sleep 20 && docker logs serving_jax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc271809",
   "metadata": {},
   "source": [
    "With the `sleep`, we give it some time for TF Serving to load the model from Cloud Storage and be ready to accept requests. Verify in the log below that it is indeed ready. (Probably printing something like `\"Entering the event loop ...\"`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7162502e",
   "metadata": {},
   "source": [
    "Send prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3801afd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-9.59468746, -6.71643162, -9.19317722, -6.08347225, -9.86418915, -7.86642075, -13.7691307, -0.586410522, -3.49557447, -0.893427849], [-8.17506, -8.27988815, -5.86172485, -2.22976971, -11.3345842, -2.34807348, -9.04989243, -7.64320087, -0.234498978, -6.24677086], [-13.4298983, -25.2465477, -9.84352875, -18.5516396, -10.8125267, -11.114028, -0.000108718872, -25.0840569, -10.8588352, -16.1272583]]\n"
     ]
    }
   ],
   "source": [
    "data = json.dumps({\"instances\": instances})\n",
    "json_response = requests.post(\n",
    "    f\"http://localhost:8501/v1/models/{MODEL_NAME}:predict\",\n",
    "    data=data,\n",
    ")\n",
    "predictions = json.loads(json_response.text)[\"predictions\"]\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd200b4b",
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serving_jax\n"
     ]
    }
   ],
   "source": [
    "!docker rm -f serving_jax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bcfff6",
   "metadata": {},
   "source": [
    "## Upload model to Vertex AI using custom container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "414278c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Creating Model\n",
      "INFO:google.cloud.aiplatform.models:Create Model backing LRO: projects/654544512569/locations/us-central1/models/2423649083060125696/operations/6377785641513517056\n",
      "INFO:google.cloud.aiplatform.models:Model created. Resource name: projects/654544512569/locations/us-central1/models/2423649083060125696\n",
      "INFO:google.cloud.aiplatform.models:To use this Model in another session:\n",
      "INFO:google.cloud.aiplatform.models:model = aiplatform.Model('projects/654544512569/locations/us-central1/models/2423649083060125696')\n",
      "jax_model_prebuilt projects/654544512569/locations/us-central1/models/2423649083060125696\n"
     ]
    }
   ],
   "source": [
    "model = aiplatform.Model.upload(\n",
    "    display_name=MODEL_NAME,\n",
    "    serving_container_image_uri=SERVING_IMAGE_URI,\n",
    "    artifact_uri=os.path.join(BASE_OUTPUT_DIR, \"model\", MODEL_NAME),\n",
    "    serving_container_predict_route=f\"/v1/models/{MODEL_NAME}:predict\",\n",
    "    serving_container_health_route=f\"/v1/models/{MODEL_NAME}\",\n",
    "    serving_container_args=[\n",
    "        \"--xla_cpu_compilation_enabled=true\",\n",
    "        f\"--model_name={MODEL_NAME}\",\n",
    "        f\"--model_base_path=$(AIP_STORAGE_URI)\",\n",
    "    ],\n",
    "    serving_container_ports=[8501],\n",
    ")\n",
    "\n",
    "print(model.display_name, model.resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc6e111",
   "metadata": {},
   "source": [
    "You need to note the model resource name (as a unique identifier for your model) for prediction later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b0a034",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-5.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m74"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
