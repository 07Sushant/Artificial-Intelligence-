{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train JAX/Flax model on Vertex AI custom container and use `jax2tf` to convert to SavedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "v-kebtVqdEyV"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from absl import flags\n",
    "from google.cloud import aiplatform\n",
    "from jax.experimental.jax2tf.examples import mnist_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "W-IAEp1UqmKR"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "\n",
    "REGION = \"us-central1\"\n",
    "os.environ['REGION'] = REGION\n",
    "\n",
    "BUCKET_NAME = PROJECT_ID\n",
    "os.environ['BUCKET_NAME'] = BUCKET_NAME\n",
    "# Use a regional bucket in the above region you have rights to.\n",
    "# Create if needed:\n",
    "# !gsutil mb -l ${REGION} gs://${BUCKET_NAME}\n",
    "\n",
    "USE_GPU = True\n",
    "\n",
    "TRAINING_APP_FOLDER = 'training_app'\n",
    "os.environ['TRAINING_APP_FOLDER'] = TRAINING_APP_FOLDER\n",
    "\n",
    "MODEL_BASE_PATH = f\"gs://{BUCKET_NAME}/savedmodels\"\n",
    "MODEL_NAME = \"jax_model_customcontainer\"\n",
    "MODEL_VERSION = 1\n",
    "os.environ['MODEL_NAME'] = MODEL_NAME\n",
    "\n",
    "# Block TF from the GPU to let JAX use it all\n",
    "tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Copyright 2021 Google LLC\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     https://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "import argparse\n",
      "import logging\n",
      "import os\n",
      "\n",
      "import tensorflow as tf\n",
      "import tensorflow_datasets as tfds\n",
      "from absl import flags\n",
      "from jax.experimental.jax2tf.examples import mnist_lib\n",
      "from jax.experimental.jax2tf.examples import saved_model_lib \n",
      "\n",
      "TRAIN_BATCH_SIZE = 128\n",
      "TEST_BATCH_SIZE = 16\n",
      "NUM_EPOCHS = 2\n",
      "\n",
      "# Block TF from the GPU to let JAX use it all\n",
      "tf.config.set_visible_devices([], 'GPU')\n",
      "\n",
      "logger = logging.getLogger()\n",
      "\n",
      "# need to initialize flags somehow to avoid errors in load_mnist\n",
      "flags.FLAGS([''])\n",
      "\n",
      "flax_mnist = mnist_lib.FlaxMNIST()\n",
      "\n",
      "train_ds = mnist_lib.load_mnist(tfds.Split.TRAIN, TRAIN_BATCH_SIZE)\n",
      "test_ds = mnist_lib.load_mnist(tfds.Split.TEST, TEST_BATCH_SIZE)\n",
      "\n",
      "image, _ = next(iter(train_ds))\n",
      "input_signature = tf.TensorSpec.from_tensor(\n",
      "    tf.expand_dims(image[0], axis=0)\n",
      ")\n",
      "\n",
      "\n",
      "def main(args):\n",
      "    logger_level = logger.level\n",
      "    logger.setLevel(logging.INFO)\n",
      "    predict_fn, params = flax_mnist.train(\n",
      "        train_ds=train_ds,\n",
      "        test_ds=test_ds,\n",
      "        num_epochs=NUM_EPOCHS,\n",
      "    )\n",
      "    logger.setLevel(logger_level)\n",
      "    \n",
      "    saved_model_lib.convert_and_save_model(\n",
      "        jax_fn=predict_fn,\n",
      "        params=params,\n",
      "        model_dir=os.path.join(args[\"output_dir\"], str(args[\"model_version\"])),\n",
      "        input_signatures=[input_signature],\n",
      "    )\n",
      "        \n",
      "\n",
      "if __name__ == '__main__':\n",
      "    parser = argparse.ArgumentParser()\n",
      "    parser.add_argument(\n",
      "        \"--output_dir\",\n",
      "        help=\"GCS location to export model_version/SavedModel\",\n",
      "        default=os.getenv(\"AIP_MODEL_DIR\")\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \"--model_version\",\n",
      "        default=1,\n",
      "        type=int\n",
      "    )\n",
      "\n",
      "    args = parser.parse_args().__dict__\n",
      "\n",
      "    main(args)\n"
     ]
    }
   ],
   "source": [
    "!cat $TRAINING_APP_FOLDER/trainer/task.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should be able to use [CustomContainerTrainingJob](https://googleapis.dev/python/aiplatform/latest/aiplatform.html#google.cloud.aiplatform.CustomContainerTrainingJob), but it gives an error, see the similar [CustomTrainingJob.run](https://googleapis.dev/python/aiplatform/latest/aiplatform.html#google.cloud.aiplatform.CustomTrainingJob.run) currently giving an error even when using the [official notebook](https://github.com/GoogleCloudPlatform/ai-platform-samples/blob/master/ai-platform-unified/notebooks/official/custom/sdk-custom-image-classification-online.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_GPU:\n",
    "    BASE_TRAINING_IMAGE = 'gcr.io/deeplearning-platform-release/tf2-gpu.2-5'\n",
    "    TRAINING_IMAGE_NAME = 'jax_vertex_training_gpu'\n",
    "else:\n",
    "    BASE_TRAINING_IMAGE = 'gcr.io/deeplearning-platform-release/tf2-cpu.2-5'    \n",
    "    TRAINING_IMAGE_NAME = 'jax_vertex_training_cpu'\n",
    "\n",
    "SERVING_IMAGE_NAME = 'jax_vertex_prediction'\n",
    "\n",
    "os.environ['BASE_TRAINING_IMAGE'] = BASE_TRAINING_IMAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write a `requirements.txt` and a `Dockerfile` that defines our custom container based on a [Deep Learning Container image](https://cloud.google.com/deep-learning-containers/docs/choosing-container#container_images), including the `pip install` of the required packages, copy of the model training code, and the Entrypoint launching our training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flax\n",
      "jax[cuda111]  # needs pip to run with `-f https://storage.googleapis.com/jax-releases/jax_releases.html`\n"
     ]
    }
   ],
   "source": [
    "!cat $TRAINING_APP_FOLDER/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 458,
     "status": "ok",
     "timestamp": 1617197650331,
     "user": {
      "displayName": "Daniel Sparing",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GimB4WP4BC_SgKVqyvIS8u7WFJXnJGMSuyFNS4Lww=s64",
      "userId": "02797666024167492243"
     },
     "user_tz": 300
    },
    "id": "WlzDz-6wdpNC",
    "outputId": "9404061c-e26c-42e3-9b2a-bce48767be1f",
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat > $TRAINING_APP_FOLDER/Dockerfile << EOF\n",
    "FROM $BASE_TRAINING_IMAGE\n",
    "\n",
    "COPY requirements.txt .\n",
    "RUN python3 -m pip install -r requirements.txt \\\n",
    "    -f https://storage.googleapis.com/jax-releases/jax_releases.html \n",
    "\n",
    "WORKDIR /app\n",
    "COPY trainer/task.py .\n",
    "\n",
    "ENTRYPOINT [\"python\", \"task.py\"]\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "FZ7Eh2Wwfc9Y"
   },
   "outputs": [],
   "source": [
    "IMAGE_TAG = 'latest'\n",
    "TRAINING_IMAGE_URI = 'gcr.io/{}/{}:{}'.format(PROJECT_ID, TRAINING_IMAGE_NAME, IMAGE_TAG)\n",
    "os.environ['TRAINING_IMAGE_URI'] = TRAINING_IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1184,
     "status": "ok",
     "timestamp": 1617197720314,
     "user": {
      "displayName": "Daniel Sparing",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GimB4WP4BC_SgKVqyvIS8u7WFJXnJGMSuyFNS4Lww=s64",
      "userId": "02797666024167492243"
     },
     "user_tz": 300
    },
    "id": "JhHx_WUIfs6j",
    "outputId": "418935e1-58c0-4812-f820-c09244711048"
   },
   "outputs": [],
   "source": [
    "!cd $TRAINING_APP_FOLDER && \\\n",
    "    gcloud builds submit --tag $TRAINING_IMAGE_URI --timeout=\"30m\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test training container locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "executionInfo": {
     "elapsed": 367,
     "status": "error",
     "timestamp": 1617776619162,
     "user": {
      "displayName": "Daniel Sparing",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GimB4WP4BC_SgKVqyvIS8u7WFJXnJGMSuyFNS4Lww=s64",
      "userId": "02797666024167492243"
     },
     "user_tz": 300
    },
    "id": "X0rCYgF6_63Y",
    "outputId": "8d917d99-39db-4ae9-cefc-ed5b6ad74289",
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latest: Pulling from dsparing-sandbox/jax_vertex_training_gpu\n",
      "Digest: sha256:66ee236080795e5f5a856feda1dbcf541e955d5d8ee8a5368478b05e81cf8ac6\n",
      "Status: Image is up to date for gcr.io/dsparing-sandbox/jax_vertex_training_gpu:latest\n",
      "gcr.io/dsparing-sandbox/jax_vertex_training_gpu:latest\n",
      "2021-06-28 07:55:27.198710: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-06-28 07:55:31.007997: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2021-06-28 07:55:31.015923: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-28 07:55:31.016621: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
      "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\n",
      "2021-06-28 07:55:31.016677: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-06-28 07:55:31.044711: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2021-06-28 07:55:31.044837: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2021-06-28 07:55:31.117960: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
      "2021-06-28 07:55:31.118575: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
      "2021-06-28 07:55:31.137537: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-06-28 07:55:31.138648: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
      "2021-06-28 07:55:31.139061: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
      "2021-06-28 07:55:31.139316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-28 07:55:31.140258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-28 07:55:31.140890: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
      "Dl Completed...: 100%|██████████| 4/4 [00:00<00:00, 16.70 file/s]2021-06-28 07:55:31.890451: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-06-28 07:55:31.890733: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-06-28 07:55:31.890770: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      \n",
      "2021-06-28 07:55:32.036873: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-06-28 07:55:32.037574: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2299995000 Hz\n",
      "2021-06-28 07:55:32.199272: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2021-06-28 07:55:32.199673: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "\n",
      "INFO:absl:Starting the local TPU driver.\n",
      "INFO:absl:Unable to initialize backend 'tpu_driver': Not found: Unable to find driver in registry given worker: local://\n",
      "INFO:absl:Unable to initialize backend 'tpu': Invalid argument: TpuPlatform is not available.\n",
      "INFO:root:mnist_flax: Epoch 0 in 6.70 sec\n",
      "INFO:root:mnist_flax: Training set accuracy 88.06%\n",
      "INFO:root:mnist_flax: Test set accuracy 88.65%\n",
      "INFO:root:mnist_flax: Epoch 1 in 1.15 sec\n",
      "INFO:root:mnist_flax: Training set accuracy 90.81%\n",
      "INFO:root:mnist_flax: Test set accuracy 91.34%\n",
      "WARNING:tensorflow:Mapping types may not work well with tf.nest. Prefer using MutableMapping for <class 'flax.core.frozen_dict.FrozenDict'>\n",
      "WARNING:tensorflow:Mapping types may not work well with tf.nest. Prefer using MutableMapping for <class 'flax.core.frozen_dict.FrozenDict'>\n",
      "\u001b[1mDownloading and preparing dataset 11.06 MiB (download: 11.06 MiB, generated: 21.00 MiB, total: 32.06 MiB) to /root/tensorflow_datasets/mnist/3.0.1...\u001b[0m\n",
      "\u001b[1mDataset mnist downloaded and prepared to /root/tensorflow_datasets/mnist/3.0.1. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!docker pull $TRAINING_IMAGE_URI \n",
    "!docker run \\\n",
    "    --name training_jax \\\n",
    "    --runtime nvidia \\\n",
    "    $TRAINING_IMAGE_URI \\\n",
    "    --output_dir=$MODEL_BASE_PATH/\"$MODEL_NAME\"_local \\\n",
    "    --model_version=$MODEL_VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "once the above container run finished:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_jax\n"
     ]
    }
   ],
   "source": [
    "!docker rm -f training_jax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create custom container for prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we below use `CustomContainerTrainingJob.run` to submit the training job, we can specify a prediction container environment as well. If we do so, the model artifact will not only be saved to cloud storage, but will also be uploaded to Vertex AI ready for batch prediction requests or ready to be deployed to an endpoint for online prediction.\n",
    "\n",
    "Therefore we specify a custom prediction container now. (If we didn't, we could still call `CustomContainerTrainingJob.run`, but without the `model_serving_container_*` arguments, and the model training job would finish at storing the artifact in Cloud Storage.)\n",
    "\n",
    "We will simply use the default TensorFlow Serving container image. We still need to build this container, because a Container Registry or Artifact Registry container is expected, so in effect we copy this from Docker Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVING_FOLDER = 'serving'\n",
    "\n",
    "USE_GPU_SERVING = False\n",
    "if USE_GPU_SERVING:\n",
    "    TFSERVING_TAG = \"latest-gpu\"\n",
    "else:\n",
    "    TFSERVING_TAG = \"latest\"\n",
    "\n",
    "os.environ['SERVING_FOLDER'] = SERVING_FOLDER\n",
    "os.environ['TFSERVING_TAG'] = TFSERVING_TAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p $SERVING_FOLDER\n",
    "cat > $SERVING_FOLDER/Dockerfile << EOF\n",
    "FROM tensorflow/serving:$TFSERVING_TAG\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_TAG = 'latest'\n",
    "SERVING_IMAGE_URI = 'gcr.io/{}/{}:{}'.format(PROJECT_ID, SERVING_IMAGE_NAME, IMAGE_TAG)\n",
    "os.environ['SERVING_IMAGE_URI'] = SERVING_IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 1 file(s) totalling 31 bytes before compression.\n",
      "Uploading tarball of [.] to [gs://dsparing-sandbox_cloudbuild/source/1624866962.12636-9a9c9c5ba19b421dbdcdd55bd1e58da9.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/dsparing-sandbox/locations/global/builds/77fedb4a-bb0e-48dc-b110-791a9a17277e].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/77fedb4a-bb0e-48dc-b110-791a9a17277e?project=654544512569].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"77fedb4a-bb0e-48dc-b110-791a9a17277e\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://dsparing-sandbox_cloudbuild/source/1624866962.12636-9a9c9c5ba19b421dbdcdd55bd1e58da9.tgz#1624866962353729\n",
      "Copying gs://dsparing-sandbox_cloudbuild/source/1624866962.12636-9a9c9c5ba19b421dbdcdd55bd1e58da9.tgz#1624866962353729...\n",
      "/ [1 files][  198.0 B/  198.0 B]                                                \n",
      "Operation completed over 1 objects/198.0 B.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon   2.56kB\n",
      "Step 1/1 : FROM tensorflow/serving:latest\n",
      "latest: Pulling from tensorflow/serving\n",
      "01bf7da0a88c: Pulling fs layer\n",
      "f3b4a5f15c7a: Pulling fs layer\n",
      "57ffbe87baa1: Pulling fs layer\n",
      "e72e6208e893: Pulling fs layer\n",
      "6ea3f464ef73: Pulling fs layer\n",
      "01e9bf86544b: Pulling fs layer\n",
      "68f6bba3dc50: Pulling fs layer\n",
      "6ea3f464ef73: Waiting\n",
      "01e9bf86544b: Waiting\n",
      "68f6bba3dc50: Waiting\n",
      "e72e6208e893: Waiting\n",
      "57ffbe87baa1: Download complete\n",
      "f3b4a5f15c7a: Verifying Checksum\n",
      "f3b4a5f15c7a: Download complete\n",
      "e72e6208e893: Verifying Checksum\n",
      "e72e6208e893: Download complete\n",
      "01e9bf86544b: Verifying Checksum\n",
      "01e9bf86544b: Download complete\n",
      "68f6bba3dc50: Verifying Checksum\n",
      "68f6bba3dc50: Download complete\n",
      "01bf7da0a88c: Verifying Checksum\n",
      "01bf7da0a88c: Download complete\n",
      "6ea3f464ef73: Verifying Checksum\n",
      "6ea3f464ef73: Download complete\n",
      "01bf7da0a88c: Pull complete\n",
      "f3b4a5f15c7a: Pull complete\n",
      "57ffbe87baa1: Pull complete\n",
      "e72e6208e893: Pull complete\n",
      "6ea3f464ef73: Pull complete\n",
      "01e9bf86544b: Pull complete\n",
      "68f6bba3dc50: Pull complete\n",
      "Digest: sha256:6651f4839e1124dbde75ee531825112af0a6b8ef082c88ab14ca53eb69a2e4bb\n",
      "Status: Downloaded newer image for tensorflow/serving:latest\n",
      " ---> e874bf5e4700\n",
      "Successfully built e874bf5e4700\n",
      "Successfully tagged gcr.io/dsparing-sandbox/jax_vertex_prediction:latest\n",
      "PUSH\n",
      "Pushing gcr.io/dsparing-sandbox/jax_vertex_prediction:latest\n",
      "The push refers to repository [gcr.io/dsparing-sandbox/jax_vertex_prediction]\n",
      "bb4423850a27: Preparing\n",
      "b60ba33781cd: Preparing\n",
      "547f89523b17: Preparing\n",
      "bd91f28d5f3c: Preparing\n",
      "8cafc6d2db45: Preparing\n",
      "a5d4bacb0351: Preparing\n",
      "5153e1acaabc: Preparing\n",
      "a5d4bacb0351: Waiting\n",
      "5153e1acaabc: Waiting\n",
      "8cafc6d2db45: Layer already exists\n",
      "bb4423850a27: Layer already exists\n",
      "bd91f28d5f3c: Layer already exists\n",
      "b60ba33781cd: Layer already exists\n",
      "547f89523b17: Layer already exists\n",
      "a5d4bacb0351: Layer already exists\n",
      "5153e1acaabc: Layer already exists\n",
      "latest: digest: sha256:6651f4839e1124dbde75ee531825112af0a6b8ef082c88ab14ca53eb69a2e4bb size: 1780\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                         IMAGES                                                   STATUS\n",
      "77fedb4a-bb0e-48dc-b110-791a9a17277e  2021-06-28T07:56:02+00:00  18S       gs://dsparing-sandbox_cloudbuild/source/1624866962.12636-9a9c9c5ba19b421dbdcdd55bd1e58da9.tgz  gcr.io/dsparing-sandbox/jax_vertex_prediction (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!cd serving && gcloud builds submit --tag $SERVING_IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run custom training job with custom container on Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below CustomContainerTrainingJob.run in theory also uploads the Model to Vertex AI. We will ignore this and only use this method to store the SavedModel on GCS, because we cannot use a pre-built container\n",
    "\n",
    "(\"Vertex AI Prediction supports pre-built containers, with no additional customization such as args (\"\"Do not specify any other subfields of containerSpec\"\" [source](https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers#using_a_pre-built_container)), and custom containers. As we need the `--xla_cpu_compilation_enabled` arg, we can only use custom containers.)\n",
    "\n",
    "and because we don't have a custom prediction container on gcr.io yet. (if we had one, we could use that. but we can't directly use docker hub, where tensorflow/serving is.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "JOB_NAME = 'jax_customcontainer_training'\n",
    "\n",
    "job = aiplatform.CustomContainerTrainingJob(\n",
    "    display_name=JOB_NAME,\n",
    "    container_uri=TRAINING_IMAGE_URI,\n",
    "    model_serving_container_image_uri=SERVING_IMAGE_URI,\n",
    "    model_serving_container_predict_route=f\"/v1/models/model:predict\",\n",
    "    model_serving_container_health_route=f\"/v1/models/model\",\n",
    "    model_serving_container_args=[\n",
    "        '--xla_cpu_compilation_enabled=true',\n",
    "        '--model_base_path=$(AIP_STORAGE_URI)',\n",
    "    ],\n",
    "    staging_bucket=f\"gs://{BUCKET_NAME}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.training_jobs:Training Output directory:\n",
      "gs://dsparing-sandbox/savedmodels/jax_model_customcontainer \n",
      "INFO:google.cloud.aiplatform.training_jobs:View Training:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/5787569723868708864?project=654544512569\n",
      "INFO:google.cloud.aiplatform.training_jobs:View backing custom job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/66168609759559680?project=654544512569\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob projects/654544512569/locations/us-central1/trainingPipelines/5787569723868708864 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob projects/654544512569/locations/us-central1/trainingPipelines/5787569723868708864 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob projects/654544512569/locations/us-central1/trainingPipelines/5787569723868708864 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob projects/654544512569/locations/us-central1/trainingPipelines/5787569723868708864 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob projects/654544512569/locations/us-central1/trainingPipelines/5787569723868708864 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob projects/654544512569/locations/us-central1/trainingPipelines/5787569723868708864 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob projects/654544512569/locations/us-central1/trainingPipelines/5787569723868708864 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob projects/654544512569/locations/us-central1/trainingPipelines/5787569723868708864 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob run completed. Resource name: projects/654544512569/locations/us-central1/trainingPipelines/5787569723868708864\n",
      "INFO:google.cloud.aiplatform.training_jobs:Model available at projects/654544512569/locations/us-central1/models/5405594986332815360\n"
     ]
    }
   ],
   "source": [
    "REPLICA_COUNT = 1\n",
    "\n",
    "if USE_GPU:\n",
    "    ACCELERATOR_TYPE = \"NVIDIA_TESLA_T4\"\n",
    "    ACCELERATOR_COUNT = 1\n",
    "else:\n",
    "    ACCELERATOR_TYPE = \"ACCELERATOR_TYPE_UNSPECIFIED\"\n",
    "    ACCELERATOR_COUNT = None   \n",
    "\n",
    "model = job.run(\n",
    "    model_display_name=MODEL_NAME,\n",
    "    base_output_dir=os.path.join(MODEL_BASE_PATH, MODEL_NAME),\n",
    "    replica_count=REPLICA_COUNT,\n",
    "    accelerator_count=ACCELERATOR_COUNT,\n",
    "    accelerator_type=ACCELERATOR_TYPE,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local prediction with SavedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0  2021-06-28T08:10:22Z  gs://dsparing-sandbox/savedmodels/jax_model_customcontainer/model/1/\n",
      "     53991  2021-06-28T08:10:24Z  gs://dsparing-sandbox/savedmodels/jax_model_customcontainer/model/1/saved_model.pb\n",
      "                                 gs://dsparing-sandbox/savedmodels/jax_model_customcontainer/model/1/assets/\n",
      "                                 gs://dsparing-sandbox/savedmodels/jax_model_customcontainer/model/1/variables/\n",
      "TOTAL: 2 objects, 53991 bytes (52.73 KiB)\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls -l $MODEL_BASE_PATH/$MODEL_NAME/model/$MODEL_VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:absl:Load dataset info from /home/jupyter/tensorflow_datasets/mnist/3.0.1\n",
      "INFO:absl:Field info.citation from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.splits from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.module_name from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Reusing dataset mnist (/home/jupyter/tensorflow_datasets/mnist/3.0.1)\n",
      "INFO:absl:Constructing tf.data.Dataset mnist for split test, from /home/jupyter/tensorflow_datasets/mnist/3.0.1\n"
     ]
    }
   ],
   "source": [
    "# need to initialize flags somehow to avoid errors in load_mnist\n",
    "flags.FLAGS([''])\n",
    "\n",
    "image_to_predict, _ = next(iter(mnist_lib.load_mnist(tfds.Split.TEST, batch_size=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_0': <tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
       " array([[-1.2814110e+01, -8.4742651e+00, -4.9168525e+00, -1.0399423e+01,\n",
       "         -8.9546528e+00, -9.5392723e+00, -8.5732741e-03, -1.7909704e+01,\n",
       "         -7.1681709e+00, -1.4106701e+01]], dtype=float32)>}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = tf.saved_model.load(f\"{MODEL_BASE_PATH}/{MODEL_NAME}/model/{MODEL_VERSION}\")\n",
    "loaded_model.signatures[\"serving_default\"](image_to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "last_runtime": {
    "build_target": "//learning/deepmind/public/tools/ml_python:ml_notebook",
    "kind": "private"
   },
   "name": "JAX Flax MNIST containerize",
   "provenance": [
    {
     "file_id": "1HenHxSnSqNQPqMPZNG_uhFampEPxN_Cj",
     "timestamp": 1617196427779
    }
   ]
  },
  "environment": {
   "name": "tf2-gpu.2-5.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m74"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
