{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train JAX/Flax model on Vertex AI custom container and use `jax2tf` to convert to SavedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "v-kebtVqdEyV"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from absl import flags\n",
    "from google.cloud import aiplatform\n",
    "from jax.experimental.jax2tf.examples import mnist_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "W-IAEp1UqmKR",
    "tags": [
     "flake8-noqa-line-1"
    ]
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "\n",
    "REGION = \"us-central1\"\n",
    "os.environ['REGION'] = REGION\n",
    "\n",
    "BUCKET_NAME = PROJECT_ID\n",
    "os.environ['BUCKET_NAME'] = BUCKET_NAME\n",
    "# Use a regional bucket in the above region you have rights to.\n",
    "# Create if needed:\n",
    "# !gsutil mb -l ${REGION} gs://${BUCKET_NAME}\n",
    "\n",
    "USE_GPU = True\n",
    "\n",
    "TRAINING_APP_FOLDER = 'training_app'\n",
    "os.environ['TRAINING_APP_FOLDER'] = TRAINING_APP_FOLDER\n",
    "\n",
    "MODEL_BASE_PATH = f\"gs://{BUCKET_NAME}/model\"\n",
    "MODEL_NAME = \"jax_model_customcontainer\"\n",
    "MODEL_VERSION = 1\n",
    "os.environ['MODEL_NAME'] = MODEL_NAME\n",
    "\n",
    "# Block TF from the GPU to let JAX use it all\n",
    "tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Copyright 2021 Google LLC\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     https://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "import argparse\n",
      "import logging\n",
      "import os\n",
      "\n",
      "import tensorflow as tf\n",
      "import tensorflow_datasets as tfds\n",
      "from absl import flags\n",
      "from jax.experimental.jax2tf.examples import mnist_lib\n",
      "from jax.experimental.jax2tf.examples import saved_model_lib\n",
      "\n",
      "TRAIN_BATCH_SIZE = 128\n",
      "TEST_BATCH_SIZE = 16\n",
      "NUM_EPOCHS = 2\n",
      "\n",
      "# Block TF from the GPU to let JAX use it all\n",
      "tf.config.set_visible_devices([], 'GPU')\n",
      "\n",
      "logger = logging.getLogger()\n",
      "\n",
      "# need to initialize flags somehow to avoid errors in load_mnist\n",
      "flags.FLAGS([''])\n",
      "\n",
      "flax_mnist = mnist_lib.FlaxMNIST()\n",
      "\n",
      "train_ds = mnist_lib.load_mnist(tfds.Split.TRAIN, TRAIN_BATCH_SIZE)\n",
      "test_ds = mnist_lib.load_mnist(tfds.Split.TEST, TEST_BATCH_SIZE)\n",
      "\n",
      "image, _ = next(iter(train_ds))\n",
      "input_signature = tf.TensorSpec.from_tensor(\n",
      "    tf.expand_dims(image[0], axis=0)\n",
      ")\n",
      "\n",
      "\n",
      "def main(args):\n",
      "    logger_level = logger.level\n",
      "    logger.setLevel(logging.INFO)\n",
      "    predict_fn, params = flax_mnist.train(\n",
      "        train_ds=train_ds,\n",
      "        test_ds=test_ds,\n",
      "        num_epochs=NUM_EPOCHS,\n",
      "    )\n",
      "    logger.setLevel(logger_level)\n",
      "\n",
      "    saved_model_lib.convert_and_save_model(\n",
      "        jax_fn=predict_fn,\n",
      "        params=params,\n",
      "        model_dir=os.path.join(\n",
      "            args[\"output_dir\"],\n",
      "            \"model\",\n",
      "            str(args[\"model_version\"])\n",
      "        ),\n",
      "        input_signatures=[input_signature],\n",
      "    )\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    parser = argparse.ArgumentParser()\n",
      "    parser.add_argument(\n",
      "        \"--output_dir\",\n",
      "        help=\"GCS location to export model_version/SavedModel\",\n",
      "        default=os.getenv(\"AIP_MODEL_DIR\")\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \"--model_version\",\n",
      "        default=1,\n",
      "        type=int\n",
      "    )\n",
      "\n",
      "    args = parser.parse_args().__dict__\n",
      "\n",
      "    main(args=args)\n"
     ]
    }
   ],
   "source": [
    "!cat $TRAINING_APP_FOLDER/trainer/task.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should be able to use [CustomContainerTrainingJob](https://googleapis.dev/python/aiplatform/latest/aiplatform.html#google.cloud.aiplatform.CustomContainerTrainingJob), but it gives an error, see the similar [CustomTrainingJob.run](https://googleapis.dev/python/aiplatform/latest/aiplatform.html#google.cloud.aiplatform.CustomTrainingJob.run) currently giving an error even when using the [official notebook](https://github.com/GoogleCloudPlatform/ai-platform-samples/blob/master/ai-platform-unified/notebooks/official/custom/sdk-custom-image-classification-online.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_GPU:\n",
    "    BASE_TRAINING_IMAGE = \"gcr.io/deeplearning-platform-release/tf2-gpu.2-5\"\n",
    "    TRAINING_IMAGE_NAME = \"jax_vertex_training_gpu\"\n",
    "else:\n",
    "    BASE_TRAINING_IMAGE = \"gcr.io/deeplearning-platform-release/tf2-cpu.2-5\"\n",
    "    TRAINING_IMAGE_NAME = \"jax_vertex_training_cpu\"\n",
    "\n",
    "SERVING_IMAGE_NAME = \"jax_vertex_prediction\"\n",
    "\n",
    "os.environ[\"BASE_TRAINING_IMAGE\"] = BASE_TRAINING_IMAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write a `requirements.txt` and a `Dockerfile` that defines our custom container based on a [Deep Learning Container image](https://cloud.google.com/deep-learning-containers/docs/choosing-container#container_images), including the `pip install` of the required packages, copy of the model training code, and the Entrypoint launching our training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flax\n",
      "jax[cuda111]  # needs pip to run with `-f https://storage.googleapis.com/jax-releases/jax_releases.html`\n"
     ]
    }
   ],
   "source": [
    "!cat $TRAINING_APP_FOLDER/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 458,
     "status": "ok",
     "timestamp": 1617197650331,
     "user": {
      "displayName": "Daniel Sparing",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GimB4WP4BC_SgKVqyvIS8u7WFJXnJGMSuyFNS4Lww=s64",
      "userId": "02797666024167492243"
     },
     "user_tz": 300
    },
    "id": "WlzDz-6wdpNC",
    "outputId": "9404061c-e26c-42e3-9b2a-bce48767be1f",
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat > $TRAINING_APP_FOLDER/Dockerfile << EOF\n",
    "FROM $BASE_TRAINING_IMAGE\n",
    "\n",
    "COPY requirements.txt .\n",
    "RUN python3 -m pip install -r requirements.txt \\\n",
    "    -f https://storage.googleapis.com/jax-releases/jax_releases.html \n",
    "\n",
    "WORKDIR /app\n",
    "COPY trainer/task.py .\n",
    "\n",
    "ENTRYPOINT [\"python\", \"task.py\"]\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "FZ7Eh2Wwfc9Y"
   },
   "outputs": [],
   "source": [
    "IMAGE_TAG = \"latest\"\n",
    "TRAINING_IMAGE_URI = f\"gcr.io/{PROJECT_ID}/{TRAINING_IMAGE_NAME}:{IMAGE_TAG}\"\n",
    "os.environ[\"TRAINING_IMAGE_URI\"] = TRAINING_IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1184,
     "status": "ok",
     "timestamp": 1617197720314,
     "user": {
      "displayName": "Daniel Sparing",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GimB4WP4BC_SgKVqyvIS8u7WFJXnJGMSuyFNS4Lww=s64",
      "userId": "02797666024167492243"
     },
     "user_tz": 300
    },
    "id": "JhHx_WUIfs6j",
    "outputId": "418935e1-58c0-4812-f820-c09244711048",
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 22 file(s) totalling 40.9 KiB before compression.\n",
      "Uploading tarball of [.] to [gs://dsparing-sandbox_cloudbuild/source/1624943581.990537-a669c12fcda948ae96704babd11bbcc2.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/dsparing-sandbox/locations/global/builds/391e40f5-5136-4803-a483-a8ae876ebd41].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/391e40f5-5136-4803-a483-a8ae876ebd41?project=654544512569].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"391e40f5-5136-4803-a483-a8ae876ebd41\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://dsparing-sandbox_cloudbuild/source/1624943581.990537-a669c12fcda948ae96704babd11bbcc2.tgz#1624943582218159\n",
      "Copying gs://dsparing-sandbox_cloudbuild/source/1624943581.990537-a669c12fcda948ae96704babd11bbcc2.tgz#1624943582218159...\n",
      "/ [1 files][ 12.7 KiB/ 12.7 KiB]                                                \n",
      "Operation completed over 1 objects/12.7 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  62.46kB\n",
      "Step 1/6 : FROM gcr.io/deeplearning-platform-release/tf2-gpu.2-5\n",
      "latest: Pulling from deeplearning-platform-release/tf2-gpu.2-5\n",
      "4bbfd2c87b75: Pulling fs layer\n",
      "d2e110be24e1: Pulling fs layer\n",
      "889a7173dcfe: Pulling fs layer\n",
      "e1948de7822d: Pulling fs layer\n",
      "9c8e4ac0274d: Pulling fs layer\n",
      "29dcb86d08de: Pulling fs layer\n",
      "70d8019dd5e8: Pulling fs layer\n",
      "5ccc73e465ef: Pulling fs layer\n",
      "e9d8630baacd: Pulling fs layer\n",
      "82ad86c72f57: Pulling fs layer\n",
      "a6746fce16a1: Pulling fs layer\n",
      "7a67c64e20d2: Pulling fs layer\n",
      "334312f3cce5: Pulling fs layer\n",
      "d1c06a369ea4: Pulling fs layer\n",
      "61aa2ea143ea: Pulling fs layer\n",
      "87555fa6733c: Pulling fs layer\n",
      "dcfc4adad992: Pulling fs layer\n",
      "dcb1b56a4779: Pulling fs layer\n",
      "95837e5e0af5: Pulling fs layer\n",
      "9fe1d9bf08cb: Pulling fs layer\n",
      "1cc20834967b: Pulling fs layer\n",
      "f492fb29e345: Pulling fs layer\n",
      "b1c9ec7e36f6: Pulling fs layer\n",
      "94e3f0ba3fb3: Pulling fs layer\n",
      "00f212e657e4: Pulling fs layer\n",
      "7d32fad557e1: Pulling fs layer\n",
      "4072293fd93e: Pulling fs layer\n",
      "27d78ef0086e: Pulling fs layer\n",
      "c733e8557bb1: Pulling fs layer\n",
      "df0cdf11e45e: Pulling fs layer\n",
      "3327ae24303f: Pulling fs layer\n",
      "3efe73011a96: Pulling fs layer\n",
      "e1948de7822d: Waiting\n",
      "9c8e4ac0274d: Waiting\n",
      "29dcb86d08de: Waiting\n",
      "70d8019dd5e8: Waiting\n",
      "5ccc73e465ef: Waiting\n",
      "e9d8630baacd: Waiting\n",
      "82ad86c72f57: Waiting\n",
      "a6746fce16a1: Waiting\n",
      "7a67c64e20d2: Waiting\n",
      "334312f3cce5: Waiting\n",
      "d1c06a369ea4: Waiting\n",
      "61aa2ea143ea: Waiting\n",
      "87555fa6733c: Waiting\n",
      "dcfc4adad992: Waiting\n",
      "dcb1b56a4779: Waiting\n",
      "95837e5e0af5: Waiting\n",
      "9fe1d9bf08cb: Waiting\n",
      "1cc20834967b: Waiting\n",
      "f492fb29e345: Waiting\n",
      "b1c9ec7e36f6: Waiting\n",
      "94e3f0ba3fb3: Waiting\n",
      "00f212e657e4: Waiting\n",
      "7d32fad557e1: Waiting\n",
      "4072293fd93e: Waiting\n",
      "27d78ef0086e: Waiting\n",
      "c733e8557bb1: Waiting\n",
      "df0cdf11e45e: Waiting\n",
      "3327ae24303f: Waiting\n",
      "3efe73011a96: Waiting\n",
      "d2e110be24e1: Verifying Checksum\n",
      "d2e110be24e1: Download complete\n",
      "889a7173dcfe: Verifying Checksum\n",
      "889a7173dcfe: Download complete\n",
      "e1948de7822d: Verifying Checksum\n",
      "e1948de7822d: Download complete\n",
      "9c8e4ac0274d: Verifying Checksum\n",
      "9c8e4ac0274d: Download complete\n",
      "29dcb86d08de: Verifying Checksum\n",
      "29dcb86d08de: Download complete\n",
      "4bbfd2c87b75: Verifying Checksum\n",
      "4bbfd2c87b75: Download complete\n",
      "70d8019dd5e8: Verifying Checksum\n",
      "70d8019dd5e8: Download complete\n",
      "e9d8630baacd: Verifying Checksum\n",
      "e9d8630baacd: Download complete\n",
      "a6746fce16a1: Verifying Checksum\n",
      "a6746fce16a1: Download complete\n",
      "4bbfd2c87b75: Pull complete\n",
      "d2e110be24e1: Pull complete\n",
      "889a7173dcfe: Pull complete\n",
      "e1948de7822d: Pull complete\n",
      "82ad86c72f57: Verifying Checksum\n",
      "82ad86c72f57: Download complete\n",
      "334312f3cce5: Verifying Checksum\n",
      "334312f3cce5: Download complete\n",
      "9c8e4ac0274d: Pull complete\n",
      "7a67c64e20d2: Verifying Checksum\n",
      "7a67c64e20d2: Download complete\n",
      "5ccc73e465ef: Verifying Checksum\n",
      "5ccc73e465ef: Download complete\n",
      "87555fa6733c: Verifying Checksum\n",
      "87555fa6733c: Download complete\n",
      "dcfc4adad992: Verifying Checksum\n",
      "dcfc4adad992: Download complete\n",
      "61aa2ea143ea: Verifying Checksum\n",
      "61aa2ea143ea: Download complete\n",
      "95837e5e0af5: Verifying Checksum\n",
      "95837e5e0af5: Download complete\n",
      "9fe1d9bf08cb: Download complete\n",
      "1cc20834967b: Verifying Checksum\n",
      "1cc20834967b: Download complete\n",
      "f492fb29e345: Download complete\n",
      "b1c9ec7e36f6: Verifying Checksum\n",
      "b1c9ec7e36f6: Download complete\n",
      "d1c06a369ea4: Verifying Checksum\n",
      "d1c06a369ea4: Download complete\n",
      "94e3f0ba3fb3: Verifying Checksum\n",
      "94e3f0ba3fb3: Download complete\n",
      "00f212e657e4: Verifying Checksum\n",
      "00f212e657e4: Download complete\n",
      "29dcb86d08de: Pull complete\n",
      "7d32fad557e1: Verifying Checksum\n",
      "7d32fad557e1: Download complete\n",
      "dcb1b56a4779: Verifying Checksum\n",
      "dcb1b56a4779: Download complete\n",
      "c733e8557bb1: Verifying Checksum\n",
      "c733e8557bb1: Download complete\n",
      "70d8019dd5e8: Pull complete\n",
      "4072293fd93e: Download complete\n",
      "3327ae24303f: Download complete\n",
      "3efe73011a96: Verifying Checksum\n",
      "3efe73011a96: Download complete\n",
      "df0cdf11e45e: Verifying Checksum\n",
      "df0cdf11e45e: Download complete\n",
      "27d78ef0086e: Verifying Checksum\n",
      "27d78ef0086e: Download complete\n",
      "5ccc73e465ef: Pull complete\n",
      "e9d8630baacd: Pull complete\n",
      "82ad86c72f57: Pull complete\n",
      "a6746fce16a1: Pull complete\n",
      "7a67c64e20d2: Pull complete\n",
      "334312f3cce5: Pull complete\n",
      "d1c06a369ea4: Pull complete\n",
      "61aa2ea143ea: Pull complete\n",
      "87555fa6733c: Pull complete\n",
      "dcfc4adad992: Pull complete\n",
      "dcb1b56a4779: Pull complete\n",
      "95837e5e0af5: Pull complete\n",
      "9fe1d9bf08cb: Pull complete\n",
      "1cc20834967b: Pull complete\n",
      "f492fb29e345: Pull complete\n",
      "b1c9ec7e36f6: Pull complete\n",
      "94e3f0ba3fb3: Pull complete\n",
      "00f212e657e4: Pull complete\n",
      "7d32fad557e1: Pull complete\n",
      "4072293fd93e: Pull complete\n",
      "27d78ef0086e: Pull complete\n",
      "c733e8557bb1: Pull complete\n",
      "df0cdf11e45e: Pull complete\n",
      "3327ae24303f: Pull complete\n",
      "3efe73011a96: Pull complete\n",
      "Digest: sha256:7fa2b006819f3a484ea0a9c006b25f7321f0769a01b24440a56abca80953a75b\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/tf2-gpu.2-5:latest\n",
      " ---> 0f998c784cd6\n",
      "Step 2/6 : COPY requirements.txt .\n",
      " ---> 06f62e9403b1\n",
      "Step 3/6 : RUN python3 -m pip install -r requirements.txt     -f https://storage.googleapis.com/jax-releases/jax_releases.html\n",
      " ---> Running in aa4ab4a6668f\n",
      "Looking in links: https://storage.googleapis.com/jax-releases/jax_releases.html\n",
      "Collecting flax\n",
      "  Downloading flax-0.3.4-py3-none-any.whl (183 kB)\n",
      "Collecting jax[cuda111]\n",
      "  Downloading jax-0.2.16.tar.gz (680 kB)\n",
      "Collecting optax\n",
      "  Downloading optax-0.0.8-py3-none-any.whl (113 kB)\n",
      "Requirement already satisfied: numpy>=1.12 in /opt/conda/lib/python3.7/site-packages (from flax->-r requirements.txt (line 1)) (1.19.5)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from flax->-r requirements.txt (line 1)) (3.4.2)\n",
      "Collecting msgpack\n",
      "  Downloading msgpack-1.0.2-cp37-cp37m-manylinux1_x86_64.whl (273 kB)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.7/site-packages (from jax[cuda111]->-r requirements.txt (line 2)) (0.13.0)\n",
      "Requirement already satisfied: opt_einsum in /opt/conda/lib/python3.7/site-packages (from jax[cuda111]->-r requirements.txt (line 2)) (3.3.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from absl-py->jax[cuda111]->-r requirements.txt (line 2)) (1.16.0)\n",
      "Collecting jaxlib==0.1.68+cuda111\n",
      "  Downloading https://storage.googleapis.com/jax-releases/cuda111/jaxlib-0.1.68%2Bcuda111-cp37-none-manylinux2010_x86_64.whl (195.9 MB)\n",
      "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /opt/conda/lib/python3.7/site-packages (from jaxlib==0.1.68+cuda111->jax[cuda111]->-r requirements.txt (line 2)) (1.12)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from jaxlib==0.1.68+cuda111->jax[cuda111]->-r requirements.txt (line 2)) (1.6.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib->flax->-r requirements.txt (line 1)) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->flax->-r requirements.txt (line 1)) (0.10.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->flax->-r requirements.txt (line 1)) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->flax->-r requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->flax->-r requirements.txt (line 1)) (8.2.0)\n",
      "Collecting chex>=0.0.4\n",
      "  Downloading chex-0.0.7-py3-none-any.whl (52 kB)\n",
      "Requirement already satisfied: dm-tree>=0.1.5 in /opt/conda/lib/python3.7/site-packages (from chex>=0.0.4->optax->flax->-r requirements.txt (line 1)) (0.1.6)\n",
      "Collecting toolz>=0.9.0\n",
      "  Downloading toolz-0.11.1-py3-none-any.whl (55 kB)\n",
      "Building wheels for collected packages: jax\n",
      "  Building wheel for jax (setup.py): started\n",
      "  Building wheel for jax (setup.py): finished with status 'done'\n",
      "  Created wheel for jax: filename=jax-0.2.16-py3-none-any.whl size=784391 sha256=9e26dc65a3bd542338b6455f62440afc87d56898ef8a33982a2b9fc6125e85b6\n",
      "  Stored in directory: /root/.cache/pip/wheels/fd/23/9e/4da95cc1faef02199df99d4928b8e870a7c3de039403dcbc6c\n",
      "Successfully built jax\n",
      "Installing collected packages: toolz, jaxlib, jax, chex, optax, msgpack, flax\n",
      "Successfully installed chex-0.0.7 flax-0.3.4 jax-0.2.16 jaxlib-0.1.68+cuda111 msgpack-1.0.2 optax-0.0.8 toolz-0.11.1\n",
      "\u001b[91mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container aa4ab4a6668f\n",
      " ---> 5e5d1c821352\n",
      "Step 4/6 : WORKDIR /app\n",
      " ---> Running in a0a3183f8437\n",
      "Removing intermediate container a0a3183f8437\n",
      " ---> 28c974087ec9\n",
      "Step 5/6 : COPY trainer/task.py .\n",
      " ---> c538bd3336ec\n",
      "Step 6/6 : ENTRYPOINT [\"python\", \"task.py\"]\n",
      " ---> Running in 0ce5c3c457be\n",
      "Removing intermediate container 0ce5c3c457be\n",
      " ---> 24ef50ce0f1a\n",
      "Successfully built 24ef50ce0f1a\n",
      "Successfully tagged gcr.io/dsparing-sandbox/jax_vertex_training_gpu:latest\n",
      "PUSH\n",
      "Pushing gcr.io/dsparing-sandbox/jax_vertex_training_gpu:latest\n",
      "The push refers to repository [gcr.io/dsparing-sandbox/jax_vertex_training_gpu]\n",
      "b6bbac71b492: Preparing\n",
      "49f2a8db8dd3: Preparing\n",
      "cf123476e447: Preparing\n",
      "f0eb64df7e01: Preparing\n",
      "03dc40ebdbd6: Preparing\n",
      "f5d954d2bd94: Preparing\n",
      "db34a056d495: Preparing\n",
      "f9b1cb8c2687: Preparing\n",
      "e68443de6bca: Preparing\n",
      "88bb87a4088d: Preparing\n",
      "29bf522d97b4: Preparing\n",
      "d96c519f0898: Preparing\n",
      "ff0a6aeeabc0: Preparing\n",
      "8a7cebfdebb3: Preparing\n",
      "61546b863e43: Preparing\n",
      "d2b843fb2f7a: Preparing\n",
      "36c9a9d68143: Preparing\n",
      "d2b843fb2f7a: Waiting\n",
      "730e84ac5c5d: Preparing\n",
      "a25ae1798c0c: Preparing\n",
      "37a19de06c8b: Preparing\n",
      "27c459f353b4: Preparing\n",
      "25d03c11e857: Preparing\n",
      "d5585264beff: Preparing\n",
      "e39414beba01: Preparing\n",
      "ff6af85bc8aa: Preparing\n",
      "98cedd6c9734: Preparing\n",
      "574aa732d388: Preparing\n",
      "686978e3bf48: Preparing\n",
      "80088b120579: Preparing\n",
      "79a187e0621d: Preparing\n",
      "72657ad6008c: Preparing\n",
      "22d4dd8ed907: Preparing\n",
      "d7e7872b888e: Preparing\n",
      "5f08512fd434: Preparing\n",
      "c7bb31fc0e08: Preparing\n",
      "50858308da3d: Preparing\n",
      "36c9a9d68143: Waiting\n",
      "730e84ac5c5d: Waiting\n",
      "a25ae1798c0c: Waiting\n",
      "37a19de06c8b: Waiting\n",
      "27c459f353b4: Waiting\n",
      "25d03c11e857: Waiting\n",
      "d5585264beff: Waiting\n",
      "e39414beba01: Waiting\n",
      "ff6af85bc8aa: Waiting\n",
      "98cedd6c9734: Waiting\n",
      "574aa732d388: Waiting\n",
      "686978e3bf48: Waiting\n",
      "80088b120579: Waiting\n",
      "79a187e0621d: Waiting\n",
      "72657ad6008c: Waiting\n",
      "22d4dd8ed907: Waiting\n",
      "d7e7872b888e: Waiting\n",
      "5f08512fd434: Waiting\n",
      "c7bb31fc0e08: Waiting\n",
      "50858308da3d: Waiting\n",
      "e68443de6bca: Waiting\n",
      "88bb87a4088d: Waiting\n",
      "29bf522d97b4: Waiting\n",
      "d96c519f0898: Waiting\n",
      "ff0a6aeeabc0: Waiting\n",
      "8a7cebfdebb3: Waiting\n",
      "61546b863e43: Waiting\n",
      "f5d954d2bd94: Waiting\n",
      "db34a056d495: Waiting\n",
      "f9b1cb8c2687: Waiting\n",
      "03dc40ebdbd6: Layer already exists\n",
      "f5d954d2bd94: Layer already exists\n",
      "db34a056d495: Layer already exists\n",
      "f9b1cb8c2687: Layer already exists\n",
      "e68443de6bca: Layer already exists\n",
      "88bb87a4088d: Layer already exists\n",
      "29bf522d97b4: Layer already exists\n",
      "d96c519f0898: Layer already exists\n",
      "ff0a6aeeabc0: Layer already exists\n",
      "8a7cebfdebb3: Layer already exists\n",
      "61546b863e43: Layer already exists\n",
      "d2b843fb2f7a: Layer already exists\n",
      "36c9a9d68143: Layer already exists\n",
      "730e84ac5c5d: Layer already exists\n",
      "a25ae1798c0c: Layer already exists\n",
      "37a19de06c8b: Layer already exists\n",
      "b6bbac71b492: Pushed\n",
      "49f2a8db8dd3: Pushed\n",
      "25d03c11e857: Layer already exists\n",
      "f0eb64df7e01: Pushed\n",
      "27c459f353b4: Layer already exists\n",
      "d5585264beff: Layer already exists\n",
      "ff6af85bc8aa: Layer already exists\n",
      "574aa732d388: Layer already exists\n",
      "e39414beba01: Layer already exists\n",
      "98cedd6c9734: Layer already exists\n",
      "72657ad6008c: Layer already exists\n",
      "79a187e0621d: Layer already exists\n",
      "80088b120579: Layer already exists\n",
      "686978e3bf48: Layer already exists\n",
      "22d4dd8ed907: Layer already exists\n",
      "c7bb31fc0e08: Layer already exists\n",
      "d7e7872b888e: Layer already exists\n",
      "5f08512fd434: Layer already exists\n",
      "50858308da3d: Layer already exists\n",
      "cf123476e447: Pushed\n",
      "latest: digest: sha256:04b8791003caa04c4dd6012b470a29b03684255eb4cb4e156ad6c4f9a093917d size: 7875\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                          IMAGES                                                     STATUS\n",
      "391e40f5-5136-4803-a483-a8ae876ebd41  2021-06-29T05:13:02+00:00  8M54S     gs://dsparing-sandbox_cloudbuild/source/1624943581.990537-a669c12fcda948ae96704babd11bbcc2.tgz  gcr.io/dsparing-sandbox/jax_vertex_training_gpu (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!cd $TRAINING_APP_FOLDER && \\\n",
    "    gcloud builds submit --tag $TRAINING_IMAGE_URI --timeout=\"30m\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test training container locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "executionInfo": {
     "elapsed": 367,
     "status": "error",
     "timestamp": 1617776619162,
     "user": {
      "displayName": "Daniel Sparing",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GimB4WP4BC_SgKVqyvIS8u7WFJXnJGMSuyFNS4Lww=s64",
      "userId": "02797666024167492243"
     },
     "user_tz": 300
    },
    "id": "X0rCYgF6_63Y",
    "outputId": "8d917d99-39db-4ae9-cefc-ed5b6ad74289",
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latest: Pulling from dsparing-sandbox/jax_vertex_training_gpu\n",
      "\n",
      "\u001b[1Bd2c87b75: Already exists \n",
      "\u001b[1B10be24e1: Already exists \n",
      "\u001b[1B7173dcfe: Already exists \n",
      "\u001b[1B8de7822d: Already exists \n",
      "\u001b[1B4ac0274d: Already exists \n",
      "\u001b[1Bb86d08de: Already exists \n",
      "\u001b[1B019dd5e8: Already exists \n",
      "\u001b[1B73e465ef: Already exists \n",
      "\u001b[1B630baacd: Already exists \n",
      "\u001b[1B86c72f57: Already exists \n",
      "\u001b[1B6fce16a1: Already exists \n",
      "\u001b[1Bc64e20d2: Already exists \n",
      "\u001b[1B12f3cce5: Already exists \n",
      "\u001b[1B6a369ea4: Already exists \n",
      "\u001b[1B2ea143ea: Already exists \n",
      "\u001b[1B5fa6733c: Already exists \n",
      "\u001b[1B4adad992: Already exists \n",
      "\u001b[1Bb56a4779: Already exists \n",
      "\u001b[1B7e5e0af5: Already exists \n",
      "\u001b[1Bd9bf08cb: Already exists \n",
      "\u001b[1B0834967b: Already exists \n",
      "\u001b[1Bfb29e345: Already exists \n",
      "\u001b[1Bec7e36f6: Already exists \n",
      "\u001b[1Bf0ba3fb3: Already exists \n",
      "\u001b[1B12e657e4: Already exists \n",
      "\u001b[1Bfad557e1: Already exists \n",
      "\u001b[1B293fd93e: Already exists \n",
      "\u001b[1B8ef0086e: Already exists \n",
      "\u001b[1Be8557bb1: Already exists \n",
      "\u001b[1Bdf11e45e: Already exists \n",
      "\u001b[1Bae24303f: Already exists \n",
      "\u001b[1B73011a96: Already exists \n",
      "\u001b[1B9800a8a7: Already exists \n",
      "\u001b[1Ba83ba8cf: Pulling fs layer \n",
      "\u001b[1B02c08c86: Pulling fs layer \n",
      "\u001b[1BDigest: sha256:04b8791003caa04c4dd6012b470a29b03684255eb4cb4e156ad6c4f9a093917d[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\n",
      "Status: Downloaded newer image for gcr.io/dsparing-sandbox/jax_vertex_training_gpu:latest\n",
      "gcr.io/dsparing-sandbox/jax_vertex_training_gpu:latest\n",
      "2021-06-29 05:22:18.274305: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-06-29 05:22:23.350241: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2021-06-29 05:22:23.358339: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-29 05:22:23.359143: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
      "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\n",
      "2021-06-29 05:22:23.359202: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-06-29 05:22:23.383986: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2021-06-29 05:22:23.384125: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2021-06-29 05:22:23.385984: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
      "2021-06-29 05:22:23.386441: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
      "2021-06-29 05:22:23.531853: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-06-29 05:22:23.642547: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
      "2021-06-29 05:22:23.643003: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
      "2021-06-29 05:22:23.643278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-29 05:22:23.644080: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-29 05:22:23.644749: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
      "Dl Completed...: 100%|██████████| 4/4 [00:00<00:00, 14.89 file/s]2021-06-29 05:22:24.413999: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-06-29 05:22:24.414291: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-06-29 05:22:24.414376: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      \n",
      "2021-06-29 05:22:24.556768: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-06-29 05:22:24.557348: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2299995000 Hz\n",
      "2021-06-29 05:22:24.660054: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2021-06-29 05:22:24.660436: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "\n",
      "INFO:absl:Starting the local TPU driver.\n",
      "INFO:absl:Unable to initialize backend 'tpu_driver': Not found: Unable to find driver in registry given worker: local://\n",
      "INFO:absl:Unable to initialize backend 'tpu': Invalid argument: TpuPlatform is not available.\n",
      "INFO:root:mnist_flax: Epoch 0 in 6.51 sec\n",
      "INFO:root:mnist_flax: Training set accuracy 88.29%\n",
      "INFO:root:mnist_flax: Test set accuracy 88.94%\n",
      "INFO:root:mnist_flax: Epoch 1 in 1.10 sec\n",
      "INFO:root:mnist_flax: Training set accuracy 91.07%\n",
      "INFO:root:mnist_flax: Test set accuracy 91.47%\n",
      "WARNING:tensorflow:Mapping types may not work well with tf.nest. Prefer using MutableMapping for <class 'flax.core.frozen_dict.FrozenDict'>\n",
      "WARNING:tensorflow:Mapping types may not work well with tf.nest. Prefer using MutableMapping for <class 'flax.core.frozen_dict.FrozenDict'>\n",
      "\u001b[1mDownloading and preparing dataset 11.06 MiB (download: 11.06 MiB, generated: 21.00 MiB, total: 32.06 MiB) to /root/tensorflow_datasets/mnist/3.0.1...\u001b[0m\n",
      "\u001b[1mDataset mnist downloaded and prepared to /root/tensorflow_datasets/mnist/3.0.1. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!docker pull $TRAINING_IMAGE_URI \n",
    "!docker run \\\n",
    "    --name training_jax \\\n",
    "    --runtime nvidia \\\n",
    "    $TRAINING_IMAGE_URI \\\n",
    "    --output_dir=$MODEL_BASE_PATH \\\n",
    "    --model_name=\"$MODEL_NAME\"_local \\\n",
    "    --model_version=$MODEL_VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "once the above container run finished:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_jax\n"
     ]
    }
   ],
   "source": [
    "!docker rm -f training_jax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create custom container for prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we below use `CustomContainerTrainingJob.run` to submit the training job, we can specify a prediction container environment as well. If we do so, the model artifact will not only be saved to cloud storage, but will also be uploaded to Vertex AI ready for batch prediction requests or ready to be deployed to an endpoint for online prediction.\n",
    "\n",
    "Therefore we specify a custom prediction container now. (If we didn't, we could still call `CustomContainerTrainingJob.run`, but without the `model_serving_container_*` arguments, and the model training job would finish at storing the artifact in Cloud Storage.)\n",
    "\n",
    "We will simply use the default TensorFlow Serving container image. We still need to build this container, because a Container Registry or Artifact Registry container is expected, so in effect we copy this from Docker Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVING_FOLDER = \"serving\"\n",
    "\n",
    "USE_GPU_SERVING = False\n",
    "if USE_GPU_SERVING:\n",
    "    TFSERVING_TAG = \"latest-gpu\"\n",
    "else:\n",
    "    TFSERVING_TAG = \"latest\"\n",
    "\n",
    "os.environ[\"SERVING_FOLDER\"] = SERVING_FOLDER\n",
    "os.environ[\"TFSERVING_TAG\"] = TFSERVING_TAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p $SERVING_FOLDER\n",
    "cat > $SERVING_FOLDER/Dockerfile << EOF\n",
    "FROM tensorflow/serving:$TFSERVING_TAG\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_TAG = \"latest\"\n",
    "SERVING_IMAGE_URI = f\"gcr.io/{PROJECT_ID}/{SERVING_IMAGE_NAME}:{IMAGE_TAG}\"\n",
    "os.environ[\"SERVING_IMAGE_URI\"] = SERVING_IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 1 file(s) totalling 31 bytes before compression.\n",
      "Uploading tarball of [.] to [gs://dsparing-sandbox_cloudbuild/source/1624944176.801508-e53027ca26d54921a6cbeae85b9d1bec.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/dsparing-sandbox/locations/global/builds/2355960a-9eed-4505-b97f-31c58d4f53ab].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/2355960a-9eed-4505-b97f-31c58d4f53ab?project=654544512569].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"2355960a-9eed-4505-b97f-31c58d4f53ab\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://dsparing-sandbox_cloudbuild/source/1624944176.801508-e53027ca26d54921a6cbeae85b9d1bec.tgz#1624944177008170\n",
      "Copying gs://dsparing-sandbox_cloudbuild/source/1624944176.801508-e53027ca26d54921a6cbeae85b9d1bec.tgz#1624944177008170...\n",
      "/ [1 files][  198.0 B/  198.0 B]                                                \n",
      "Operation completed over 1 objects/198.0 B.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon   2.56kB\n",
      "Step 1/1 : FROM tensorflow/serving:latest\n",
      "latest: Pulling from tensorflow/serving\n",
      "01bf7da0a88c: Pulling fs layer\n",
      "f3b4a5f15c7a: Pulling fs layer\n",
      "57ffbe87baa1: Pulling fs layer\n",
      "e72e6208e893: Pulling fs layer\n",
      "6ea3f464ef73: Pulling fs layer\n",
      "01e9bf86544b: Pulling fs layer\n",
      "68f6bba3dc50: Pulling fs layer\n",
      "e72e6208e893: Waiting\n",
      "6ea3f464ef73: Waiting\n",
      "01e9bf86544b: Waiting\n",
      "68f6bba3dc50: Waiting\n",
      "57ffbe87baa1: Download complete\n",
      "f3b4a5f15c7a: Download complete\n",
      "e72e6208e893: Verifying Checksum\n",
      "e72e6208e893: Download complete\n",
      "01e9bf86544b: Verifying Checksum\n",
      "01e9bf86544b: Download complete\n",
      "68f6bba3dc50: Verifying Checksum\n",
      "68f6bba3dc50: Download complete\n",
      "01bf7da0a88c: Verifying Checksum\n",
      "01bf7da0a88c: Download complete\n",
      "6ea3f464ef73: Verifying Checksum\n",
      "6ea3f464ef73: Download complete\n",
      "01bf7da0a88c: Pull complete\n",
      "f3b4a5f15c7a: Pull complete\n",
      "57ffbe87baa1: Pull complete\n",
      "e72e6208e893: Pull complete\n",
      "6ea3f464ef73: Pull complete\n",
      "01e9bf86544b: Pull complete\n",
      "68f6bba3dc50: Pull complete\n",
      "Digest: sha256:6651f4839e1124dbde75ee531825112af0a6b8ef082c88ab14ca53eb69a2e4bb\n",
      "Status: Downloaded newer image for tensorflow/serving:latest\n",
      " ---> e874bf5e4700\n",
      "Successfully built e874bf5e4700\n",
      "Successfully tagged gcr.io/dsparing-sandbox/jax_vertex_prediction:latest\n",
      "PUSH\n",
      "Pushing gcr.io/dsparing-sandbox/jax_vertex_prediction:latest\n",
      "The push refers to repository [gcr.io/dsparing-sandbox/jax_vertex_prediction]\n",
      "bb4423850a27: Preparing\n",
      "b60ba33781cd: Preparing\n",
      "547f89523b17: Preparing\n",
      "bd91f28d5f3c: Preparing\n",
      "8cafc6d2db45: Preparing\n",
      "a5d4bacb0351: Preparing\n",
      "5153e1acaabc: Preparing\n",
      "a5d4bacb0351: Waiting\n",
      "5153e1acaabc: Waiting\n",
      "547f89523b17: Layer already exists\n",
      "bd91f28d5f3c: Layer already exists\n",
      "b60ba33781cd: Layer already exists\n",
      "8cafc6d2db45: Layer already exists\n",
      "bb4423850a27: Layer already exists\n",
      "a5d4bacb0351: Layer already exists\n",
      "5153e1acaabc: Layer already exists\n",
      "latest: digest: sha256:6651f4839e1124dbde75ee531825112af0a6b8ef082c88ab14ca53eb69a2e4bb size: 1780\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                          IMAGES                                                   STATUS\n",
      "2355960a-9eed-4505-b97f-31c58d4f53ab  2021-06-29T05:22:57+00:00  20S       gs://dsparing-sandbox_cloudbuild/source/1624944176.801508-e53027ca26d54921a6cbeae85b9d1bec.tgz  gcr.io/dsparing-sandbox/jax_vertex_prediction (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!cd serving && \\\n",
    "    gcloud builds submit --tag $SERVING_IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run custom training job with custom container on Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below CustomContainerTrainingJob.run in theory also uploads the Model to Vertex AI. We will ignore this and only use this method to store the SavedModel on GCS, because we cannot use a pre-built container\n",
    "\n",
    "(\"Vertex AI Prediction supports pre-built containers, with no additional customization such as args (\"\"Do not specify any other subfields of containerSpec\"\" [source](https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers#using_a_pre-built_container)), and custom containers. As we need the `--xla_cpu_compilation_enabled` arg, we can only use custom containers.)\n",
    "\n",
    "and because we don't have a custom prediction container on gcr.io yet. (if we had one, we could use that. but we can't directly use docker hub, where tensorflow/serving is.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "JOB_NAME = 'jax_customcontainer_training'\n",
    "\n",
    "job = aiplatform.CustomContainerTrainingJob(\n",
    "    display_name=JOB_NAME,\n",
    "    container_uri=TRAINING_IMAGE_URI,\n",
    "    model_serving_container_image_uri=SERVING_IMAGE_URI,\n",
    "    model_serving_container_predict_route=f\"/v1/models/{MODEL_NAME}:predict\",\n",
    "    model_serving_container_health_route=f\"/v1/models/{MODEL_NAME}\",\n",
    "    model_serving_container_args=[\n",
    "        '--xla_cpu_compilation_enabled=true',\n",
    "        # f'--model_name={MODEL_NAME}',\n",
    "        # f'--model_base_path=$(AIP_STORAGE_URI)/{MODEL_NAME}',\n",
    "    ],\n",
    "    model_serving_container_environment_variables={\n",
    "        \"MODEL_NAME\": MODEL_NAME,\n",
    "        \"MODEL_BASE_PATH\": AIP_STORAGE_URI,\n",
    "    }\n",
    "    model_serving_container_ports=[8501],\n",
    "    staging_bucket=f\"gs://{BUCKET_NAME}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.training_jobs:Training Output directory:\n",
      "gs://dsparing-sandbox/savedmodels/jax_model_customcontainer \n",
      "INFO:google.cloud.aiplatform.training_jobs:View Training:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/9124385204029358080?project=654544512569\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob projects/654544512569/locations/us-central1/trainingPipelines/9124385204029358080 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:View backing custom job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/2807875620409704448?project=654544512569\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob projects/654544512569/locations/us-central1/trainingPipelines/9124385204029358080 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob projects/654544512569/locations/us-central1/trainingPipelines/9124385204029358080 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob projects/654544512569/locations/us-central1/trainingPipelines/9124385204029358080 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob projects/654544512569/locations/us-central1/trainingPipelines/9124385204029358080 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob projects/654544512569/locations/us-central1/trainingPipelines/9124385204029358080 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob projects/654544512569/locations/us-central1/trainingPipelines/9124385204029358080 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob projects/654544512569/locations/us-central1/trainingPipelines/9124385204029358080 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob run completed. Resource name: projects/654544512569/locations/us-central1/trainingPipelines/9124385204029358080\n",
      "INFO:google.cloud.aiplatform.training_jobs:Model available at projects/654544512569/locations/us-central1/models/4133328091600650240\n",
      "jax_model_customcontainer\n",
      "projects/654544512569/locations/us-central1/models/4133328091600650240\n"
     ]
    }
   ],
   "source": [
    "REPLICA_COUNT = 1\n",
    "\n",
    "if USE_GPU:\n",
    "    ACCELERATOR_TYPE = \"NVIDIA_TESLA_T4\"\n",
    "    ACCELERATOR_COUNT = 1\n",
    "else:\n",
    "    ACCELERATOR_TYPE = \"ACCELERATOR_TYPE_UNSPECIFIED\"\n",
    "    ACCELERATOR_COUNT = None\n",
    "\n",
    "model = job.run(\n",
    "    model_display_name=MODEL_NAME,\n",
    "    base_output_dir=MODEL_BASE_PATH,\n",
    "    args=[\n",
    "        f\"--model_name={MODEL_NAME}\",\n",
    "        f\"--model_version={MODEL_VERSION}\",\n",
    "    ]\n",
    "    replica_count=REPLICA_COUNT,\n",
    "    accelerator_count=ACCELERATOR_COUNT,\n",
    "    accelerator_type=ACCELERATOR_TYPE,\n",
    ")\n",
    "\n",
    "print(model.display_name)\n",
    "print(model.resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local prediction with SavedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0  2021-06-29T05:35:09Z  gs://dsparing-sandbox/savedmodels/jax_model_customcontainer/model/model/1/\n",
      "     53991  2021-06-29T05:35:12Z  gs://dsparing-sandbox/savedmodels/jax_model_customcontainer/model/model/1/saved_model.pb\n",
      "                                 gs://dsparing-sandbox/savedmodels/jax_model_customcontainer/model/model/1/assets/\n",
      "                                 gs://dsparing-sandbox/savedmodels/jax_model_customcontainer/model/model/1/variables/\n",
      "TOTAL: 2 objects, 53991 bytes (52.73 KiB)\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls -l $MODEL_BASE_PATH/$MODEL_NAME/$MODEL_VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:absl:Load dataset info from /home/jupyter/tensorflow_datasets/mnist/3.0.1\n",
      "INFO:absl:Field info.citation from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.splits from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.module_name from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Reusing dataset mnist (/home/jupyter/tensorflow_datasets/mnist/3.0.1)\n",
      "INFO:absl:Constructing tf.data.Dataset mnist for split test, from /home/jupyter/tensorflow_datasets/mnist/3.0.1\n"
     ]
    }
   ],
   "source": [
    "# need to initialize flags somehow to avoid errors in load_mnist\n",
    "flags.FLAGS([\"\"])\n",
    "\n",
    "image_to_predict, _ = next(\n",
    "    iter(mnist_lib.load_mnist(tfds.Split.TEST, batch_size=1))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_0': <tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
       " array([[-1.0466415e+01, -2.2813143e+01, -1.6662024e+01, -1.1372093e+01,\n",
       "         -1.3500595e+01, -1.1352137e+01, -1.9475191e+01, -3.8474598e-03,\n",
       "         -1.1796446e+01, -5.5782042e+00]], dtype=float32)>}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = tf.saved_model.load(\n",
    "    os.path.join(MODEL_BASE_PATH, MODEL_NAME, str(MODEL_VERSION))\n",
    ")\n",
    "loaded_model.signatures[\"serving_default\"](image_to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "last_runtime": {
    "build_target": "//learning/deepmind/public/tools/ml_python:ml_notebook",
    "kind": "private"
   },
   "name": "JAX Flax MNIST containerize",
   "provenance": [
    {
     "file_id": "1HenHxSnSqNQPqMPZNG_uhFampEPxN_Cj",
     "timestamp": 1617196427779
    }
   ]
  },
  "environment": {
   "name": "tf2-gpu.2-5.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m74"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
