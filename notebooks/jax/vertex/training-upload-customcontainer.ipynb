{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train JAX/Flax model on Vertex AI custom container and use `jax2tf` to convert to SavedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "v-kebtVqdEyV"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from absl import flags\n",
    "from google.cloud import aiplatform\n",
    "from jax.experimental.jax2tf.examples import mnist_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "W-IAEp1UqmKR",
    "tags": [
     "flake8-noqa-line-1"
    ]
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = !(gcloud config get-value project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "\n",
    "REGION = \"us-central1\"\n",
    "\n",
    "BUCKET_NAME = PROJECT_ID\n",
    "# Use a regional bucket in the above region you have rights to.\n",
    "# Create if needed:\n",
    "# !gsutil mb -l $REGION gs://$BUCKET_NAME\n",
    "\n",
    "USE_GPU = True\n",
    "\n",
    "TRAINING_APP_FOLDER = 'training_app'\n",
    "\n",
    "BASE_OUTPUT_DIR = f\"gs://{BUCKET_NAME}\"\n",
    "MODEL_NAME = \"jax_model_customcontainer\"\n",
    "MODEL_VERSION = 1\n",
    "\n",
    "SERVING_BATCH_SIZE = 3\n",
    "\n",
    "# Block TF from the GPU to let JAX use it all\n",
    "tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Copyright 2021 Google LLC\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     https://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "\n",
      "import argparse\n",
      "import logging\n",
      "import os\n",
      "\n",
      "import tensorflow as tf\n",
      "import tensorflow_datasets as tfds\n",
      "from absl import flags\n",
      "from jax.experimental.jax2tf.examples import mnist_lib, saved_model_lib\n",
      "\n",
      "TRAIN_BATCH_SIZE = 128\n",
      "TEST_BATCH_SIZE = 16\n",
      "NUM_EPOCHS = 2\n",
      "\n",
      "# Block TF from the GPU to let JAX use it all\n",
      "tf.config.set_visible_devices([], \"GPU\")\n",
      "\n",
      "logger = logging.getLogger()\n",
      "\n",
      "# need to initialize flags somehow to avoid errors in load_mnist\n",
      "flags.FLAGS([\"\"])\n",
      "\n",
      "flax_mnist = mnist_lib.FlaxMNIST()\n",
      "\n",
      "train_ds = mnist_lib.load_mnist(tfds.Split.TRAIN, TRAIN_BATCH_SIZE)\n",
      "test_ds = mnist_lib.load_mnist(tfds.Split.TEST, TEST_BATCH_SIZE)\n",
      "\n",
      "# Batch-polymorphic SavedModel\n",
      "input_signatures = [tf.TensorSpec((None,) + mnist_lib.input_shape, tf.float32),]\n",
      "polymorphic_shapes = \"(batch, ...)\"\n",
      "\n",
      "\n",
      "def main(args):\n",
      "    logger_level = logger.level\n",
      "    logger.setLevel(logging.INFO)\n",
      "    predict_fn, params = flax_mnist.train(\n",
      "        train_ds=train_ds,\n",
      "        test_ds=test_ds,\n",
      "        num_epochs=NUM_EPOCHS,\n",
      "    )\n",
      "    logger.setLevel(logger_level)\n",
      "\n",
      "    saved_model_lib.convert_and_save_model(\n",
      "        jax_fn=predict_fn,\n",
      "        params=params,\n",
      "        model_dir=os.path.join(\n",
      "            args[\"output_dir\"], args[\"model_name\"], str(args[\"model_version\"])\n",
      "        ),\n",
      "        input_signatures=input_signatures,\n",
      "        polymorphic_shapes=polymorphic_shapes,\n",
      "    )\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    parser = argparse.ArgumentParser()\n",
      "    parser.add_argument(\n",
      "        \"--output_dir\",\n",
      "        help=\"GCS location to export model_version/SavedModel\",\n",
      "        default=os.getenv(\"AIP_MODEL_DIR\"),\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \"--model_name\",\n",
      "        default=\"model\",\n",
      "    )\n",
      "    parser.add_argument(\"--model_version\", default=1, type=int)\n",
      "\n",
      "    args = parser.parse_args().__dict__\n",
      "\n",
      "    main(args=args)\n"
     ]
    }
   ],
   "source": [
    "!cat $TRAINING_APP_FOLDER/trainer/task.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_GPU:\n",
    "    BASE_TRAINING_IMAGE = \"gcr.io/deeplearning-platform-release/tf2-gpu.2-5\"\n",
    "    TRAINING_IMAGE_NAME = \"jax_vertex_training_gpu\"\n",
    "else:\n",
    "    BASE_TRAINING_IMAGE = \"gcr.io/deeplearning-platform-release/tf2-cpu.2-5\"\n",
    "    TRAINING_IMAGE_NAME = \"jax_vertex_training_cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write a `requirements.txt` and a `Dockerfile` that defines our custom container based on a [Deep Learning Container image](https://cloud.google.com/deep-learning-containers/docs/choosing-container#container_images), including the `pip install` of the required packages, copy of the model training code, and the Entrypoint launching our training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flax\n",
      "jax[cuda111]  # needs pip to run with `-f https://storage.googleapis.com/jax-releases/jax_releases.html`\n"
     ]
    }
   ],
   "source": [
    "!cat $TRAINING_APP_FOLDER/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TRAINING_APP_FOLDER\"] = TRAINING_APP_FOLDER\n",
    "os.environ[\"BASE_TRAINING_IMAGE\"] = BASE_TRAINING_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 458,
     "status": "ok",
     "timestamp": 1617197650331,
     "user": {
      "displayName": "Daniel Sparing",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GimB4WP4BC_SgKVqyvIS8u7WFJXnJGMSuyFNS4Lww=s64",
      "userId": "02797666024167492243"
     },
     "user_tz": 300
    },
    "id": "WlzDz-6wdpNC",
    "outputId": "9404061c-e26c-42e3-9b2a-bce48767be1f",
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat > $TRAINING_APP_FOLDER/Dockerfile << EOF\n",
    "FROM $BASE_TRAINING_IMAGE\n",
    "\n",
    "COPY requirements.txt .\n",
    "RUN python3 -m pip install -r requirements.txt \\\n",
    "    -f https://storage.googleapis.com/jax-releases/jax_releases.html \n",
    "\n",
    "WORKDIR /app\n",
    "COPY trainer/task.py .\n",
    "\n",
    "ENTRYPOINT [\"python\", \"task.py\"]\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "FZ7Eh2Wwfc9Y"
   },
   "outputs": [],
   "source": [
    "TRAINING_IMAGE_URI = f\"gcr.io/{PROJECT_ID}/{TRAINING_IMAGE_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1184,
     "status": "ok",
     "timestamp": 1617197720314,
     "user": {
      "displayName": "Daniel Sparing",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GimB4WP4BC_SgKVqyvIS8u7WFJXnJGMSuyFNS4Lww=s64",
      "userId": "02797666024167492243"
     },
     "user_tz": 300
    },
    "id": "JhHx_WUIfs6j",
    "outputId": "418935e1-58c0-4812-f820-c09244711048",
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 22 file(s) totalling 41.8 KiB before compression.\n",
      "Uploading tarball of [.] to [gs://dsparing-sandbox_cloudbuild/source/1625151978.12105-0ef148ea8fa14c8487dfa9d05bf08444.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/dsparing-sandbox/locations/global/builds/2aacee55-6e6a-4ace-8137-ccc434dbb314].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/2aacee55-6e6a-4ace-8137-ccc434dbb314?project=654544512569].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"2aacee55-6e6a-4ace-8137-ccc434dbb314\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://dsparing-sandbox_cloudbuild/source/1625151978.12105-0ef148ea8fa14c8487dfa9d05bf08444.tgz#1625151978338636\n",
      "Copying gs://dsparing-sandbox_cloudbuild/source/1625151978.12105-0ef148ea8fa14c8487dfa9d05bf08444.tgz#1625151978338636...\n",
      "/ [1 files][ 13.4 KiB/ 13.4 KiB]                                                \n",
      "Operation completed over 1 objects/13.4 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  63.49kB\n",
      "Step 1/6 : FROM gcr.io/deeplearning-platform-release/tf2-gpu.2-5\n",
      "latest: Pulling from deeplearning-platform-release/tf2-gpu.2-5\n",
      "4bbfd2c87b75: Pulling fs layer\n",
      "d2e110be24e1: Pulling fs layer\n",
      "889a7173dcfe: Pulling fs layer\n",
      "e1948de7822d: Pulling fs layer\n",
      "9c8e4ac0274d: Pulling fs layer\n",
      "29dcb86d08de: Pulling fs layer\n",
      "70d8019dd5e8: Pulling fs layer\n",
      "5ccc73e465ef: Pulling fs layer\n",
      "e9d8630baacd: Pulling fs layer\n",
      "82ad86c72f57: Pulling fs layer\n",
      "a6746fce16a1: Pulling fs layer\n",
      "7a67c64e20d2: Pulling fs layer\n",
      "e1948de7822d: Waiting\n",
      "9c8e4ac0274d: Waiting\n",
      "29dcb86d08de: Waiting\n",
      "70d8019dd5e8: Waiting\n",
      "5ccc73e465ef: Waiting\n",
      "e9d8630baacd: Waiting\n",
      "82ad86c72f57: Waiting\n",
      "a6746fce16a1: Waiting\n",
      "334312f3cce5: Pulling fs layer\n",
      "7a67c64e20d2: Waiting\n",
      "334312f3cce5: Waiting\n",
      "d1c06a369ea4: Pulling fs layer\n",
      "61aa2ea143ea: Pulling fs layer\n",
      "87555fa6733c: Pulling fs layer\n",
      "d1c06a369ea4: Waiting\n",
      "61aa2ea143ea: Waiting\n",
      "87555fa6733c: Waiting\n",
      "dcfc4adad992: Pulling fs layer\n",
      "dcb1b56a4779: Pulling fs layer\n",
      "95837e5e0af5: Pulling fs layer\n",
      "9fe1d9bf08cb: Pulling fs layer\n",
      "1cc20834967b: Pulling fs layer\n",
      "f492fb29e345: Pulling fs layer\n",
      "b1c9ec7e36f6: Pulling fs layer\n",
      "94e3f0ba3fb3: Pulling fs layer\n",
      "00f212e657e4: Pulling fs layer\n",
      "7d32fad557e1: Pulling fs layer\n",
      "4072293fd93e: Pulling fs layer\n",
      "27d78ef0086e: Pulling fs layer\n",
      "c733e8557bb1: Pulling fs layer\n",
      "df0cdf11e45e: Pulling fs layer\n",
      "3327ae24303f: Pulling fs layer\n",
      "3efe73011a96: Pulling fs layer\n",
      "dcfc4adad992: Waiting\n",
      "dcb1b56a4779: Waiting\n",
      "95837e5e0af5: Waiting\n",
      "9fe1d9bf08cb: Waiting\n",
      "1cc20834967b: Waiting\n",
      "f492fb29e345: Waiting\n",
      "b1c9ec7e36f6: Waiting\n",
      "94e3f0ba3fb3: Waiting\n",
      "00f212e657e4: Waiting\n",
      "7d32fad557e1: Waiting\n",
      "4072293fd93e: Waiting\n",
      "27d78ef0086e: Waiting\n",
      "c733e8557bb1: Waiting\n",
      "df0cdf11e45e: Waiting\n",
      "3327ae24303f: Waiting\n",
      "3efe73011a96: Waiting\n",
      "d2e110be24e1: Verifying Checksum\n",
      "d2e110be24e1: Download complete\n",
      "889a7173dcfe: Verifying Checksum\n",
      "889a7173dcfe: Download complete\n",
      "4bbfd2c87b75: Verifying Checksum\n",
      "e1948de7822d: Verifying Checksum\n",
      "e1948de7822d: Download complete\n",
      "4bbfd2c87b75: Download complete\n",
      "9c8e4ac0274d: Verifying Checksum\n",
      "9c8e4ac0274d: Download complete\n",
      "29dcb86d08de: Download complete\n",
      "70d8019dd5e8: Verifying Checksum\n",
      "70d8019dd5e8: Download complete\n",
      "e9d8630baacd: Verifying Checksum\n",
      "e9d8630baacd: Download complete\n",
      "a6746fce16a1: Verifying Checksum\n",
      "a6746fce16a1: Download complete\n",
      "4bbfd2c87b75: Pull complete\n",
      "d2e110be24e1: Pull complete\n",
      "889a7173dcfe: Pull complete\n",
      "e1948de7822d: Pull complete\n",
      "9c8e4ac0274d: Pull complete\n",
      "29dcb86d08de: Pull complete\n",
      "70d8019dd5e8: Pull complete\n",
      "82ad86c72f57: Verifying Checksum\n",
      "82ad86c72f57: Download complete\n",
      "334312f3cce5: Download complete\n",
      "7a67c64e20d2: Verifying Checksum\n",
      "7a67c64e20d2: Download complete\n",
      "5ccc73e465ef: Verifying Checksum\n",
      "5ccc73e465ef: Download complete\n",
      "87555fa6733c: Verifying Checksum\n",
      "87555fa6733c: Download complete\n",
      "dcfc4adad992: Verifying Checksum\n",
      "dcfc4adad992: Download complete\n",
      "61aa2ea143ea: Verifying Checksum\n",
      "61aa2ea143ea: Download complete\n",
      "95837e5e0af5: Download complete\n",
      "9fe1d9bf08cb: Verifying Checksum\n",
      "9fe1d9bf08cb: Download complete\n",
      "d1c06a369ea4: Verifying Checksum\n",
      "d1c06a369ea4: Download complete\n",
      "1cc20834967b: Verifying Checksum\n",
      "1cc20834967b: Download complete\n",
      "f492fb29e345: Verifying Checksum\n",
      "f492fb29e345: Download complete\n",
      "b1c9ec7e36f6: Verifying Checksum\n",
      "b1c9ec7e36f6: Download complete\n",
      "94e3f0ba3fb3: Download complete\n",
      "00f212e657e4: Verifying Checksum\n",
      "00f212e657e4: Download complete\n",
      "7d32fad557e1: Verifying Checksum\n",
      "7d32fad557e1: Download complete\n",
      "dcb1b56a4779: Verifying Checksum\n",
      "dcb1b56a4779: Download complete\n",
      "c733e8557bb1: Download complete\n",
      "4072293fd93e: Verifying Checksum\n",
      "4072293fd93e: Download complete\n",
      "3327ae24303f: Verifying Checksum\n",
      "3327ae24303f: Download complete\n",
      "3efe73011a96: Verifying Checksum\n",
      "3efe73011a96: Download complete\n",
      "df0cdf11e45e: Verifying Checksum\n",
      "df0cdf11e45e: Download complete\n",
      "27d78ef0086e: Download complete\n",
      "5ccc73e465ef: Pull complete\n",
      "e9d8630baacd: Pull complete\n",
      "82ad86c72f57: Pull complete\n",
      "a6746fce16a1: Pull complete\n",
      "7a67c64e20d2: Pull complete\n",
      "334312f3cce5: Pull complete\n",
      "d1c06a369ea4: Pull complete\n",
      "61aa2ea143ea: Pull complete\n",
      "87555fa6733c: Pull complete\n",
      "dcfc4adad992: Pull complete\n",
      "dcb1b56a4779: Pull complete\n",
      "95837e5e0af5: Pull complete\n",
      "9fe1d9bf08cb: Pull complete\n",
      "1cc20834967b: Pull complete\n",
      "f492fb29e345: Pull complete\n",
      "b1c9ec7e36f6: Pull complete\n",
      "94e3f0ba3fb3: Pull complete\n",
      "00f212e657e4: Pull complete\n",
      "7d32fad557e1: Pull complete\n",
      "4072293fd93e: Pull complete\n",
      "27d78ef0086e: Pull complete\n",
      "c733e8557bb1: Pull complete\n",
      "df0cdf11e45e: Pull complete\n",
      "3327ae24303f: Pull complete\n",
      "3efe73011a96: Pull complete\n",
      "Digest: sha256:7fa2b006819f3a484ea0a9c006b25f7321f0769a01b24440a56abca80953a75b\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/tf2-gpu.2-5:latest\n",
      " ---> 0f998c784cd6\n",
      "Step 2/6 : COPY requirements.txt .\n",
      " ---> a275984d41b1\n",
      "Step 3/6 : RUN python3 -m pip install -r requirements.txt     -f https://storage.googleapis.com/jax-releases/jax_releases.html\n",
      " ---> Running in 0a60cea80ee1\n",
      "Looking in links: https://storage.googleapis.com/jax-releases/jax_releases.html\n",
      "Collecting flax\n",
      "  Downloading flax-0.3.4-py3-none-any.whl (183 kB)\n",
      "Collecting jax[cuda111]\n",
      "  Downloading jax-0.2.16.tar.gz (680 kB)\n",
      "Collecting optax\n",
      "  Downloading optax-0.0.8-py3-none-any.whl (113 kB)\n",
      "Collecting msgpack\n",
      "  Downloading msgpack-1.0.2-cp37-cp37m-manylinux1_x86_64.whl (273 kB)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from flax->-r requirements.txt (line 1)) (3.4.2)\n",
      "Requirement already satisfied: numpy>=1.12 in /opt/conda/lib/python3.7/site-packages (from flax->-r requirements.txt (line 1)) (1.19.5)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.7/site-packages (from jax[cuda111]->-r requirements.txt (line 2)) (0.13.0)\n",
      "Requirement already satisfied: opt_einsum in /opt/conda/lib/python3.7/site-packages (from jax[cuda111]->-r requirements.txt (line 2)) (3.3.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from absl-py->jax[cuda111]->-r requirements.txt (line 2)) (1.16.0)\n",
      "Collecting jaxlib==0.1.68+cuda111\n",
      "  Downloading https://storage.googleapis.com/jax-releases/cuda111/jaxlib-0.1.68%2Bcuda111-cp37-none-manylinux2010_x86_64.whl (195.9 MB)\n",
      "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /opt/conda/lib/python3.7/site-packages (from jaxlib==0.1.68+cuda111->jax[cuda111]->-r requirements.txt (line 2)) (1.12)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from jaxlib==0.1.68+cuda111->jax[cuda111]->-r requirements.txt (line 2)) (1.6.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->flax->-r requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->flax->-r requirements.txt (line 1)) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib->flax->-r requirements.txt (line 1)) (2.8.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->flax->-r requirements.txt (line 1)) (8.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->flax->-r requirements.txt (line 1)) (2.4.7)\n",
      "Collecting chex>=0.0.4\n",
      "  Downloading chex-0.0.7-py3-none-any.whl (52 kB)\n",
      "Requirement already satisfied: dm-tree>=0.1.5 in /opt/conda/lib/python3.7/site-packages (from chex>=0.0.4->optax->flax->-r requirements.txt (line 1)) (0.1.6)\n",
      "Collecting toolz>=0.9.0\n",
      "  Downloading toolz-0.11.1-py3-none-any.whl (55 kB)\n",
      "Building wheels for collected packages: jax\n",
      "  Building wheel for jax (setup.py): started\n",
      "  Building wheel for jax (setup.py): finished with status 'done'\n",
      "  Created wheel for jax: filename=jax-0.2.16-py3-none-any.whl size=784391 sha256=7a6e8b252ce4385a05c6dbebf76a3fccf70971ae8434bf1e8f5aac26e1b92dc4\n",
      "  Stored in directory: /root/.cache/pip/wheels/fd/23/9e/4da95cc1faef02199df99d4928b8e870a7c3de039403dcbc6c\n",
      "Successfully built jax\n",
      "Installing collected packages: toolz, jaxlib, jax, chex, optax, msgpack, flax\n",
      "Successfully installed chex-0.0.7 flax-0.3.4 jax-0.2.16 jaxlib-0.1.68+cuda111 msgpack-1.0.2 optax-0.0.8 toolz-0.11.1\n",
      "\u001b[91mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 0a60cea80ee1\n",
      " ---> c7c4408b29ac\n",
      "Step 4/6 : WORKDIR /app\n",
      " ---> Running in bb363c4b85da\n",
      "Removing intermediate container bb363c4b85da\n",
      " ---> 713ebf6e315a\n",
      "Step 5/6 : COPY trainer/task.py .\n",
      " ---> d8bd9d29dc2d\n",
      "Step 6/6 : ENTRYPOINT [\"python\", \"task.py\"]\n",
      " ---> Running in 82a0aa7a6538\n",
      "Removing intermediate container 82a0aa7a6538\n",
      " ---> b351d17648d5\n",
      "Successfully built b351d17648d5\n",
      "Successfully tagged gcr.io/dsparing-sandbox/jax_vertex_training_gpu:latest\n",
      "PUSH\n",
      "Pushing gcr.io/dsparing-sandbox/jax_vertex_training_gpu\n",
      "The push refers to repository [gcr.io/dsparing-sandbox/jax_vertex_training_gpu]\n",
      "0f61260b8bb1: Preparing\n",
      "bb84dc5fc8c8: Preparing\n",
      "46b77c9099da: Preparing\n",
      "f0eb64df7e01: Preparing\n",
      "03dc40ebdbd6: Preparing\n",
      "f5d954d2bd94: Preparing\n",
      "db34a056d495: Preparing\n",
      "f9b1cb8c2687: Preparing\n",
      "e68443de6bca: Preparing\n",
      "88bb87a4088d: Preparing\n",
      "29bf522d97b4: Preparing\n",
      "d96c519f0898: Preparing\n",
      "ff0a6aeeabc0: Preparing\n",
      "8a7cebfdebb3: Preparing\n",
      "61546b863e43: Preparing\n",
      "d2b843fb2f7a: Preparing\n",
      "36c9a9d68143: Preparing\n",
      "730e84ac5c5d: Preparing\n",
      "a25ae1798c0c: Preparing\n",
      "37a19de06c8b: Preparing\n",
      "27c459f353b4: Preparing\n",
      "25d03c11e857: Preparing\n",
      "d5585264beff: Preparing\n",
      "e39414beba01: Preparing\n",
      "ff6af85bc8aa: Preparing\n",
      "98cedd6c9734: Preparing\n",
      "574aa732d388: Preparing\n",
      "686978e3bf48: Preparing\n",
      "80088b120579: Preparing\n",
      "79a187e0621d: Preparing\n",
      "72657ad6008c: Preparing\n",
      "22d4dd8ed907: Preparing\n",
      "d7e7872b888e: Preparing\n",
      "5f08512fd434: Preparing\n",
      "c7bb31fc0e08: Preparing\n",
      "50858308da3d: Preparing\n",
      "f5d954d2bd94: Waiting\n",
      "db34a056d495: Waiting\n",
      "f9b1cb8c2687: Waiting\n",
      "e68443de6bca: Waiting\n",
      "88bb87a4088d: Waiting\n",
      "29bf522d97b4: Waiting\n",
      "d96c519f0898: Waiting\n",
      "ff0a6aeeabc0: Waiting\n",
      "8a7cebfdebb3: Waiting\n",
      "61546b863e43: Waiting\n",
      "d2b843fb2f7a: Waiting\n",
      "36c9a9d68143: Waiting\n",
      "730e84ac5c5d: Waiting\n",
      "a25ae1798c0c: Waiting\n",
      "37a19de06c8b: Waiting\n",
      "27c459f353b4: Waiting\n",
      "25d03c11e857: Waiting\n",
      "d5585264beff: Waiting\n",
      "e39414beba01: Waiting\n",
      "ff6af85bc8aa: Waiting\n",
      "98cedd6c9734: Waiting\n",
      "574aa732d388: Waiting\n",
      "686978e3bf48: Waiting\n",
      "80088b120579: Waiting\n",
      "79a187e0621d: Waiting\n",
      "72657ad6008c: Waiting\n",
      "22d4dd8ed907: Waiting\n",
      "d7e7872b888e: Waiting\n",
      "5f08512fd434: Waiting\n",
      "c7bb31fc0e08: Waiting\n",
      "50858308da3d: Waiting\n",
      "03dc40ebdbd6: Layer already exists\n",
      "f5d954d2bd94: Layer already exists\n",
      "db34a056d495: Layer already exists\n",
      "f9b1cb8c2687: Layer already exists\n",
      "e68443de6bca: Layer already exists\n",
      "88bb87a4088d: Layer already exists\n",
      "29bf522d97b4: Layer already exists\n",
      "d96c519f0898: Layer already exists\n",
      "ff0a6aeeabc0: Layer already exists\n",
      "8a7cebfdebb3: Layer already exists\n",
      "61546b863e43: Layer already exists\n",
      "d2b843fb2f7a: Layer already exists\n",
      "0f61260b8bb1: Pushed\n",
      "36c9a9d68143: Layer already exists\n",
      "f0eb64df7e01: Pushed\n",
      "bb84dc5fc8c8: Pushed\n",
      "730e84ac5c5d: Layer already exists\n",
      "37a19de06c8b: Layer already exists\n",
      "a25ae1798c0c: Layer already exists\n",
      "d5585264beff: Layer already exists\n",
      "27c459f353b4: Layer already exists\n",
      "25d03c11e857: Layer already exists\n",
      "e39414beba01: Layer already exists\n",
      "ff6af85bc8aa: Layer already exists\n",
      "98cedd6c9734: Layer already exists\n",
      "574aa732d388: Layer already exists\n",
      "686978e3bf48: Layer already exists\n",
      "22d4dd8ed907: Layer already exists\n",
      "d7e7872b888e: Layer already exists\n",
      "72657ad6008c: Layer already exists\n",
      "79a187e0621d: Layer already exists\n",
      "80088b120579: Layer already exists\n",
      "c7bb31fc0e08: Layer already exists\n",
      "50858308da3d: Layer already exists\n",
      "5f08512fd434: Layer already exists\n",
      "46b77c9099da: Pushed\n",
      "latest: digest: sha256:422d9a7fb1c08dd1d1d7eeeb263be4cfbad9ff1efaf585b7aacdccd8d5cee7eb size: 7875\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                         IMAGES                                                     STATUS\n",
      "2aacee55-6e6a-4ace-8137-ccc434dbb314  2021-07-01T15:06:18+00:00  9M16S     gs://dsparing-sandbox_cloudbuild/source/1625151978.12105-0ef148ea8fa14c8487dfa9d05bf08444.tgz  gcr.io/dsparing-sandbox/jax_vertex_training_gpu (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!cd $TRAINING_APP_FOLDER && \\\n",
    "    gcloud builds submit --tag $TRAINING_IMAGE_URI --timeout=\"30m\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test training container locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "executionInfo": {
     "elapsed": 367,
     "status": "error",
     "timestamp": 1617776619162,
     "user": {
      "displayName": "Daniel Sparing",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GimB4WP4BC_SgKVqyvIS8u7WFJXnJGMSuyFNS4Lww=s64",
      "userId": "02797666024167492243"
     },
     "user_tz": 300
    },
    "id": "X0rCYgF6_63Y",
    "outputId": "8d917d99-39db-4ae9-cefc-ed5b6ad74289",
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default tag: latest\n",
      "latest: Pulling from dsparing-sandbox/jax_vertex_training_gpu\n",
      "\n",
      "\u001b[1Bd2c87b75: Already exists \n",
      "\u001b[1B10be24e1: Already exists \n",
      "\u001b[1B7173dcfe: Already exists \n",
      "\u001b[1B8de7822d: Already exists \n",
      "\u001b[1B4ac0274d: Already exists \n",
      "\u001b[1Bb86d08de: Already exists \n",
      "\u001b[1B019dd5e8: Already exists \n",
      "\u001b[1B73e465ef: Already exists \n",
      "\u001b[1B630baacd: Already exists \n",
      "\u001b[1B86c72f57: Already exists \n",
      "\u001b[1B6fce16a1: Already exists \n",
      "\u001b[1Bc64e20d2: Already exists \n",
      "\u001b[1B12f3cce5: Already exists \n",
      "\u001b[1B6a369ea4: Already exists \n",
      "\u001b[1B2ea143ea: Already exists \n",
      "\u001b[1B5fa6733c: Already exists \n",
      "\u001b[1B4adad992: Already exists \n",
      "\u001b[1Bb56a4779: Already exists \n",
      "\u001b[1B7e5e0af5: Already exists \n",
      "\u001b[1Bd9bf08cb: Already exists \n",
      "\u001b[1B0834967b: Already exists \n",
      "\u001b[1Bfb29e345: Already exists \n",
      "\u001b[1Bec7e36f6: Already exists \n",
      "\u001b[1Bf0ba3fb3: Already exists \n",
      "\u001b[1B12e657e4: Already exists \n",
      "\u001b[1Bfad557e1: Already exists \n",
      "\u001b[1B293fd93e: Already exists \n",
      "\u001b[1B8ef0086e: Already exists \n",
      "\u001b[1Be8557bb1: Already exists \n",
      "\u001b[1Bdf11e45e: Already exists \n",
      "\u001b[1Bae24303f: Already exists \n",
      "\u001b[1B73011a96: Already exists \n",
      "\u001b[1B9800a8a7: Already exists \n",
      "\u001b[1B889830d3: Pulling fs layer \n",
      "\u001b[1Bdf2261b8: Pulling fs layer \n",
      "\u001b[1BDigest: sha256:422d9a7fb1c08dd1d1d7eeeb263be4cfbad9ff1efaf585b7aacdccd8d5cee7eb[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\n",
      "Status: Downloaded newer image for gcr.io/dsparing-sandbox/jax_vertex_training_gpu:latest\n",
      "gcr.io/dsparing-sandbox/jax_vertex_training_gpu:latest\n",
      "2021-07-01 15:16:10.093662: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-07-01 15:16:13.572399: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2021-07-01 15:16:13.580544: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-07-01 15:16:13.581250: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
      "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\n",
      "2021-07-01 15:16:13.581309: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-07-01 15:16:13.584758: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2021-07-01 15:16:13.584861: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2021-07-01 15:16:13.586581: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
      "2021-07-01 15:16:13.587077: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
      "2021-07-01 15:16:13.590502: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-07-01 15:16:13.591479: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
      "2021-07-01 15:16:13.591840: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
      "2021-07-01 15:16:13.592074: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-07-01 15:16:13.592971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-07-01 15:16:13.593598: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
      "Dl Completed...: 100%|██████████| 4/4 [00:00<00:00, 10.69 file/s]2021-07-01 15:16:14.465776: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-07-01 15:16:14.466060: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-07-01 15:16:14.466106: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      \n",
      "\n",
      "INFO:absl:Starting the local TPU driver.\n",
      "INFO:absl:Unable to initialize backend 'tpu_driver': Not found: Unable to find driver in registry given worker: local://\n",
      "INFO:absl:Unable to initialize backend 'tpu': Invalid argument: TpuPlatform is not available.\n",
      "2021-07-01 15:16:29.762180: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-07-01 15:16:29.762806: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2299995000 Hz\n",
      "INFO:root:mnist_flax: Epoch 0 in 6.83 sec\n",
      "INFO:root:mnist_flax: Training set accuracy 88.44%\n",
      "INFO:root:mnist_flax: Test set accuracy 88.85%\n",
      "INFO:root:mnist_flax: Epoch 1 in 1.12 sec\n",
      "INFO:root:mnist_flax: Training set accuracy 90.86%\n",
      "INFO:root:mnist_flax: Test set accuracy 91.30%\n",
      "WARNING:tensorflow:Mapping types may not work well with tf.nest. Prefer using MutableMapping for <class 'flax.core.frozen_dict.FrozenDict'>\n",
      "WARNING:tensorflow:Mapping types may not work well with tf.nest. Prefer using MutableMapping for <class 'flax.core.frozen_dict.FrozenDict'>\n",
      "\u001b[1mDownloading and preparing dataset 11.06 MiB (download: 11.06 MiB, generated: 21.00 MiB, total: 32.06 MiB) to /root/tensorflow_datasets/mnist/3.0.1...\u001b[0m\n",
      "\u001b[1mDataset mnist downloaded and prepared to /root/tensorflow_datasets/mnist/3.0.1. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!docker pull $TRAINING_IMAGE_URI \n",
    "!docker run \\\n",
    "    --name training_jax \\\n",
    "    --runtime nvidia \\\n",
    "    $TRAINING_IMAGE_URI \\\n",
    "    --output_dir=$BASE_OUTPUT_DIR/model \\\n",
    "    --model_name=\"$MODEL_NAME\"_local \\\n",
    "    --model_version=$MODEL_VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "once the above container run finished:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_jax\n"
     ]
    }
   ],
   "source": [
    "!docker rm -f training_jax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create custom container for prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we below use `CustomContainerTrainingJob.run` to submit the training job, we can specify a prediction container environment as well. If we do so, the model artifact will not only be saved to cloud storage, but will also be uploaded to Vertex AI ready for batch prediction requests or ready to be deployed to an endpoint for online prediction.\n",
    "\n",
    "Therefore we specify a custom prediction container now. (If we didn't, we could still call `CustomContainerTrainingJob.run`, but without the `model_serving_container_*` arguments, and the model training job would finish at storing the artifact in Cloud Storage.)\n",
    "\n",
    "We will simply use the default TensorFlow Serving container image. We still need to build this container, because a Container Registry or Artifact Registry container is expected, so in effect we copy this from Docker Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVING_FOLDER = \"serving\"\n",
    "SERVING_IMAGE_NAME = \"tensorflow-serving\"\n",
    "SERVING_IMAGE_URI = f\"gcr.io/{PROJECT_ID}/{SERVING_IMAGE_NAME}\"\n",
    "\n",
    "USE_GPU_SERVING = False\n",
    "if USE_GPU_SERVING:\n",
    "    TFSERVING_TAG = \"latest-gpu\"\n",
    "else:\n",
    "    TFSERVING_TAG = \"latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"SERVING_FOLDER\"] = SERVING_FOLDER\n",
    "os.environ[\"TFSERVING_TAG\"] = TFSERVING_TAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p $SERVING_FOLDER\n",
    "cat > $SERVING_FOLDER/Dockerfile << EOF\n",
    "FROM tensorflow/serving:$TFSERVING_TAG\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only build the container if it is not already available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default tag: latest\n",
      "latest: Pulling from dsparing-sandbox/tensorflow-serving\n",
      "Digest: sha256:6651f4839e1124dbde75ee531825112af0a6b8ef082c88ab14ca53eb69a2e4bb\n",
      "Status: Image is up to date for gcr.io/dsparing-sandbox/tensorflow-serving:latest\n",
      "gcr.io/dsparing-sandbox/tensorflow-serving:latest\n"
     ]
    }
   ],
   "source": [
    "!cd $SERVING_FOLDER && docker pull $SERVING_IMAGE_URI || gcloud builds submit --tag $SERVING_IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run custom training job with custom container on Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below CustomContainerTrainingJob.run in theory also uploads the Model to Vertex AI. We will ignore this and only use this method to store the SavedModel on GCS, because we cannot use a pre-built container\n",
    "\n",
    "(\"Vertex AI Prediction supports pre-built containers, with no additional customization such as args (\"\"Do not specify any other subfields of containerSpec\"\" [source](https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers#using_a_pre-built_container)), and custom containers. As we need the `--xla_cpu_compilation_enabled` arg, we can only use custom containers.)\n",
    "\n",
    "and because we don't have a custom prediction container on gcr.io yet. (if we had one, we could use that. but we can't directly use docker hub, where tensorflow/serving is.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "JOB_NAME = \"jax_customcontainer_training\"\n",
    "\n",
    "job = aiplatform.CustomContainerTrainingJob(\n",
    "    display_name=JOB_NAME,\n",
    "    container_uri=TRAINING_IMAGE_URI,\n",
    "    model_serving_container_image_uri=SERVING_IMAGE_URI,\n",
    "    model_serving_container_predict_route=f\"/v1/models/{MODEL_NAME}:predict\",\n",
    "    model_serving_container_health_route=f\"/v1/models/{MODEL_NAME}\",\n",
    "    model_serving_container_args=[\n",
    "        \"--xla_cpu_compilation_enabled=true\",\n",
    "        f\"--model_name={MODEL_NAME}\",\n",
    "        f\"--model_base_path=$(AIP_STORAGE_URI)/{MODEL_NAME}\",\n",
    "    ],\n",
    "    model_serving_container_ports=[8501],\n",
    "    staging_bucket=f\"gs://{BUCKET_NAME}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.training_jobs:Training Output directory:\n",
      "gs://dsparing-sandbox \n",
      "INFO:google.cloud.aiplatform.training_jobs:View Training:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/1929603324328280064?project=654544512569\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob projects/654544512569/locations/us-central1/trainingPipelines/1929603324328280064 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:View backing custom job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/5952514060221153280?project=654544512569\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob projects/654544512569/locations/us-central1/trainingPipelines/1929603324328280064 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob projects/654544512569/locations/us-central1/trainingPipelines/1929603324328280064 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob projects/654544512569/locations/us-central1/trainingPipelines/1929603324328280064 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob projects/654544512569/locations/us-central1/trainingPipelines/1929603324328280064 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob projects/654544512569/locations/us-central1/trainingPipelines/1929603324328280064 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob projects/654544512569/locations/us-central1/trainingPipelines/1929603324328280064 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob projects/654544512569/locations/us-central1/trainingPipelines/1929603324328280064 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob run completed. Resource name: projects/654544512569/locations/us-central1/trainingPipelines/1929603324328280064\n",
      "INFO:google.cloud.aiplatform.training_jobs:Model available at projects/654544512569/locations/us-central1/models/653734429503520768\n",
      "jax_model_customcontainer projects/654544512569/locations/us-central1/models/653734429503520768\n"
     ]
    }
   ],
   "source": [
    "REPLICA_COUNT = 1\n",
    "\n",
    "if USE_GPU:\n",
    "    ACCELERATOR_TYPE = \"NVIDIA_TESLA_T4\"\n",
    "    ACCELERATOR_COUNT = 1\n",
    "else:\n",
    "    ACCELERATOR_TYPE = \"ACCELERATOR_TYPE_UNSPECIFIED\"\n",
    "    ACCELERATOR_COUNT = None\n",
    "\n",
    "model = job.run(\n",
    "    model_display_name=MODEL_NAME,\n",
    "    base_output_dir=BASE_OUTPUT_DIR,\n",
    "    args=[\n",
    "        f\"--model_name={MODEL_NAME}\",\n",
    "        f\"--model_version={MODEL_VERSION}\",\n",
    "    ],\n",
    "    replica_count=REPLICA_COUNT,\n",
    "    accelerator_count=ACCELERATOR_COUNT,\n",
    "    accelerator_type=ACCELERATOR_TYPE,\n",
    ")\n",
    "\n",
    "print(model.display_name, model.resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local prediction with SavedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0  2021-06-30T04:57:38Z  gs://dsparing-sandbox/model/jax_model_customcontainer/1/\n",
      "     59519  2021-07-01T15:30:58Z  gs://dsparing-sandbox/model/jax_model_customcontainer/1/saved_model.pb\n",
      "                                 gs://dsparing-sandbox/model/jax_model_customcontainer/1/assets/\n",
      "                                 gs://dsparing-sandbox/model/jax_model_customcontainer/1/variables/\n",
      "TOTAL: 2 objects, 59519 bytes (58.12 KiB)\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls -l $BASE_OUTPUT_DIR/model/$MODEL_NAME/$MODEL_VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:absl:Load dataset info from /home/jupyter/tensorflow_datasets/mnist/3.0.1\n",
      "INFO:absl:Field info.citation from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.splits from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.module_name from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Reusing dataset mnist (/home/jupyter/tensorflow_datasets/mnist/3.0.1)\n",
      "INFO:absl:Constructing tf.data.Dataset mnist for split test, from /home/jupyter/tensorflow_datasets/mnist/3.0.1\n"
     ]
    }
   ],
   "source": [
    "# need to initialize flags somehow to avoid errors in load_mnist\n",
    "flags.FLAGS([\"\"])\n",
    "\n",
    "images_to_predict, _ = next(\n",
    "    iter(mnist_lib.load_mnist(tfds.Split.TEST, batch_size=SERVING_BATCH_SIZE))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_0': <tf.Tensor: shape=(3, 10), dtype=float32, numpy=\n",
       " array([[-12.268999  , -22.472895  , -14.996583  , -13.254833  ,\n",
       "          -0.04260941,  -7.159297  ,  -7.789949  ,  -7.701139  ,\n",
       "          -6.578989  ,  -3.2525737 ],\n",
       "        [ -7.5400867 , -20.041262  , -15.078344  ,  -9.657801  ,\n",
       "          -7.344303  ,  -6.194578  , -16.317587  ,  -0.31555295,\n",
       "          -7.76667   ,  -1.3208492 ],\n",
       "        [-11.988442  ,  -6.592482  ,  -0.46542993,  -1.0133778 ,\n",
       "         -13.975661  ,  -9.580245  ,  -5.0168834 , -11.966478  ,\n",
       "          -6.8450675 , -13.723193  ]], dtype=float32)>}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = tf.saved_model.load(\n",
    "    os.path.join(BASE_OUTPUT_DIR, \"model\", MODEL_NAME, str(MODEL_VERSION))\n",
    ")\n",
    "loaded_model.signatures[\"serving_default\"](images_to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "last_runtime": {
    "build_target": "//learning/deepmind/public/tools/ml_python:ml_notebook",
    "kind": "private"
   },
   "name": "JAX Flax MNIST containerize",
   "provenance": [
    {
     "file_id": "1HenHxSnSqNQPqMPZNG_uhFampEPxN_Cj",
     "timestamp": 1617196427779
    }
   ]
  },
  "environment": {
   "name": "tf2-gpu.2-5.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m74"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
