{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train JAX/Flax model on Vertex AI pre-built container and use `jax2tf` to convert to SavedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "v-kebtVqdEyV"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from absl import flags\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import aiplatform_v1\n",
    "from jax.experimental.jax2tf.examples.mnist_lib import load_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "W-IAEp1UqmKR",
    "tags": [
     "flake8-noqa-line-1"
    ]
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "\n",
    "REGION = \"us-central1\"\n",
    "os.environ['REGION'] = REGION\n",
    "\n",
    "BUCKET_NAME = \"dsparing-sandbox-bucket\"\n",
    "os.environ['BUCKET_NAME'] = BUCKET_NAME\n",
    "# Use a regional bucket in the above region you have rights to.\n",
    "# Create if needed:\n",
    "# !gsutil mb -l ${REGION} gs://${BUCKET_NAME}\n",
    "\n",
    "TRAINING_APP_FOLDER = 'training_app'\n",
    "os.environ['TRAINING_APP_FOLDER'] = TRAINING_APP_FOLDER\n",
    "\n",
    "MODEL_NAME = \"jax_model_prebuilt\"\n",
    "MODELPACKAGE_DIR = f\"gs://{BUCKET_NAME}/models/{MODEL_NAME}/package\"\n",
    "MODELPACKAGE_NAME = \"jax_flax_trainer-0.1.tar.gz\"\n",
    "SAVEDMODEL_BASEDIR = f\"gs://{BUCKET_NAME}/models/{MODEL_NAME}/output\"\n",
    "os.environ['MODELPACKAGE_DIR'] = MODELPACKAGE_DIR\n",
    "os.environ['MODELPACKAGE_NAME'] = MODELPACKAGE_NAME\n",
    "os.environ['SAVEDMODEL_BASEDIR'] = SAVEDMODEL_BASEDIR\n",
    "\n",
    "# Block TF from the GPU to let JAX use it all\n",
    "tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no pre-built Vertex TF2.5 container yet, and `jax.experimental.jax2tf.examples.saved_model_lib.convert_and_save_model` uses the `jit_compile` argument for `tf.function`, which is the TF2.5 new name for `experimental_compile`. So we define a copy `./saved_model_lib_tf24.py` of that module with the only difference that jit_compile is still called `experimental_compile` for TF2.4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no pre-built Vertex TF2.5 container yet, and jax2tf in jax==0.2.14 uses `tensorflow.compiler.tf2xla.conv(... preferred_element_type)` which is not there in TF2.4, only in TF2.5. We can avoid this though by using `convert_and_save_model( ... enable_xla=False)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import argparse\n",
      "import logging\n",
      "import os\n",
      "\n",
      "import tensorflow as tf\n",
      "import tensorflow_datasets as tfds\n",
      "from absl import flags\n",
      "from jax.experimental.jax2tf.examples.mnist_lib import (\n",
      "    load_mnist, FlaxMNIST\n",
      ")\n",
      "# from jax.experimental.jax2tf.examples.saved_model_lib import (\n",
      "#     convert_and_save_model\n",
      "# )\n",
      "\n",
      "from trainer.saved_model_lib_tf2_4 import convert_and_save_model\n",
      "\n",
      "TRAIN_BATCH_SIZE = 128\n",
      "TEST_BATCH_SIZE = 16\n",
      "NUM_EPOCHS = 2\n",
      "\n",
      "# Block TF from the GPU to let JAX use it all\n",
      "tf.config.set_visible_devices([], 'GPU')\n",
      "\n",
      "logger = logging.getLogger()\n",
      "\n",
      "# need to initialize flags somehow to avoid errors in load_mnist\n",
      "flags.FLAGS(['e'])\n",
      "\n",
      "flax_mnist = FlaxMNIST()\n",
      "\n",
      "train_ds = load_mnist(tfds.Split.TRAIN, TRAIN_BATCH_SIZE)\n",
      "test_ds = load_mnist(tfds.Split.TEST, TEST_BATCH_SIZE)\n",
      "\n",
      "image, _ = next(iter(train_ds))\n",
      "input_signature = tf.TensorSpec.from_tensor(\n",
      "    tf.expand_dims(image[0], axis=0)\n",
      ")\n",
      "\n",
      "\n",
      "def main(output_dir):\n",
      "    logger.setLevel(logging.INFO)\n",
      "    predict_fn, params = flax_mnist.train(\n",
      "        train_ds=train_ds,\n",
      "        test_ds=test_ds,\n",
      "        num_epochs=NUM_EPOCHS,\n",
      "    )\n",
      "    logger.setLevel(logging.NOTSET)\n",
      "\n",
      "    convert_and_save_model(\n",
      "        jax_fn=predict_fn,\n",
      "        params=params,\n",
      "        model_dir=output_dir,\n",
      "        input_signatures=[input_signature],\n",
      "        enable_xla=False,\n",
      "    )\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    parser = argparse.ArgumentParser()\n",
      "    parser.add_argument(\n",
      "        \"--output_dir\",\n",
      "        help=\"GCS location to export SavedModel\",\n",
      "        default=os.getenv(\"AIP_MODEL_DIR\")\n",
      "    )\n",
      "    args = parser.parse_args().__dict__\n",
      "\n",
      "    main(output_dir=args[\"output_dir\"])\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat $TRAINING_APP_FOLDER/trainer/task_tf2_4.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test training Python package locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-26 23:10:36.695574: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2021-06-26 23:10:36.695954: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO:absl:Starting the local TPU driver.\n",
      "INFO:absl:Unable to initialize backend 'tpu_driver': Not found: Unable to find driver in registry given worker: local://\n",
      "INFO:absl:Unable to initialize backend 'tpu': Invalid argument: TpuPlatform is not available.\n",
      "INFO:root:mnist_flax: Epoch 0 in 5.85 sec\n",
      "INFO:root:mnist_flax: Training set accuracy 88.23%\n",
      "INFO:root:mnist_flax: Test set accuracy 88.95%\n",
      "INFO:root:mnist_flax: Epoch 1 in 1.10 sec\n",
      "INFO:root:mnist_flax: Training set accuracy 90.92%\n",
      "INFO:root:mnist_flax: Test set accuracy 91.35%\n",
      "WARNING:tensorflow:Mapping types may not work well with tf.nest. Prefer using MutableMapping for <class 'flax.core.frozen_dict.FrozenDict'>\n",
      "WARNING:tensorflow:Mapping types may not work well with tf.nest. Prefer using MutableMapping for <class 'flax.core.frozen_dict.FrozenDict'>\n",
      "Level 1:tensorflow:Creating new FuncGraph for Python function <function convert_and_save_model.<locals>.<lambda> at 0x7fbc90429e60> (key: CacheKey(input_signature=('URu', (TensorSpec(shape=(1, 28, 28, 1), dtype=tf.float32, name=None),)), parent_graph=None, device_functions=(), colocation_stack=(), in_cross_replica_context=False, variable_policy=None, xla_context_id=0))\n",
      "Level 1:tensorflow:Creating new FuncGraph for Python function <function convert_and_save_model.<locals>.<lambda> at 0x7fbc90429e60> (key: CacheKey(input_signature=('URu', (TensorSpec(shape=(1, 28, 28, 1), dtype=tf.float32, name=None),)), parent_graph=None, device_functions=(), colocation_stack=(), in_cross_replica_context=False, variable_policy=None, xla_context_id=0))\n",
      "Level 2:tensorflow:Python function signature [args: (TensorSpec(shape=(1, 28, 28, 1), dtype=tf.float32, name=None),)] [kwargs: {}]\n",
      "Level 2:tensorflow:Python function signature [args: (TensorSpec(shape=(1, 28, 28, 1), dtype=tf.float32, name=None),)] [kwargs: {}]\n",
      "Level 1:tensorflow:Creating new FuncGraph for Python function <function canonicalize_signatures.<locals>.signature_wrapper at 0x7fbc902859e0> (key: CacheKey(input_signature=('UUuDRRu', ('inputs', TensorSpec(shape=(1, 28, 28, 1), dtype=tf.float32, name='inputs'))), parent_graph=None, device_functions=(), colocation_stack=(), in_cross_replica_context=False, variable_policy=<VariablePolicy.NONE: None>, xla_context_id=0))\n",
      "Level 1:tensorflow:Creating new FuncGraph for Python function <function canonicalize_signatures.<locals>.signature_wrapper at 0x7fbc902859e0> (key: CacheKey(input_signature=('UUuDRRu', ('inputs', TensorSpec(shape=(1, 28, 28, 1), dtype=tf.float32, name='inputs'))), parent_graph=None, device_functions=(), colocation_stack=(), in_cross_replica_context=False, variable_policy=<VariablePolicy.NONE: None>, xla_context_id=0))\n",
      "Level 2:tensorflow:Python function signature [args: ()] [kwargs: {'inputs': TensorSpec(shape=(1, 28, 28, 1), dtype=tf.float32, name='inputs')}]\n",
      "Level 2:tensorflow:Python function signature [args: ()] [kwargs: {'inputs': TensorSpec(shape=(1, 28, 28, 1), dtype=tf.float32, name='inputs')}]\n",
      "Level 1:tensorflow:Creating new FuncGraph for Python function <function convert_and_save_model.<locals>.<lambda> at 0x7fbc90429e60> (key: CacheKey(input_signature=('URu', (TensorSpec(shape=(1, 28, 28, 1), dtype=tf.float32, name='inputs'),)), parent_graph=None, device_functions=(), colocation_stack=(), in_cross_replica_context=False, variable_policy=<VariablePolicy.NONE: None>, xla_context_id=0))\n",
      "Level 1:tensorflow:Creating new FuncGraph for Python function <function convert_and_save_model.<locals>.<lambda> at 0x7fbc90429e60> (key: CacheKey(input_signature=('URu', (TensorSpec(shape=(1, 28, 28, 1), dtype=tf.float32, name='inputs'),)), parent_graph=None, device_functions=(), colocation_stack=(), in_cross_replica_context=False, variable_policy=<VariablePolicy.NONE: None>, xla_context_id=0))\n",
      "Level 2:tensorflow:Python function signature [args: (TensorSpec(shape=(1, 28, 28, 1), dtype=tf.float32, name='inputs'),)] [kwargs: {}]\n",
      "Level 2:tensorflow:Python function signature [args: (TensorSpec(shape=(1, 28, 28, 1), dtype=tf.float32, name='inputs'),)] [kwargs: {}]\n",
      "Level 1:tensorflow:Creating new FuncGraph for Python function <function MultiDeviceSaver._traced_save at 0x7fbc901d8830> (key: CacheKey(input_signature=(TensorSpec(shape=(), dtype=tf.string, name=None),), parent_graph=<tensorflow.python.framework.ops.Graph object at 0x7fbc9370fe10>, device_functions=(), colocation_stack=(), in_cross_replica_context=False, variable_policy=<VariablePolicy.NONE: None>, xla_context_id=0))\n",
      "Level 1:tensorflow:Creating new FuncGraph for Python function <function MultiDeviceSaver._traced_save at 0x7fbc901d8830> (key: CacheKey(input_signature=(TensorSpec(shape=(), dtype=tf.string, name=None),), parent_graph=<tensorflow.python.framework.ops.Graph object at 0x7fbc9370fe10>, device_functions=(), colocation_stack=(), in_cross_replica_context=False, variable_policy=<VariablePolicy.NONE: None>, xla_context_id=0))\n",
      "Level 2:tensorflow:Python function signature [args: None] [kwargs: None]\n",
      "Level 2:tensorflow:Python function signature [args: None] [kwargs: None]\n",
      "Level 1:tensorflow:Creating new FuncGraph for Python function <function MultiDeviceSaver._traced_restore at 0x7fbc901f0e60> (key: CacheKey(input_signature=(TensorSpec(shape=(), dtype=tf.string, name=None),), parent_graph=<tensorflow.python.framework.ops.Graph object at 0x7fbc9370fe10>, device_functions=(), colocation_stack=(), in_cross_replica_context=False, variable_policy=<VariablePolicy.NONE: None>, xla_context_id=0))\n",
      "Level 1:tensorflow:Creating new FuncGraph for Python function <function MultiDeviceSaver._traced_restore at 0x7fbc901f0e60> (key: CacheKey(input_signature=(TensorSpec(shape=(), dtype=tf.string, name=None),), parent_graph=<tensorflow.python.framework.ops.Graph object at 0x7fbc9370fe10>, device_functions=(), colocation_stack=(), in_cross_replica_context=False, variable_policy=<VariablePolicy.NONE: None>, xla_context_id=0))\n",
      "Level 2:tensorflow:Python function signature [args: None] [kwargs: None]\n",
      "Level 2:tensorflow:Python function signature [args: None] [kwargs: None]\n",
      "INFO:tensorflow:Assets written to: gs://dsparing-sandbox-bucket/models/jax_model_prebuilt/output/localmodel/assets\n",
      "INFO:tensorflow:Assets written to: gs://dsparing-sandbox-bucket/models/jax_model_prebuilt/output/localmodel/assets\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export PYTHONPATH=${PYTHONPATH}:${PWD}/$TRAINING_APP_FOLDER\n",
    "python3 -m trainer.task_tf2_4 --output_dir=$SAVEDMODEL_BASEDIR/localmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a source distribution and upload to Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from setuptools import find_packages\n",
      "from setuptools import setup\n",
      "\n",
      "REQUIRED_PACKAGES = ['flax', 'jax']\n",
      "\n",
      "setup(\n",
      "    name='jax_flax_trainer',\n",
      "    version='0.1',\n",
      "    install_requires=REQUIRED_PACKAGES,\n",
      "    packages=find_packages(),\n",
      "    include_package_data=True,\n",
      "    description='JAX/FLAX model training application.'\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat $TRAINING_APP_FOLDER/setup.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running sdist\n",
      "running egg_info\n",
      "writing jax_flax_trainer.egg-info/PKG-INFO\n",
      "writing dependency_links to jax_flax_trainer.egg-info/dependency_links.txt\n",
      "writing requirements to jax_flax_trainer.egg-info/requires.txt\n",
      "writing top-level names to jax_flax_trainer.egg-info/top_level.txt\n",
      "reading manifest file 'jax_flax_trainer.egg-info/SOURCES.txt'\n",
      "writing manifest file 'jax_flax_trainer.egg-info/SOURCES.txt'\n",
      "running check\n",
      "creating jax_flax_trainer-0.1\n",
      "creating jax_flax_trainer-0.1/jax_flax_trainer.egg-info\n",
      "creating jax_flax_trainer-0.1/trainer\n",
      "copying files to jax_flax_trainer-0.1...\n",
      "copying setup.py -> jax_flax_trainer-0.1\n",
      "copying jax_flax_trainer.egg-info/PKG-INFO -> jax_flax_trainer-0.1/jax_flax_trainer.egg-info\n",
      "copying jax_flax_trainer.egg-info/SOURCES.txt -> jax_flax_trainer-0.1/jax_flax_trainer.egg-info\n",
      "copying jax_flax_trainer.egg-info/dependency_links.txt -> jax_flax_trainer-0.1/jax_flax_trainer.egg-info\n",
      "copying jax_flax_trainer.egg-info/requires.txt -> jax_flax_trainer-0.1/jax_flax_trainer.egg-info\n",
      "copying jax_flax_trainer.egg-info/top_level.txt -> jax_flax_trainer-0.1/jax_flax_trainer.egg-info\n",
      "copying trainer/__init__.py -> jax_flax_trainer-0.1/trainer\n",
      "copying trainer/saved_model_lib_tf2_4.py -> jax_flax_trainer-0.1/trainer\n",
      "copying trainer/task.py -> jax_flax_trainer-0.1/trainer\n",
      "copying trainer/task_tf2_4.py -> jax_flax_trainer-0.1/trainer\n",
      "Writing jax_flax_trainer-0.1/setup.cfg\n",
      "Creating tar archive\n",
      "removing 'jax_flax_trainer-0.1' (and everything under it)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: sdist: standard file not found: should have one of README, README.rst, README.txt, README.md\n",
      "\n",
      "warning: check: missing required meta-data: url\n",
      "\n",
      "warning: check: missing meta-data: either (author and author_email) or (maintainer and maintainer_email) must be supplied\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd $TRAINING_APP_FOLDER\n",
    "python ./setup.py sdist --formats=gztar\n",
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "gsutil -q cp $TRAINING_APP_FOLDER/dist/$MODELPACKAGE_NAME $MODELPACKAGE_DIR/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run custom training job with Python package on Vertex AI "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should be able to use [CustomTrainingJob](https://googleapis.dev/python/aiplatform/latest/aiplatform.html#google.cloud.aiplatform.CustomTrainingJob) or [CustomPythonPackageTrainingJob](https://googleapis.dev/python/aiplatform/latest/aiplatform.html#google.cloud.aiplatform.CustomPythonPackageTrainingJob) from the high-level API, but it gives an error (see the similar [CustomTrainingJob.run](https://googleapis.dev/python/aiplatform/latest/aiplatform.html#google.cloud.aiplatform.CustomTrainingJob.run) currently giving an error even when using the [official notebook](https://github.com/GoogleCloudPlatform/ai-platform-samples/blob/master/ai-platform-unified/notebooks/official/custom/sdk-custom-image-classification-online.ipynb)), so we use [create_custom_job](https://googleapis.dev/python/aiplatform/latest/aiplatform_v1/job_service.html?#google.cloud.aiplatform_v1.services.job_service.JobServiceClient.create_custom_job) from the low-level API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response: name: \"projects/654544512569/locations/us-central1/customJobs/3380114248062468096\"\n",
      "display_name: \"jax_prebuilt_training\"\n",
      "job_spec {\n",
      "  worker_pool_specs {\n",
      "    machine_spec {\n",
      "      machine_type: \"n1-standard-4\"\n",
      "      accelerator_type: NVIDIA_TESLA_T4\n",
      "      accelerator_count: 1\n",
      "    }\n",
      "    replica_count: 1\n",
      "    disk_spec {\n",
      "      boot_disk_type: \"pd-ssd\"\n",
      "      boot_disk_size_gb: 100\n",
      "    }\n",
      "    python_package_spec {\n",
      "      executor_image_uri: \"us-docker.pkg.dev/vertex-ai/training/tf-gpu.2-4:latest\"\n",
      "      package_uris: \"gs://dsparing-sandbox-bucket/models/jax_model_prebuilt/package/jax_flax_trainer-0.1.tar.gz\"\n",
      "      package_uris: \"gs://jax-releases/cuda110/jaxlib-0.1.67+cuda110-cp37-none-manylinux2010_x86_64.whl\"\n",
      "      python_module: \"trainer.task_tf2_4\"\n",
      "    }\n",
      "  }\n",
      "  base_output_directory {\n",
      "    output_uri_prefix: \"gs://dsparing-sandbox-bucket/models/jax_model_prebuilt/output\"\n",
      "  }\n",
      "}\n",
      "state: JOB_STATE_PENDING\n",
      "create_time {\n",
      "  seconds: 1624749073\n",
      "  nanos: 578971000\n",
      "}\n",
      "update_time {\n",
      "  seconds: 1624749073\n",
      "  nanos: 578971000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "JOB_NAME = \"jax_prebuilt_training\"\n",
    "\n",
    "# Vertex AI machines to use for training\n",
    "JAXLIB_URI = (\"gs://jax-releases/cuda110/jaxlib-0.1.67+\"\n",
    "              \"cuda110-cp37-none-manylinux2010_x86_64.whl\")\n",
    "PYTHON_PACKAGE_URIS = [f\"{MODELPACKAGE_DIR}/{MODELPACKAGE_NAME}\", JAXLIB_URI]\n",
    "MACHINE_TYPE = \"n1-standard-4\"\n",
    "REPLICA_COUNT = 1\n",
    "PYTHON_MODULE = \"trainer.task_tf2_4\"\n",
    "\n",
    "USE_GPU = True\n",
    "if USE_GPU:\n",
    "    TRAINING_IMAGE = 'us-docker.pkg.dev/vertex-ai/training/tf-gpu.2-4:latest'\n",
    "    ACCELERATOR_TYPE = \"NVIDIA_TESLA_T4\"\n",
    "    ACCELERATOR_COUNT = 1\n",
    "else:\n",
    "    TRAINING_IMAGE = 'us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-4:latest'\n",
    "    ACCELERATOR_TYPE = None\n",
    "    ACCELERATOR_COUNT = None\n",
    "\n",
    "api_endpoint: str = f\"{REGION}-aiplatform.googleapis.com\"\n",
    "\n",
    "# The AI Platform services require regional API endpoints.\n",
    "client_options = {\"api_endpoint\": api_endpoint}\n",
    "# Initialize client that will be used to create and send requests.\n",
    "# This client only needs to be created once, and can be reused for multiple\n",
    "# requests.\n",
    "client = aiplatform.gapic.JobServiceClient(client_options=client_options)\n",
    "custom_job = {\n",
    "    \"display_name\": JOB_NAME,\n",
    "    \"job_spec\": {\n",
    "        \"worker_pool_specs\": [\n",
    "            {\n",
    "                \"machine_spec\": {\n",
    "                    \"machine_type\": MACHINE_TYPE,\n",
    "                    \"accelerator_type\": ACCELERATOR_TYPE,\n",
    "                    \"accelerator_count\": ACCELERATOR_COUNT,\n",
    "                },\n",
    "                \"replica_count\": REPLICA_COUNT,\n",
    "                \"python_package_spec\": {\n",
    "                    \"executor_image_uri\": TRAINING_IMAGE,\n",
    "                    \"package_uris\": PYTHON_PACKAGE_URIS,\n",
    "                    \"python_module\": PYTHON_MODULE,\n",
    "                },\n",
    "            }\n",
    "        ],\n",
    "        \"base_output_directory\": {\n",
    "            \"output_uri_prefix\": SAVEDMODEL_BASEDIR\n",
    "        },\n",
    "    },\n",
    "}\n",
    "parent = f\"projects/{PROJECT_ID}/locations/{REGION}\"\n",
    "response = client.create_custom_job(parent=parent, custom_job=custom_job)\n",
    "print(\"response:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JobState.JOB_STATE_RUNNING\n",
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_RUNNING\n",
      "JobState.JOB_STATE_RUNNING\n",
      "JobState.JOB_STATE_SUCCEEDED\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    job_state = client.get_custom_job(name=response.name).state\n",
    "    print(job_state)\n",
    "    if job_state not in (\n",
    "        aiplatform_v1.JobState.JOB_STATE_QUEUED,\n",
    "        aiplatform_v1.JobState.JOB_STATE_PENDING,\n",
    "        aiplatform_v1.JobState.JOB_STATE_RUNNING\n",
    "    ):\n",
    "        break\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local prediction with SavedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0  2021-06-26T17:43:46Z  gs://dsparing-sandbox-bucket/models/jax_model_prebuilt/output/model/\n",
      "     44195  2021-06-26T23:21:05Z  gs://dsparing-sandbox-bucket/models/jax_model_prebuilt/output/model/saved_model.pb\n",
      "                                 gs://dsparing-sandbox-bucket/models/jax_model_prebuilt/output/model/assets/\n",
      "                                 gs://dsparing-sandbox-bucket/models/jax_model_prebuilt/output/model/variables/\n",
      "TOTAL: 2 objects, 44195 bytes (43.16 KiB)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gsutil ls -l $SAVEDMODEL_BASEDIR/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:absl:Load dataset info from /home/jupyter/tensorflow_datasets/mnist/3.0.1\n",
      "INFO:absl:Field info.citation from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.splits from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.module_name from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Reusing dataset mnist (/home/jupyter/tensorflow_datasets/mnist/3.0.1)\n",
      "INFO:absl:Constructing tf.data.Dataset mnist for split test, from /home/jupyter/tensorflow_datasets/mnist/3.0.1\n"
     ]
    }
   ],
   "source": [
    "# need to initialize flags somehow to avoid errors in load_mnist\n",
    "flags.FLAGS(['e'])\n",
    "\n",
    "image_to_predict, _ = next(iter(load_mnist(tfds.Split.TEST, batch_size=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_0': <tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
       " array([[-11.065625  , -21.81225   , -18.527025  , -11.501289  ,\n",
       "          -7.305455  ,  -8.047893  , -17.828339  ,  -0.03009149,\n",
       "          -9.536808  ,  -3.5559657 ]], dtype=float32)>}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = tf.saved_model.load(f\"{SAVEDMODEL_BASEDIR}/model\")\n",
    "loaded_model.signatures[\"serving_default\"](image_to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "last_runtime": {
    "build_target": "//learning/deepmind/public/tools/ml_python:ml_notebook",
    "kind": "private"
   },
   "name": "JAX Flax MNIST containerize",
   "provenance": [
    {
     "file_id": "1HenHxSnSqNQPqMPZNG_uhFampEPxN_Cj",
     "timestamp": 1617196427779
    }
   ]
  },
  "environment": {
   "name": "tf2-gpu.2-5.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m74"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
