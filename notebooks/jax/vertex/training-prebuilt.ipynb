{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train JAX/Flax model on Vertex AI pre-built container and use `jax2tf` to convert to SavedModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "runs on TF2.5 [Vertex Notebook](https://cloud.google.com/vertex-ai/docs/general/notebooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "v-kebtVqdEyV"
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "W-IAEp1UqmKR"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "\n",
    "REGION = \"us-central1\"\n",
    "os.environ['REGION'] = REGION\n",
    "\n",
    "BUCKET_NAME = \"dsparing-sandbox-bucket\"\n",
    "os.environ['BUCKET_NAME'] = BUCKET_NAME\n",
    "# Use a regional bucket in the above region you have rights to.\n",
    "# Create if needed:\n",
    "# !gsutil mb -l ${REGION} gs://${BUCKET_NAME}\n",
    "\n",
    "TRAINING_APP_FOLDER = 'training_app'\n",
    "os.environ['TRAINING_APP_FOLDER'] = TRAINING_APP_FOLDER\n",
    "\n",
    "MODEL_NAME = \"jax_model\"\n",
    "MODEL_DIR = f\"gs://{BUCKET_NAME}/models/{MODEL_NAME}\"\n",
    "os.environ['MODEL_DIR'] = MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p $TRAINING_APP_FOLDER/trainer\n",
    "touch $TRAINING_APP_FOLDER/trainer/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no pre-built Vertex TF2.5 container yet, and `jax.experimental.jax2tf.examples.saved_model_lib.convert_and_save_model` uses the `jit_compile` argument for `tf.function`, which is the TF2.5 new name for `experimental_compile`. So we define a copy of that model with the only difference that jit_compile is still called `experimental_compile` for TF2.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting training_app/trainer/saved_model_lib.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/trainer/saved_model_lib.py\n",
    "\n",
    "from typing import Any, Callable, Sequence, Optional, Union\n",
    "\n",
    "from jax.experimental import jax2tf  # type: ignore[import]\n",
    "import tensorflow as tf  # type: ignore[import]\n",
    "\n",
    "\n",
    "def convert_and_save_model(\n",
    "    jax_fn: Callable[[Any, Any], Any],\n",
    "    params,\n",
    "    model_dir: str,\n",
    "    *,\n",
    "    input_signatures: Sequence[tf.TensorSpec],\n",
    "    polymorphic_shapes: Optional[Union[str, jax2tf.PolyShape]] = None,\n",
    "    with_gradient: bool = False,\n",
    "    enable_xla: bool = True,\n",
    "    compile_model: bool = True,\n",
    "    saved_model_options: Optional[tf.saved_model.SaveOptions] = None):\n",
    "  \"\"\"Convert a JAX function and saves a SavedModel.\n",
    "  This is an example, for serious uses you will likely want to copy and\n",
    "  expand it as needed (see note at the top of the model).\n",
    "  Use this function if you have a trained ML model that has both a prediction\n",
    "  function and trained parameters, which you want to save separately from the\n",
    "  function graph as variables (e.g., to avoid limits on the size of the\n",
    "  GraphDef, or to enable fine-tuning.) If you don't have such parameters,\n",
    "  you can still use this library function but probably don't need it\n",
    "  (see jax2tf/README.md for some simple examples).\n",
    "  In order to use this wrapper you must first convert your model to a function\n",
    "  with two arguments: the parameters and the input on which you want to do\n",
    "  inference. Both arguments may be np.ndarray or (nested)\n",
    "  tuples/lists/dictionaries thereof.\n",
    "  See the README.md for a discussion of how to prepare Flax and Haiku models.\n",
    "  Args:\n",
    "    jax_fn: a JAX function taking two arguments, the parameters and the inputs.\n",
    "      Both arguments may be (nested) tuples/lists/dictionaries of np.ndarray.\n",
    "    params: the parameters, to be used as first argument for `jax_fn`. These\n",
    "      must be (nested) tuples/lists/dictionaries of np.ndarray, and will be\n",
    "      saved as the variables of the SavedModel.\n",
    "    model_dir: the directory where the model should be saved.\n",
    "    input_signatures: the input signatures for the second argument of `jax_fn`\n",
    "      (the input). A signature must be a `tensorflow.TensorSpec` instance, or a\n",
    "      (nested) tuple/list/dictionary thereof with a structure matching the\n",
    "      second argument of `jax_fn`. The first input_signature will be saved as\n",
    "      the default serving signature. The additional signatures will be used\n",
    "      only to ensure that the `jax_fn` is traced and converted to TF for the\n",
    "      corresponding input shapes.\n",
    "    with_gradient: whether the SavedModel should support gradients. If True,\n",
    "      then a custom gradient is saved. If False, then a\n",
    "      tf.raw_ops.PreventGradient is saved to error if a gradient is attempted.\n",
    "      (At the moment due to a bug in SavedModel, custom gradients are not\n",
    "      supported.)\n",
    "    enable_xla: whether the jax2tf converter is allowed to use TFXLA ops. If\n",
    "      False, the conversion tries harder to use purely TF ops and raises an\n",
    "      exception if it is not possible. (default: True)\n",
    "    compile_model: use TensorFlow jit_compiler on the SavedModel. This\n",
    "      is needed if the SavedModel will be used for TensorFlow serving.\n",
    "    polymorphic_shapes: if given then it will be used as the\n",
    "      `polymorphic_shapes` argument to jax2tf.convert for the second parameter of\n",
    "      `jax_fn`. In this case, a single `input_signatures` is supported, and\n",
    "      should have `None` in the polymorphic dimensions.\n",
    "    saved_model_options: options to pass to savedmodel.save.\n",
    "  \"\"\"\n",
    "  if not input_signatures:\n",
    "    raise ValueError(\"At least one input_signature must be given\")\n",
    "  if polymorphic_shapes is not None:\n",
    "    if len(input_signatures) > 1:\n",
    "      raise ValueError(\"For shape-polymorphic conversion a single \"\n",
    "                       \"input_signature is supported.\")\n",
    "  tf_fn = jax2tf.convert(\n",
    "    jax_fn,\n",
    "    with_gradient=with_gradient,\n",
    "    polymorphic_shapes=[None, polymorphic_shapes],\n",
    "    enable_xla=enable_xla)\n",
    "\n",
    "  # Create tf.Variables for the parameters. If you want more useful variable\n",
    "  # names, you can use `tree.map_structure_with_path` from the `dm-tree` package\n",
    "  param_vars = tf.nest.map_structure(\n",
    "    # Due to a bug in SavedModel it is not possible to use tf.GradientTape on\n",
    "    # a function converted with jax2tf and loaded from SavedModel. Thus, we\n",
    "    # mark the variables as non-trainable to ensure that users of the\n",
    "    # SavedModel will not try to fine tune them.\n",
    "    lambda param: tf.Variable(param, trainable=with_gradient),\n",
    "    params)\n",
    "  tf_graph = tf.function(lambda inputs: tf_fn(param_vars, inputs),\n",
    "                         autograph=False,\n",
    "                         experimental_compile=compile_model)\n",
    "\n",
    "  signatures = {}\n",
    "  # This signature is needed for TensorFlow Serving use.\n",
    "  signatures[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY] = \\\n",
    "    tf_graph.get_concrete_function(input_signatures[0])\n",
    "  for input_signature in input_signatures[1:]:\n",
    "    # If there are more signatures, trace and cache a TF function for each one\n",
    "    tf_graph.get_concrete_function(input_signature)\n",
    "  wrapper = _ReusableSavedModelWrapper(tf_graph, param_vars)\n",
    "  if with_gradient:\n",
    "    if not saved_model_options:\n",
    "      saved_model_options = tf.saved_model.SaveOptions(experimental_custom_gradients=True)\n",
    "    else:\n",
    "      saved_model_options.experimental_custom_gradients = True\n",
    "  tf.saved_model.save(wrapper, model_dir, signatures=signatures,\n",
    "                      options=saved_model_options)\n",
    "\n",
    "\n",
    "class _ReusableSavedModelWrapper(tf.train.Checkpoint):\n",
    "  \"\"\"Wraps a function and its parameters for saving to a SavedModel.\n",
    "  Implements the interface described at\n",
    "  https://www.tensorflow.org/hub/reusable_saved_models.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, tf_graph, param_vars):\n",
    "    \"\"\"Args:\n",
    "      tf_graph: a tf.function taking one argument (the inputs), which can be\n",
    "         be tuples/lists/dictionaries of np.ndarray or tensors. The function\n",
    "         may have references to the tf.Variables in `param_vars`.\n",
    "      param_vars: the parameters, as tuples/lists/dictionaries of tf.Variable,\n",
    "         to be saved as the variables of the SavedModel.\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    # Implement the interface from https://www.tensorflow.org/hub/reusable_saved_models\n",
    "    self.variables = tf.nest.flatten(param_vars)\n",
    "    self.trainable_variables = [v for v in self.variables if v.trainable]\n",
    "    # If you intend to prescribe regularization terms for users of the model,\n",
    "    # add them as @tf.functions with no inputs to this list. Else drop this.\n",
    "    self.regularization_losses = []\n",
    "    self.__call__ = tf_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no pre-built Vertex TF2.5 container yet, and jax2tf in jax==0.2.14 uses `tensorflow.compiler.tf2xla.conv(... preferred_element_type)` which is not there in TF2.4, only in TF2.5. We can avoid this though by using `convert_and_save_model( ... enable_xla=False)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting training_app/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/trainer/task.py\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from absl import flags\n",
    "from jax.experimental.jax2tf.examples.mnist_lib import load_mnist, FlaxMNIST\n",
    "# from jax.experimental.jax2tf.examples.saved_model_lib import convert_and_save_model\n",
    "from trainer.saved_model_lib import convert_and_save_model\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        help=\"GCS location to export SavedModel\",\n",
    "        default = os.getenv(\"AIP_MODEL_DIR\")\n",
    "    )\n",
    "    args = parser.parse_args().__dict__\n",
    "\n",
    "    \n",
    "    flags.FLAGS(['e']) # need to initialize flags somehow to avoid errors in load_mnist\n",
    "\n",
    "    train_batch_size = 128\n",
    "    test_batch_size = 16\n",
    "\n",
    "\n",
    "    flax_mnist = FlaxMNIST()\n",
    "\n",
    "    train_ds = load_mnist(tfds.Split.TRAIN, train_batch_size)\n",
    "    test_ds = load_mnist(tfds.Split.TEST, test_batch_size)\n",
    "\n",
    "    predict_fn, params = flax_mnist.train(\n",
    "        train_ds=train_ds,\n",
    "        test_ds=test_ds,\n",
    "        num_epochs=2\n",
    "    )\n",
    "\n",
    "    image, _ = next(iter(train_ds))\n",
    "    input_signature = tf.TensorSpec.from_tensor(tf.expand_dims(image[0], axis=0))\n",
    "\n",
    "    convert_and_save_model(\n",
    "        jax_fn=predict_fn,\n",
    "        params=params,\n",
    "        model_dir=args[\"output_dir\"],\n",
    "        input_signatures=[input_signature],\n",
    "        enable_xla=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional local test:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%bash\n",
    "export PYTHONPATH=${PYTHONPATH}:${PWD}/$TRAINING_APP_FOLDER\n",
    "python3 -m trainer.task --output_dir=$MODEL_DIR\n",
    "gsutil ls -l $MODEL_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should be able to use [CustomTrainingJob](https://googleapis.dev/python/aiplatform/latest/aiplatform.html#google.cloud.aiplatform.CustomTrainingJob) or [CustomPythonPackageTrainingJob](https://googleapis.dev/python/aiplatform/latest/aiplatform.html#google.cloud.aiplatform.CustomPythonPackageTrainingJob), but it gives an error, see the similar [CustomTrainingJob.run](https://googleapis.dev/python/aiplatform/latest/aiplatform.html#google.cloud.aiplatform.CustomTrainingJob.run) currently giving an error even when using the [official notebook](https://github.com/GoogleCloudPlatform/ai-platform-samples/blob/master/ai-platform-unified/notebooks/official/custom/sdk-custom-image-classification-online.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting training_app/setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/setup.py\n",
    "from setuptools import find_packages\n",
    "from setuptools import setup\n",
    "\n",
    "REQUIRED_PACKAGES = ['flax', 'jax']\n",
    "\n",
    "setup(\n",
    "    name='jax_flax_trainer',\n",
    "    version='0.1',\n",
    "    install_requires=REQUIRED_PACKAGES,\n",
    "    packages=find_packages(),\n",
    "    include_package_data=True,\n",
    "    description='JAX/FLAX model training application.'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running sdist\n",
      "running egg_info\n",
      "writing jax_flax_trainer.egg-info/PKG-INFO\n",
      "writing dependency_links to jax_flax_trainer.egg-info/dependency_links.txt\n",
      "writing requirements to jax_flax_trainer.egg-info/requires.txt\n",
      "writing top-level names to jax_flax_trainer.egg-info/top_level.txt\n",
      "reading manifest file 'jax_flax_trainer.egg-info/SOURCES.txt'\n",
      "writing manifest file 'jax_flax_trainer.egg-info/SOURCES.txt'\n",
      "running check\n",
      "creating jax_flax_trainer-0.1\n",
      "creating jax_flax_trainer-0.1/jax_flax_trainer.egg-info\n",
      "creating jax_flax_trainer-0.1/trainer\n",
      "copying files to jax_flax_trainer-0.1...\n",
      "copying setup.py -> jax_flax_trainer-0.1\n",
      "copying jax_flax_trainer.egg-info/PKG-INFO -> jax_flax_trainer-0.1/jax_flax_trainer.egg-info\n",
      "copying jax_flax_trainer.egg-info/SOURCES.txt -> jax_flax_trainer-0.1/jax_flax_trainer.egg-info\n",
      "copying jax_flax_trainer.egg-info/dependency_links.txt -> jax_flax_trainer-0.1/jax_flax_trainer.egg-info\n",
      "copying jax_flax_trainer.egg-info/requires.txt -> jax_flax_trainer-0.1/jax_flax_trainer.egg-info\n",
      "copying jax_flax_trainer.egg-info/top_level.txt -> jax_flax_trainer-0.1/jax_flax_trainer.egg-info\n",
      "copying trainer/__init__.py -> jax_flax_trainer-0.1/trainer\n",
      "copying trainer/saved_model_lib.py -> jax_flax_trainer-0.1/trainer\n",
      "copying trainer/task.py -> jax_flax_trainer-0.1/trainer\n",
      "Writing jax_flax_trainer-0.1/setup.cfg\n",
      "Creating tar archive\n",
      "removing 'jax_flax_trainer-0.1' (and everything under it)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: sdist: standard file not found: should have one of README, README.rst, README.txt, README.md\n",
      "\n",
      "warning: check: missing required meta-data: url\n",
      "\n",
      "warning: check: missing meta-data: either (author and author_email) or (maintainer and maintainer_email) must be supplied\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd $TRAINING_APP_FOLDER\n",
    "python ./setup.py sdist --formats=gztar\n",
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gsutil -q cp $TRAINING_APP_FOLDER/dist/jax_flax_trainer-0.1.tar.gz gs://$BUCKET_NAME/jax/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response: name: \"projects/654544512569/locations/us-central1/customJobs/587618596302094336\"\n",
      "display_name: \"jax_job\"\n",
      "job_spec {\n",
      "  worker_pool_specs {\n",
      "    machine_spec {\n",
      "      machine_type: \"n1-standard-4\"\n",
      "      accelerator_type: NVIDIA_TESLA_T4\n",
      "      accelerator_count: 1\n",
      "    }\n",
      "    replica_count: 1\n",
      "    disk_spec {\n",
      "      boot_disk_type: \"pd-ssd\"\n",
      "      boot_disk_size_gb: 100\n",
      "    }\n",
      "    python_package_spec {\n",
      "      executor_image_uri: \"us-docker.pkg.dev/vertex-ai/training/tf-gpu.2-4:latest\"\n",
      "      package_uris: \"gs://dsparing-sandbox-bucket/jax/jax_flax_trainer-0.1.tar.gz\"\n",
      "      package_uris: \"gs://jax-releases/cuda110/jaxlib-0.1.67+cuda110-cp37-none-manylinux2010_x86_64.whl\"\n",
      "      python_module: \"trainer.task\"\n",
      "    }\n",
      "  }\n",
      "  base_output_directory {\n",
      "    output_uri_prefix: \"gs://dsparing-sandbox-bucket/jax/output\"\n",
      "  }\n",
      "}\n",
      "state: JOB_STATE_PENDING\n",
      "create_time {\n",
      "  seconds: 1624266105\n",
      "  nanos: 296884000\n",
      "}\n",
      "update_time {\n",
      "  seconds: 1624266105\n",
      "  nanos: 296884000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import aiplatform_v1\n",
    "\n",
    "JOB_NAME=\"jax_job\"\n",
    "\n",
    "# Vertex AI machines to use for training\n",
    "JAXLIB_URI = \"gs://jax-releases/cuda110/jaxlib-0.1.67+cuda110-cp37-none-manylinux2010_x86_64.whl\"\n",
    "PYTHON_PACKAGE_URIS = [f\"gs://{BUCKET_NAME}/jax/jax_flax_trainer-0.1.tar.gz\", JAXLIB_URI]\n",
    "MACHINE_TYPE=\"n1-standard-4\"\n",
    "REPLICA_COUNT=1\n",
    "PYTHON_MODULE=\"trainer.task\"\n",
    "\n",
    "USE_GPU = True\n",
    "if USE_GPU:\n",
    "    TRAINING_IMAGE = 'us-docker.pkg.dev/vertex-ai/training/tf-gpu.2-4:latest'\n",
    "    ACCELERATOR_TYPE = \"NVIDIA_TESLA_T4\"\n",
    "    ACCELERATOR_COUNT = 1\n",
    "else:\n",
    "    TRAINING_IMAGE = 'us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-4:latest'\n",
    "    ACCELERATOR_TYPE = None\n",
    "    ACCELERATOR_COUNT = None\n",
    "    \n",
    "api_endpoint: str = f\"{REGION}-aiplatform.googleapis.com\"\n",
    "\n",
    "# The AI Platform services require regional API endpoints.\n",
    "client_options = {\"api_endpoint\": api_endpoint}\n",
    "# Initialize client that will be used to create and send requests.\n",
    "# This client only needs to be created once, and can be reused for multiple requests.\n",
    "client = aiplatform.gapic.JobServiceClient(client_options=client_options)\n",
    "custom_job = {\n",
    "    \"display_name\": JOB_NAME,\n",
    "    \"job_spec\": {\n",
    "        \"worker_pool_specs\": [\n",
    "            {\n",
    "                \"machine_spec\": {\n",
    "                    \"machine_type\": MACHINE_TYPE,\n",
    "                    \"accelerator_type\": ACCELERATOR_TYPE,\n",
    "                    \"accelerator_count\": ACCELERATOR_COUNT,\n",
    "                },\n",
    "                \"replica_count\": REPLICA_COUNT,\n",
    "                \"python_package_spec\": {\n",
    "                    \"executor_image_uri\": TRAINING_IMAGE,\n",
    "                    \"package_uris\": PYTHON_PACKAGE_URIS,\n",
    "                    \"python_module\": PYTHON_MODULE,\n",
    "                },\n",
    "            }\n",
    "        ],\n",
    "        \"base_output_directory\": {\n",
    "            \"output_uri_prefix\" : f\"gs://{BUCKET_NAME}/jax/output\"\n",
    "        },\n",
    "    },\n",
    "}\n",
    "parent = f\"projects/{PROJECT_ID}/locations/{REGION}\"\n",
    "response = client.create_custom_job(parent=parent, custom_job=custom_job)\n",
    "print(\"response:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_PENDING\n",
      "JobState.JOB_STATE_RUNNING\n",
      "JobState.JOB_STATE_RUNNING\n",
      "JobState.JOB_STATE_SUCCEEDED\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    job_state = client.get_custom_job(name=response.name).state\n",
    "    print(job_state)\n",
    "    if job_state not in (aiplatform_v1.JobState.JOB_STATE_QUEUED, aiplatform_v1.JobState.JOB_STATE_PENDING, aiplatform_v1.JobState.JOB_STATE_RUNNING):\n",
    "        break\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure we can actually predict with savedmodel (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip3 install --user --upgrade jax jaxlib==0.1.67+cuda110 -f https://storage.googleapis.com/jax-releases/jax_releases.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0  2021-06-21T06:06:58Z  gs://dsparing-sandbox-bucket/jax/output/model/\n",
      "     44207  2021-06-21T09:13:07Z  gs://dsparing-sandbox-bucket/jax/output/model/saved_model.pb\n",
      "                                 gs://dsparing-sandbox-bucket/jax/output/model/assets/\n",
      "                                 gs://dsparing-sandbox-bucket/jax/output/model/variables/\n",
      "TOTAL: 2 objects, 44207 bytes (43.17 KiB)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gsutil ls -l gs://$BUCKET_NAME/jax/output/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:absl:Load dataset info from /home/jupyter/tensorflow_datasets/mnist/3.0.1\n",
      "INFO:absl:Field info.citation from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.splits from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.module_name from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Reusing dataset mnist (/home/jupyter/tensorflow_datasets/mnist/3.0.1)\n",
      "INFO:absl:Constructing tf.data.Dataset mnist for split test, from /home/jupyter/tensorflow_datasets/mnist/3.0.1\n"
     ]
    }
   ],
   "source": [
    "from jax.experimental.jax2tf.examples.mnist_lib import load_mnist\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from absl import flags\n",
    "\n",
    "flags.FLAGS(['e']) # need to initialize flags somehow to avoid errors in load_mnist\n",
    "\n",
    "image_to_predict, _ = next(iter(load_mnist(tfds.Split.TEST, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_0': <tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
       " array([[ -5.494674  , -12.381934  , -11.991706  ,  -8.824324  ,\n",
       "          -7.54799   ,  -0.01416086,  -9.895304  ,  -8.375378  ,\n",
       "          -4.7775846 ,  -7.468926  ]], dtype=float32)>}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = tf.saved_model.load(f\"gs://{BUCKET_NAME}/jax/output/model\")\n",
    "loaded_model.signatures[\"serving_default\"](image_to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "last_runtime": {
    "build_target": "//learning/deepmind/public/tools/ml_python:ml_notebook",
    "kind": "private"
   },
   "name": "JAX Flax MNIST containerize",
   "provenance": [
    {
     "file_id": "1HenHxSnSqNQPqMPZNG_uhFampEPxN_Cj",
     "timestamp": 1617196427779
    }
   ]
  },
  "environment": {
   "name": "tf2-gpu.2-5.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m74"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
