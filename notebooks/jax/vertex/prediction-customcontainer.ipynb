{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45981da8",
   "metadata": {},
   "source": [
    "# Predict with JAX/Flax-trained SavedModel on Vertex AI custom container with TF Serving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afa2475",
   "metadata": {},
   "source": [
    "Vertex AI Prediction supports pre-built containers, with no additional customization such as args (\"Do not specify any other subfields of containerSpec\" [source](https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers#using_a_pre-built_container)), and custom containers. As we need the `--xla_cpu_compilation_enabled` arg, we can only use custom containers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5f3728",
   "metadata": {},
   "source": [
    "### Create TFServing container with SavedModel baked in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ab772a",
   "metadata": {},
   "source": [
    "NOTE: serving does not work for GPU yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e85d31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcr.io/dsparing-sandbox/jax_tfserving_image:latest-cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "SAVEDMODEL_DIR = \"gs://dsparing-sandbox-bucket/models/jax_model_prebuilt/output/model\"\n",
    "SERVING_MODELNAME = \"jax_model\"\n",
    "MODEL_VERSION = 1\n",
    "\n",
    "USE_GPU = False\n",
    "if USE_GPU:\n",
    "    IMAGE_TAG='latest-gpu'\n",
    "    TFSERVING_TAG = \"latest-gpu\"\n",
    "else:\n",
    "    IMAGE_TAG='latest-cpu'\n",
    "    TFSERVING_TAG = \"latest\"    \n",
    "\n",
    "IMAGE_NAME='jax_tfserving_image'\n",
    "IMAGE_URI = f'gcr.io/{PROJECT_ID}/{IMAGE_NAME}:{IMAGE_TAG}'\n",
    "print(IMAGE_URI)\n",
    "\n",
    "os.environ[\"SERVING_MODELNAME\"] = SERVING_MODELNAME\n",
    "os.environ[\"SAVEDMODEL_DIR\"] = SAVEDMODEL_DIR\n",
    "os.environ[\"MODEL_VERSION\"] = str(MODEL_VERSION)\n",
    "os.environ[\"IMAGE_URI\"] = IMAGE_URI\n",
    "os.environ[\"IMAGE_TAG\"] = IMAGE_TAG\n",
    "os.environ[\"TFSERVING_TAG\"] = TFSERVING_TAG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71152f9",
   "metadata": {},
   "source": [
    "Check that `SAVEDMODEL_DIR` actually contains a SavedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8a84c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://dsparing-sandbox-bucket/models/jax_model_prebuilt/output/model/\n",
      "gs://dsparing-sandbox-bucket/models/jax_model_prebuilt/output/model/saved_model.pb\n",
      "gs://dsparing-sandbox-bucket/models/jax_model_prebuilt/output/model/assets/\n",
      "gs://dsparing-sandbox-bucket/models/jax_model_prebuilt/output/model/variables/\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gsutil ls $SAVEDMODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a983e0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p $SERVING_MODELNAME/$MODEL_VERSION # tf serving wants model versions in numbered directories.\n",
    "gsutil -q cp -r $SAVEDMODEL_DIR/* $SERVING_MODELNAME/$MODEL_VERSION/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63172fd0",
   "metadata": {},
   "source": [
    "following [TF Serving tutorial](https://www.tensorflow.org/tfx/serving/docker#creating_your_own_serving_image) below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f23885d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0c7bdc7de8fe1aa98b9de311be141164cd623db4fbd5eb17208d80e55e9adbe3\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker run -d --name serving_base tensorflow/serving:$TFSERVING_TAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "389690d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sha256:879ed17c03e9116d7925befb890bec81d7e0ca57cf17fd531aafa0e2d859d4a2\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker cp $SERVING_MODELNAME serving_base:/models/$SERVING_MODELNAME\n",
    "docker commit --change \"ENV MODEL_NAME $SERVING_MODELNAME\" serving_base $IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "251461c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serving_base\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker rm -f serving_base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad562ff2",
   "metadata": {},
   "source": [
    "### Optional: Try serving locally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1485b6d7",
   "metadata": {},
   "source": [
    "Get image to predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00b3c600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from absl import flags\n",
    "from jax.experimental.jax2tf.examples.mnist_lib import load_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "184ef0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "flags.FLAGS(['e']) # need to initialize flags somehow to avoid errors in load_mnist\n",
    "\n",
    "image_to_predict, _ = next(iter(load_mnist(tfds.Split.TEST, 1)))\n",
    "image_json = json.dumps(image_to_predict.numpy().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22aa69a",
   "metadata": {},
   "source": [
    "Start up container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81f0b14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62e139c00ab0ab36d54dedf068cc0b180d86697957ccb40466d726a0ab6aaac1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker run -d -p 8501:8501 -e MODEL_NAME=$SERVING_MODELNAME --name serving_jax $IMAGE_URI --xla_cpu_compilation_enabled=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a5dc7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-26 18:36:03.355282: I tensorflow_serving/model_servers/server.cc:89] Building single TensorFlow model file config:  model_name: jax_model model_base_path: /models/jax_model\n",
      "2021-06-26 18:36:03.355546: I tensorflow_serving/model_servers/server_core.cc:465] Adding/updating models.\n",
      "2021-06-26 18:36:03.355576: I tensorflow_serving/model_servers/server_core.cc:591]  (Re-)adding model: jax_model\n",
      "2021-06-26 18:36:03.455981: I tensorflow_serving/core/basic_manager.cc:740] Successfully reserved resources to load servable {name: jax_model version: 1}\n",
      "2021-06-26 18:36:03.456017: I tensorflow_serving/core/loader_harness.cc:66] Approving load for servable version {name: jax_model version: 1}\n",
      "2021-06-26 18:36:03.456030: I tensorflow_serving/core/loader_harness.cc:74] Loading servable version {name: jax_model version: 1}\n",
      "2021-06-26 18:36:03.456078: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:38] Reading SavedModel from: /models/jax_model/1\n",
      "2021-06-26 18:36:03.457659: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:90] Reading meta graph with tags { serve }\n",
      "2021-06-26 18:36:03.457693: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /models/jax_model/1\n",
      "2021-06-26 18:36:03.457834: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-06-26 18:36:03.508701: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:206] Restoring SavedModel bundle.\n",
      "2021-06-26 18:36:03.509816: I external/org_tensorflow/tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2299995000 Hz\n",
      "2021-06-26 18:36:03.541002: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:190] Running initialization op on SavedModel bundle at path: /models/jax_model/1\n",
      "2021-06-26 18:36:03.546037: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:277] SavedModel load for tags { serve }; Status: success: OK. Took 89956 microseconds.\n",
      "2021-06-26 18:36:03.546686: I tensorflow_serving/servables/tensorflow/saved_model_warmup_util.cc:59] No warmup data file found at /models/jax_model/1/assets.extra/tf_serving_warmup_requests\n",
      "2021-06-26 18:36:03.547093: I tensorflow_serving/core/loader_harness.cc:87] Successfully loaded servable version {name: jax_model version: 1}\n",
      "2021-06-26 18:36:03.548040: I tensorflow_serving/model_servers/server_core.cc:486] Finished adding/updating models\n",
      "2021-06-26 18:36:03.548124: I tensorflow_serving/model_servers/server.cc:367] Profiler service is enabled\n",
      "2021-06-26 18:36:03.549536: I tensorflow_serving/model_servers/server.cc:393] Running gRPC ModelServer at 0.0.0.0:8500 ...\n",
      "[warn] getaddrinfo: address family for nodename not supported\n",
      "[evhttp_server.cc : 245] NET_LOG: Entering the event loop ...\n",
      "2021-06-26 18:36:03.551482: I tensorflow_serving/model_servers/server.cc:414] Exporting HTTP/REST API at:localhost:8501 ...\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "sleep 2\n",
    "docker logs serving_jax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ff2358",
   "metadata": {},
   "source": [
    "Send prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bae4ef5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-14.7594662, -0.0016374588, -10.8501253, -7.16693, -10.4939137, -11.1298847, -12.9479427, -10.0422325, -7.21408, -10.8261595]]\n"
     ]
    }
   ],
   "source": [
    "data = json.dumps({\"instances\": image_to_predict.numpy().tolist()})\n",
    "json_response = requests.post(f'http://localhost:8501/v1/models/{SERVING_MODELNAME}:predict', data=data)\n",
    "predictions = json.loads(json_response.text)['predictions']\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b5a9743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serving_jax\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker rm -f serving_jax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07040b5e",
   "metadata": {},
   "source": [
    "### Push image to registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa807403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The push refers to repository [gcr.io/dsparing-sandbox/jax_tfserving_image]\n",
      "74af849d2512: Preparing\n",
      "bb4423850a27: Preparing\n",
      "b60ba33781cd: Preparing\n",
      "547f89523b17: Preparing\n",
      "bd91f28d5f3c: Preparing\n",
      "8cafc6d2db45: Preparing\n",
      "a5d4bacb0351: Preparing\n",
      "5153e1acaabc: Preparing\n",
      "8cafc6d2db45: Waiting\n",
      "5153e1acaabc: Waiting\n",
      "a5d4bacb0351: Waiting\n",
      "bb4423850a27: Layer already exists\n",
      "547f89523b17: Layer already exists\n",
      "bd91f28d5f3c: Layer already exists\n",
      "b60ba33781cd: Layer already exists\n",
      "8cafc6d2db45: Layer already exists\n",
      "a5d4bacb0351: Layer already exists\n",
      "5153e1acaabc: Layer already exists\n",
      "74af849d2512: Pushed\n",
      "latest-cpu: digest: sha256:b7e799de9dd2f27f1bae320334dc87f5f3a5c095fdd1f0b2e838631e310f8776 size: 1991\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker push $IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c46b50a",
   "metadata": {},
   "source": [
    "### Upload prediction container to Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "213cecd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "REGION = \"us-central1\"\n",
    "\n",
    "MACHINE_TYPE=\"n1-standard-2\"\n",
    "\n",
    "if USE_GPU:\n",
    "    MODEL_DISPLAYNAME = f\"{SERVING_MODELNAME}-gpu\"\n",
    "    ACCELERATOR_TYPE = \"NVIDIA_TESLA_T4\"\n",
    "    ACCELERATOR_COUNT = 1\n",
    "else:\n",
    "    MODEL_DISPLAYNAME = f\"{SERVING_MODELNAME}-cpu\"\n",
    "    ACCELERATOR_TYPE = None\n",
    "    ACCELERATOR_COUNT = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f27ea1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a349076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax_model-cpu\n",
      "projects/654544512569/locations/us-central1/models/4069151796910620672\n"
     ]
    }
   ],
   "source": [
    "model = aiplatform.Model.upload(\n",
    "    display_name=MODEL_DISPLAYNAME,\n",
    "    serving_container_image_uri=IMAGE_URI,\n",
    "    serving_container_predict_route=f\"/v1/models/{SERVING_MODELNAME}:predict\",\n",
    "    serving_container_health_route=f\"/v1/models/{SERVING_MODELNAME}\",\n",
    "    serving_container_args=['--xla_cpu_compilation_enabled=true'],\n",
    "    serving_container_ports=[8501],\n",
    ")\n",
    "\n",
    "print(model.display_name)\n",
    "print(model.resource_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20447986",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = model.deploy(\n",
    "    machine_type=MACHINE_TYPE,\n",
    "    accelerator_type=ACCELERATOR_TYPE,\n",
    "    accelerator_count=ACCELERATOR_COUNT,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0863315",
   "metadata": {},
   "source": [
    "### Get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00a2025d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from absl import flags\n",
    "from jax.experimental.jax2tf.examples.mnist_lib import load_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b09c33ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "flags.FLAGS(['e']) # need to initialize flags somehow to avoid errors in load_mnist\n",
    "\n",
    "image_to_predict, _ = next(iter(load_mnist(tfds.Split.TEST, 1)))\n",
    "print(image_to_predict.shape)\n",
    "image_json = json.dumps(image_to_predict.numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8fc0cbfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-16.5650826, -28.286602, -15.1550636, -10.3459358, -9.69814873, -13.5413094, -23.0040817, -8.21421432, -10.2531109, -0.000401496887]]\n"
     ]
    }
   ],
   "source": [
    "instances = image_to_predict.numpy().tolist()\n",
    "prediction = endpoint.predict(instances)\n",
    "print(prediction.predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ffe228",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-5.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m74"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
