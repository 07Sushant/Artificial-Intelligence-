{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "266d9cc1",
   "metadata": {},
   "source": [
    "# Predict with JAX/Flax-trained SavedModel on Vertex AI custom container with TF Serving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319efcd2",
   "metadata": {},
   "source": [
    "runs on TF2.5 [Vertex Notebook](https://cloud.google.com/vertex-ai/docs/general/notebooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b6c56d",
   "metadata": {},
   "source": [
    "Vertex AI Prediction supports pre-built containers, with no additional customization such as args (\"Do not specify any other subfields of containerSpec\" [source](https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers#using_a_pre-built_container)), and custom containers. As we need the `--xla_cpu_compilation_enabled` arg, we can only use custom containers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b7b417",
   "metadata": {},
   "source": [
    "### Create TFServing container with SavedModel baked in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5190a929",
   "metadata": {},
   "source": [
    "NOTE: serving does not work for GPU yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9183793f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcr.io/dsparing-sandbox/jax_tfserving_image:latest-cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "SAVEDMODEL_DIR = \"gs://dsparing-sandbox-bucket/models/jax_model\"\n",
    "SERVING_MODELNAME = \"jax_model\"\n",
    "MODEL_VERSION = 1\n",
    "\n",
    "USE_GPU = False\n",
    "if USE_GPU:\n",
    "    IMAGE_TAG='latest-gpu'\n",
    "    TFSERVING_TAG = \"latest-gpu\"\n",
    "else:\n",
    "    IMAGE_TAG='latest-cpu'\n",
    "    TFSERVING_TAG = \"latest\"    \n",
    "\n",
    "IMAGE_NAME='jax_tfserving_image'\n",
    "IMAGE_URI = f'gcr.io/{PROJECT_ID}/{IMAGE_NAME}:{IMAGE_TAG}'\n",
    "print(IMAGE_URI)\n",
    "\n",
    "os.environ[\"SERVING_MODELNAME\"] = SERVING_MODELNAME\n",
    "os.environ[\"SAVEDMODEL_DIR\"] = SAVEDMODEL_DIR\n",
    "os.environ[\"MODEL_VERSION\"] = str(MODEL_VERSION)\n",
    "os.environ[\"IMAGE_URI\"] = IMAGE_URI\n",
    "os.environ[\"IMAGE_TAG\"] = IMAGE_TAG\n",
    "os.environ[\"TFSERVING_TAG\"] = TFSERVING_TAG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b042ad0a",
   "metadata": {},
   "source": [
    "Check that `SAVEDMODEL_DIR` actually contains a SavedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87e6d41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://dsparing-sandbox-bucket/models/jax_model/\n",
      "gs://dsparing-sandbox-bucket/models/jax_model/saved_model.pb\n",
      "gs://dsparing-sandbox-bucket/models/jax_model/assets/\n",
      "gs://dsparing-sandbox-bucket/models/jax_model/variables/\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gsutil ls $SAVEDMODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8369692",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p $SERVING_MODELNAME/$MODEL_VERSION # tf serving wants model versions in numbered directories.\n",
    "gsutil -q cp -r $SAVEDMODEL_DIR/* $SERVING_MODELNAME/$MODEL_VERSION/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b164e2",
   "metadata": {},
   "source": [
    "following [TF Serving tutorial](https://www.tensorflow.org/tfx/serving/docker#creating_your_own_serving_image) below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84f58dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71ec89fa52eb26147e52cd6781d2d4d35dd72c698fffd7792d6d62a38ec9827d\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker run -d --name serving_base tensorflow/serving:$TFSERVING_TAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3345e3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sha256:a6c04272045bf4441eaf54c1aa420c1977cfa731e1282b1f3b39ff2d4ca169db\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker cp $SERVING_MODELNAME serving_base:/models/$SERVING_MODELNAME\n",
    "docker commit --change \"ENV MODEL_NAME $SERVING_MODELNAME\" serving_base $IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6b305a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serving_base\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker rm -f serving_base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef15ba54",
   "metadata": {},
   "source": [
    "### Optional: Try serving locally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f90d78c",
   "metadata": {},
   "source": [
    "Get image to predict "
   ]
  },
  {
   "cell_type": "raw",
   "id": "081d2aaa",
   "metadata": {},
   "source": [
    "import json\n",
    "import requests\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from absl import flags\n",
    "from jax.experimental.jax2tf.examples.mnist_lib import load_mnist"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eadcc036",
   "metadata": {},
   "source": [
    "flags.FLAGS(['e']) # need to initialize flags somehow to avoid errors in load_mnist\n",
    "\n",
    "image_to_predict, _ = next(iter(load_mnist(tfds.Split.TEST, 1)))\n",
    "print(image_to_predict.shape)\n",
    "image_json = json.dumps(image_to_predict.numpy().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5653af2",
   "metadata": {},
   "source": [
    "Start up container"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c9698bdd",
   "metadata": {},
   "source": [
    "%%bash\n",
    "docker run -d -p 8501:8501 -e MODEL_NAME=$SERVING_MODELNAME --name serving_jax $IMAGE_URI --xla_cpu_compilation_enabled=true"
   ]
  },
  {
   "cell_type": "raw",
   "id": "95c0cd1b",
   "metadata": {},
   "source": [
    "%%bash\n",
    "sleep 2\n",
    "docker logs serving_jax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3e86f4",
   "metadata": {},
   "source": [
    "Send prediction"
   ]
  },
  {
   "cell_type": "raw",
   "id": "10d6d79d",
   "metadata": {},
   "source": [
    "data = json.dumps({\"instances\": image_to_predict.numpy().tolist()})\n",
    "json_response = requests.post(f'http://localhost:8501/v1/models/{SERVING_MODELNAME}:predict', data=data)\n",
    "predictions = json.loads(json_response.text)['predictions']\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "518e6de4",
   "metadata": {},
   "source": [
    "%%bash\n",
    "docker rm -f serving_jax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22120c61",
   "metadata": {},
   "source": [
    "### Push image to registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a37901c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The push refers to repository [gcr.io/dsparing-sandbox/jax_tfserving_image]\n",
      "c607ec907455: Preparing\n",
      "bb4423850a27: Preparing\n",
      "b60ba33781cd: Preparing\n",
      "547f89523b17: Preparing\n",
      "bd91f28d5f3c: Preparing\n",
      "8cafc6d2db45: Preparing\n",
      "a5d4bacb0351: Preparing\n",
      "5153e1acaabc: Preparing\n",
      "8cafc6d2db45: Waiting\n",
      "a5d4bacb0351: Waiting\n",
      "5153e1acaabc: Waiting\n",
      "bd91f28d5f3c: Layer already exists\n",
      "b60ba33781cd: Layer already exists\n",
      "bb4423850a27: Layer already exists\n",
      "547f89523b17: Layer already exists\n",
      "8cafc6d2db45: Layer already exists\n",
      "a5d4bacb0351: Layer already exists\n",
      "5153e1acaabc: Layer already exists\n",
      "c607ec907455: Pushed\n",
      "latest-cpu: digest: sha256:20d7802c6ea76f2660ca7456921d0e71867f46727aa42794e77d2c59c704e464 size: 1991\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker push $IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21074323",
   "metadata": {},
   "source": [
    "### Upload prediction container to Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3b38d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "REGION = \"us-central1\"\n",
    "\n",
    "MACHINE_TYPE=\"n1-standard-2\"\n",
    "\n",
    "if USE_GPU:\n",
    "    MODEL_DISPLAYNAME = f\"{SERVING_MODELNAME}-gpu\"\n",
    "    ACCELERATOR_TYPE = \"NVIDIA_TESLA_T4\"\n",
    "    ACCELERATOR_COUNT = 1\n",
    "else:\n",
    "    MODEL_DISPLAYNAME = f\"{SERVING_MODELNAME}-cpu\"\n",
    "    ACCELERATOR_TYPE = None\n",
    "    ACCELERATOR_COUNT = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4e6457f",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d6461f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Creating Model\n",
      "INFO:google.cloud.aiplatform.models:Create Model backing LRO: projects/654544512569/locations/us-central1/models/8700118851242688512/operations/3500014066926092288\n",
      "INFO:google.cloud.aiplatform.models:Model created. Resource name: projects/654544512569/locations/us-central1/models/8700118851242688512\n",
      "INFO:google.cloud.aiplatform.models:To use this Model in another session:\n",
      "INFO:google.cloud.aiplatform.models:model = aiplatform.Model('projects/654544512569/locations/us-central1/models/8700118851242688512')\n",
      "jax_model-cpu\n",
      "projects/654544512569/locations/us-central1/models/8700118851242688512\n"
     ]
    }
   ],
   "source": [
    "model = aiplatform.Model.upload(\n",
    "    display_name=MODEL_DISPLAYNAME,\n",
    "    serving_container_image_uri=IMAGE_URI,\n",
    "    serving_container_predict_route=f\"/v1/models/{SERVING_MODELNAME}:predict\",\n",
    "    serving_container_health_route=f\"/v1/models/{SERVING_MODELNAME}\",\n",
    "    serving_container_args=['--xla_cpu_compilation_enabled=true', '--runtime=nvidia'],\n",
    "    serving_container_ports=[8501],\n",
    ")\n",
    "\n",
    "print(model.display_name)\n",
    "print(model.resource_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13c49127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Creating Endpoint\n",
      "INFO:google.cloud.aiplatform.models:Create Endpoint backing LRO: projects/654544512569/locations/us-central1/endpoints/7109266263339171840/operations/1215000205988986880\n",
      "INFO:google.cloud.aiplatform.models:Endpoint created. Resource name: projects/654544512569/locations/us-central1/endpoints/7109266263339171840\n",
      "INFO:google.cloud.aiplatform.models:To use this Endpoint in another session:\n",
      "INFO:google.cloud.aiplatform.models:endpoint = aiplatform.Endpoint('projects/654544512569/locations/us-central1/endpoints/7109266263339171840')\n",
      "INFO:google.cloud.aiplatform.models:Deploying model to Endpoint : projects/654544512569/locations/us-central1/endpoints/7109266263339171840\n",
      "INFO:google.cloud.aiplatform.models:Deploy Endpoint model backing LRO: projects/654544512569/locations/us-central1/endpoints/7109266263339171840/operations/5826686224416374784\n",
      "INFO:google.cloud.aiplatform.models:Endpoint model deployed. Resource name: projects/654544512569/locations/us-central1/endpoints/7109266263339171840\n"
     ]
    }
   ],
   "source": [
    "endpoint = model.deploy(\n",
    "    machine_type=MACHINE_TYPE,\n",
    "    accelerator_type=ACCELERATOR_TYPE,\n",
    "    accelerator_count=ACCELERATOR_COUNT,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60750a0",
   "metadata": {},
   "source": [
    "### Get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05f7b84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from absl import flags\n",
    "from jax.experimental.jax2tf.examples.mnist_lib import load_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0df57731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:absl:Load dataset info from /home/jupyter/tensorflow_datasets/mnist/3.0.1\n",
      "INFO:absl:Field info.citation from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.splits from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.module_name from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Reusing dataset mnist (/home/jupyter/tensorflow_datasets/mnist/3.0.1)\n",
      "INFO:absl:Constructing tf.data.Dataset mnist for split test, from /home/jupyter/tensorflow_datasets/mnist/3.0.1\n",
      "(1, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "flags.FLAGS(['e']) # need to initialize flags somehow to avoid errors in load_mnist\n",
    "\n",
    "image_to_predict, _ = next(iter(load_mnist(tfds.Split.TEST, 1)))\n",
    "print(image_to_predict.shape)\n",
    "image_json = json.dumps(image_to_predict.numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ab34734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-10.2773552, -8.94001102, -8.18527, -6.2940197, -12.1361103, -4.93151569, -12.9197159, -11.7381096, -0.00995016098, -7.88032]]\n"
     ]
    }
   ],
   "source": [
    "instances = image_to_predict.numpy().tolist()\n",
    "prediction = endpoint.predict(instances)\n",
    "print(prediction.predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de5b923",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-5.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m74"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
