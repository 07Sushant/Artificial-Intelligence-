{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a3257a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582b592d",
   "metadata": {},
   "source": [
    "# Predict with JAX/Flax-trained SavedModel on Vertex AI custom container with TF Serving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0030e26d",
   "metadata": {},
   "source": [
    "Vertex AI Prediction supports pre-built containers, with no additional customization such as args (\"Do not specify any other subfields of containerSpec\" [source](https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers#using_a_pre-built_container)), and custom containers. As we need the `--xla_cpu_compilation_enabled` arg, we can only use custom containers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83158c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import requests\n",
    "import tensorflow_datasets as tfds\n",
    "from absl import flags\n",
    "from google.cloud import aiplatform\n",
    "from jax.experimental.jax2tf.examples.mnist_lib import load_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129bb8f6",
   "metadata": {},
   "source": [
    "## Create TFServing container with SavedModel baked in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f74572",
   "metadata": {},
   "source": [
    "NOTE: serving does not work for GPU yet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e60102f",
   "metadata": {},
   "source": [
    "Below, `SAVEDMODELDIR` should point to a model created in [training-prebuilt.ipynb](training-prebuilt.ipynb). Change it to any directory containing a SavedModel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47987ec8",
   "metadata": {
    "tags": [
     "flake8-noqa-line-1"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcr.io/dsparing-sandbox/jax_tfserving_image:latest-cpu\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "\n",
    "REGION = \"us-central1\"\n",
    "os.environ['REGION'] = REGION\n",
    "\n",
    "BUCKET_NAME = PROJECT_ID\n",
    "os.environ['BUCKET_NAME'] = BUCKET_NAME\n",
    "# Use a regional bucket in the above region you have rights to.\n",
    "# Create if needed:\n",
    "# !gsutil mb -l ${REGION} gs://${BUCKET_NAME}\n",
    "\n",
    "MODEL_NAME = \"jax_model_prebuilt\"\n",
    "\n",
    "SAVEDMODEL_DIR = (\"gs://{BUCKET_NAME}/models/\"\n",
    "                  f\"{MODEL_NAME}/output/model\")\n",
    "\n",
    "SERVING_MODELNAME = \"jax_model\"\n",
    "MODEL_VERSION = 1\n",
    "\n",
    "USE_GPU = False\n",
    "if USE_GPU:\n",
    "    IMAGE_TAG = \"latest-gpu\"\n",
    "    TFSERVING_TAG = \"latest-gpu\"\n",
    "else:\n",
    "    IMAGE_TAG = \"latest-cpu\"\n",
    "    TFSERVING_TAG = \"latest\"\n",
    "\n",
    "IMAGE_NAME = \"jax_tfserving_image\"\n",
    "IMAGE_URI = f\"gcr.io/{PROJECT_ID}/{IMAGE_NAME}:{IMAGE_TAG}\"\n",
    "print(IMAGE_URI)\n",
    "\n",
    "os.environ[\"SERVING_MODELNAME\"] = SERVING_MODELNAME\n",
    "os.environ[\"SAVEDMODEL_DIR\"] = SAVEDMODEL_DIR\n",
    "os.environ[\"MODEL_VERSION\"] = str(MODEL_VERSION)\n",
    "os.environ[\"IMAGE_URI\"] = IMAGE_URI\n",
    "os.environ[\"IMAGE_TAG\"] = IMAGE_TAG\n",
    "os.environ[\"TFSERVING_TAG\"] = TFSERVING_TAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f95f04b",
   "metadata": {},
   "source": [
    "Check that `SAVEDMODEL_DIR` actually contains a SavedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c663ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://dsparing-sandbox-bucket/models/jax_model_prebuilt/output/model/\n",
      "gs://dsparing-sandbox-bucket/models/jax_model_prebuilt/output/model/saved_model.pb\n",
      "gs://dsparing-sandbox-bucket/models/jax_model_prebuilt/output/model/assets/\n",
      "gs://dsparing-sandbox-bucket/models/jax_model_prebuilt/output/model/variables/\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gsutil ls $SAVEDMODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ed58a52",
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p $SERVING_MODELNAME/$MODEL_VERSION # tf serving wants model versions in numbered directories.\n",
    "gsutil -q cp -r $SAVEDMODEL_DIR/* $SERVING_MODELNAME/$MODEL_VERSION/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fca882",
   "metadata": {},
   "source": [
    "Below we follow the [TF Serving tutorial](https://www.tensorflow.org/tfx/serving/docker#creating_your_own_serving_image) for creating your own serving image with the model baked in. We spin up a TF Serving container, copy the model inside the container, and commit this change together with the `MODEL_NAME` environment variable. After the commit, the base TF Serving container can be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6181edf",
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37b012feee8e6a6b16923d27dcabe4491d8c97210ff52c097d38d36fe8fbb1eb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unable to find image 'tensorflow/serving:latest' locally\n",
      "latest: Pulling from tensorflow/serving\n",
      "01bf7da0a88c: Pulling fs layer\n",
      "f3b4a5f15c7a: Pulling fs layer\n",
      "57ffbe87baa1: Pulling fs layer\n",
      "e72e6208e893: Pulling fs layer\n",
      "6ea3f464ef73: Pulling fs layer\n",
      "01e9bf86544b: Pulling fs layer\n",
      "68f6bba3dc50: Pulling fs layer\n",
      "6ea3f464ef73: Waiting\n",
      "01e9bf86544b: Waiting\n",
      "68f6bba3dc50: Waiting\n",
      "e72e6208e893: Waiting\n",
      "57ffbe87baa1: Verifying Checksum\n",
      "57ffbe87baa1: Download complete\n",
      "f3b4a5f15c7a: Verifying Checksum\n",
      "f3b4a5f15c7a: Download complete\n",
      "01bf7da0a88c: Verifying Checksum\n",
      "01bf7da0a88c: Download complete\n",
      "e72e6208e893: Download complete\n",
      "01e9bf86544b: Download complete\n",
      "68f6bba3dc50: Verifying Checksum\n",
      "68f6bba3dc50: Download complete\n",
      "01bf7da0a88c: Pull complete\n",
      "6ea3f464ef73: Verifying Checksum\n",
      "6ea3f464ef73: Download complete\n",
      "f3b4a5f15c7a: Pull complete\n",
      "57ffbe87baa1: Pull complete\n",
      "e72e6208e893: Pull complete\n",
      "6ea3f464ef73: Pull complete\n",
      "01e9bf86544b: Pull complete\n",
      "68f6bba3dc50: Pull complete\n",
      "Digest: sha256:6651f4839e1124dbde75ee531825112af0a6b8ef082c88ab14ca53eb69a2e4bb\n",
      "Status: Downloaded newer image for tensorflow/serving:latest\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker run -d --name serving_base tensorflow/serving:$TFSERVING_TAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc1eb104",
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sha256:8d3310d6efe401cb35b065f2d44329b85b529ba050065fa5f0b1d1f00ab659ac\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker cp $SERVING_MODELNAME serving_base:/models/$SERVING_MODELNAME\n",
    "docker commit --change \"ENV MODEL_NAME $SERVING_MODELNAME\" serving_base $IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ec3985d",
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serving_base\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker rm -f serving_base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f69de75",
   "metadata": {},
   "source": [
    "## Try serving locally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b32474",
   "metadata": {},
   "source": [
    "Get image to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "583227ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:absl:Load dataset info from /home/jupyter/tensorflow_datasets/mnist/3.0.1\n",
      "INFO:absl:Field info.citation from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.splits from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.module_name from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Reusing dataset mnist (/home/jupyter/tensorflow_datasets/mnist/3.0.1)\n",
      "INFO:absl:Constructing tf.data.Dataset mnist for split test, from /home/jupyter/tensorflow_datasets/mnist/3.0.1\n"
     ]
    }
   ],
   "source": [
    "# need to initialize flags somehow to avoid errors in load_mnist\n",
    "flags.FLAGS(['e'])\n",
    "\n",
    "image_to_predict, _ = next(iter(load_mnist(tfds.Split.TEST, batch_size=1)))\n",
    "instances = image_to_predict.numpy().tolist()\n",
    "image_json = json.dumps(instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32145a41",
   "metadata": {},
   "source": [
    "Start up container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b8cd2f2",
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86618ca15a6ddf2df432024af8ff5a2c85f3cfde782c4e00ed7e87a0f0026b9c\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker run -d -p 8501:8501 -e MODEL_NAME=$SERVING_MODELNAME --name serving_jax $IMAGE_URI --xla_cpu_compilation_enabled=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39d23bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-26 23:00:36.960212: I tensorflow_serving/model_servers/server.cc:89] Building single TensorFlow model file config:  model_name: jax_model model_base_path: /models/jax_model\n",
      "2021-06-26 23:00:36.960474: I tensorflow_serving/model_servers/server_core.cc:465] Adding/updating models.\n",
      "2021-06-26 23:00:36.960496: I tensorflow_serving/model_servers/server_core.cc:591]  (Re-)adding model: jax_model\n",
      "2021-06-26 23:00:37.060946: I tensorflow_serving/core/basic_manager.cc:740] Successfully reserved resources to load servable {name: jax_model version: 1}\n",
      "2021-06-26 23:00:37.060992: I tensorflow_serving/core/loader_harness.cc:66] Approving load for servable version {name: jax_model version: 1}\n",
      "2021-06-26 23:00:37.061009: I tensorflow_serving/core/loader_harness.cc:74] Loading servable version {name: jax_model version: 1}\n",
      "2021-06-26 23:00:37.061067: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:38] Reading SavedModel from: /models/jax_model/1\n",
      "2021-06-26 23:00:37.062541: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:90] Reading meta graph with tags { serve }\n",
      "2021-06-26 23:00:37.062597: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /models/jax_model/1\n",
      "2021-06-26 23:00:37.062690: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-06-26 23:00:37.098856: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:206] Restoring SavedModel bundle.\n",
      "2021-06-26 23:00:37.099885: I external/org_tensorflow/tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2299995000 Hz\n",
      "2021-06-26 23:00:37.121543: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:190] Running initialization op on SavedModel bundle at path: /models/jax_model/1\n",
      "2021-06-26 23:00:37.126306: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:277] SavedModel load for tags { serve }; Status: success: OK. Took 65236 microseconds.\n",
      "2021-06-26 23:00:37.126822: I tensorflow_serving/servables/tensorflow/saved_model_warmup_util.cc:59] No warmup data file found at /models/jax_model/1/assets.extra/tf_serving_warmup_requests\n",
      "2021-06-26 23:00:37.127134: I tensorflow_serving/core/loader_harness.cc:87] Successfully loaded servable version {name: jax_model version: 1}\n",
      "2021-06-26 23:00:37.127936: I tensorflow_serving/model_servers/server_core.cc:486] Finished adding/updating models\n",
      "2021-06-26 23:00:37.128003: I tensorflow_serving/model_servers/server.cc:367] Profiler service is enabled\n",
      "2021-06-26 23:00:37.129142: I tensorflow_serving/model_servers/server.cc:393] Running gRPC ModelServer at 0.0.0.0:8500 ...\n",
      "[warn] getaddrinfo: address family for nodename not supported\n",
      "2021-06-26 23:00:37.131057: I tensorflow_serving/model_servers/server.cc:414] Exporting HTTP/REST API at:localhost:8501 ...\n",
      "[evhttp_server.cc : 245] NET_LOG: Entering the event loop ...\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "sleep 2\n",
    "docker logs serving_jax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf20c26",
   "metadata": {},
   "source": [
    "Send prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20849e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-5.36204624, -4.91872883, -4.52080679, -0.384578705, -12.0657101, -1.39947963, -5.91683674, -9.53136826, -3.0643661, -8.50711155]]\n"
     ]
    }
   ],
   "source": [
    "data = json.dumps({\"instances\": image_to_predict.numpy().tolist()})\n",
    "json_response = requests.post(\n",
    "    f'http://localhost:8501/v1/models/{SERVING_MODELNAME}:predict',\n",
    "    data=data,\n",
    ")\n",
    "predictions = json.loads(json_response.text)['predictions']\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8db54082",
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serving_jax\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker rm -f serving_jax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31808c3e",
   "metadata": {},
   "source": [
    "## Push image to Container Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4409e141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The push refers to repository [gcr.io/dsparing-sandbox/jax_tfserving_image]\n",
      "5a3a97ca4978: Preparing\n",
      "bb4423850a27: Preparing\n",
      "b60ba33781cd: Preparing\n",
      "547f89523b17: Preparing\n",
      "bd91f28d5f3c: Preparing\n",
      "8cafc6d2db45: Preparing\n",
      "a5d4bacb0351: Preparing\n",
      "5153e1acaabc: Preparing\n",
      "a5d4bacb0351: Waiting\n",
      "5153e1acaabc: Waiting\n",
      "8cafc6d2db45: Waiting\n",
      "bd91f28d5f3c: Layer already exists\n",
      "547f89523b17: Layer already exists\n",
      "b60ba33781cd: Layer already exists\n",
      "bb4423850a27: Layer already exists\n",
      "8cafc6d2db45: Layer already exists\n",
      "a5d4bacb0351: Layer already exists\n",
      "5153e1acaabc: Layer already exists\n",
      "5a3a97ca4978: Pushed\n",
      "latest-cpu: digest: sha256:5469c3038606bbcb54d7f2bb384b33bb03e3c3d33da32fd5c1a81864fd6222d9 size: 1991\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker push $IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682443b8",
   "metadata": {},
   "source": [
    "## Upload prediction container to Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65e573a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = \"us-central1\"\n",
    "\n",
    "MACHINE_TYPE = \"n1-standard-2\"\n",
    "\n",
    "if USE_GPU:\n",
    "    MODEL_DISPLAYNAME = f\"{SERVING_MODELNAME}-gpu\"\n",
    "    ACCELERATOR_TYPE = \"NVIDIA_TESLA_T4\"\n",
    "    ACCELERATOR_COUNT = 1\n",
    "else:\n",
    "    MODEL_DISPLAYNAME = f\"{SERVING_MODELNAME}-cpu\"\n",
    "    ACCELERATOR_TYPE = None\n",
    "    ACCELERATOR_COUNT = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7baeadb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Creating Model\n",
      "INFO:google.cloud.aiplatform.models:Create Model backing LRO: projects/654544512569/locations/us-central1/models/501174992126345216/operations/4723293320594325504\n",
      "INFO:google.cloud.aiplatform.models:Model created. Resource name: projects/654544512569/locations/us-central1/models/501174992126345216\n",
      "INFO:google.cloud.aiplatform.models:To use this Model in another session:\n",
      "INFO:google.cloud.aiplatform.models:model = aiplatform.Model('projects/654544512569/locations/us-central1/models/501174992126345216')\n",
      "jax_model-cpu\n",
      "projects/654544512569/locations/us-central1/models/501174992126345216\n"
     ]
    }
   ],
   "source": [
    "model = aiplatform.Model.upload(\n",
    "    display_name=MODEL_DISPLAYNAME,\n",
    "    serving_container_image_uri=IMAGE_URI,\n",
    "    serving_container_predict_route=f\"/v1/models/{SERVING_MODELNAME}:predict\",\n",
    "    serving_container_health_route=f\"/v1/models/{SERVING_MODELNAME}\",\n",
    "    serving_container_args=['--xla_cpu_compilation_enabled=true'],\n",
    "    serving_container_ports=[8501],\n",
    ")\n",
    "\n",
    "print(model.display_name)\n",
    "print(model.resource_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d084d022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Creating Endpoint\n",
      "INFO:google.cloud.aiplatform.models:Create Endpoint backing LRO: projects/654544512569/locations/us-central1/endpoints/366814671212118016/operations/573226263972413440\n",
      "INFO:google.cloud.aiplatform.models:Endpoint created. Resource name: projects/654544512569/locations/us-central1/endpoints/366814671212118016\n",
      "INFO:google.cloud.aiplatform.models:To use this Endpoint in another session:\n",
      "INFO:google.cloud.aiplatform.models:endpoint = aiplatform.Endpoint('projects/654544512569/locations/us-central1/endpoints/366814671212118016')\n",
      "INFO:google.cloud.aiplatform.models:Deploying model to Endpoint : projects/654544512569/locations/us-central1/endpoints/366814671212118016\n",
      "INFO:google.cloud.aiplatform.models:Deploy Endpoint model backing LRO: projects/654544512569/locations/us-central1/endpoints/366814671212118016/operations/3536594818782199808\n",
      "INFO:google.cloud.aiplatform.models:Endpoint model deployed. Resource name: projects/654544512569/locations/us-central1/endpoints/366814671212118016\n"
     ]
    }
   ],
   "source": [
    "endpoint = model.deploy(\n",
    "    machine_type=MACHINE_TYPE,\n",
    "    accelerator_type=ACCELERATOR_TYPE,\n",
    "    accelerator_count=ACCELERATOR_COUNT,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add4d7fe",
   "metadata": {},
   "source": [
    "## Get Online Predictions from Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d4019d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-5.36204624, -4.91872883, -4.52080679, -0.384578705, -12.0657101, -1.39947963, -5.91683674, -9.53136826, -3.0643661, -8.50711155]]\n"
     ]
    }
   ],
   "source": [
    "prediction = endpoint.predict(instances)\n",
    "print(prediction.predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfd5758",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-5.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m74"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
