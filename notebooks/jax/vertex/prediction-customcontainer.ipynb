{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bd9a737",
   "metadata": {},
   "source": [
    "# Predict with JAX/Flax-trained SavedModel on Vertex AI custom container with TF Serving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569082a3",
   "metadata": {},
   "source": [
    "Vertex AI Prediction supports pre-built containers, with no additional customization such as args (\"Do not specify any other subfields of containerSpec\" [source](https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers#using_a_pre-built_container)), and custom containers. As we need the `--xla_cpu_compilation_enabled` arg, we can only use custom containers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "853a4848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import requests\n",
    "import tensorflow_datasets as tfds\n",
    "from absl import flags\n",
    "from google.cloud import aiplatform\n",
    "from jax.experimental.jax2tf.examples.mnist_lib import load_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1bb878",
   "metadata": {},
   "source": [
    "## Create TFServing container with SavedModel baked in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee9c880",
   "metadata": {},
   "source": [
    "NOTE: serving does not work for GPU yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e221ca40",
   "metadata": {
    "tags": [
     "flake8-noqa-line-1"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcr.io/dsparing-sandbox/jax_tfserving_image:latest-cpu\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "SAVEDMODEL_DIR = (\"gs://dsparing-sandbox-bucket/models/\"\n",
    "                  \"jax_model_prebuilt/output/model\")\n",
    "SERVING_MODELNAME = \"jax_model\"\n",
    "MODEL_VERSION = 1\n",
    "\n",
    "USE_GPU = False\n",
    "if USE_GPU:\n",
    "    IMAGE_TAG = \"latest-gpu\"\n",
    "    TFSERVING_TAG = \"latest-gpu\"\n",
    "else:\n",
    "    IMAGE_TAG = \"latest-cpu\"\n",
    "    TFSERVING_TAG = \"latest\"\n",
    "\n",
    "IMAGE_NAME = \"jax_tfserving_image\"\n",
    "IMAGE_URI = f\"gcr.io/{PROJECT_ID}/{IMAGE_NAME}:{IMAGE_TAG}\"\n",
    "print(IMAGE_URI)\n",
    "\n",
    "os.environ[\"SERVING_MODELNAME\"] = SERVING_MODELNAME\n",
    "os.environ[\"SAVEDMODEL_DIR\"] = SAVEDMODEL_DIR\n",
    "os.environ[\"MODEL_VERSION\"] = str(MODEL_VERSION)\n",
    "os.environ[\"IMAGE_URI\"] = IMAGE_URI\n",
    "os.environ[\"IMAGE_TAG\"] = IMAGE_TAG\n",
    "os.environ[\"TFSERVING_TAG\"] = TFSERVING_TAG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8123b12",
   "metadata": {},
   "source": [
    "Check that `SAVEDMODEL_DIR` actually contains a SavedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abd8d871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://dsparing-sandbox-bucket/models/jax_model_prebuilt/output/model/\n",
      "gs://dsparing-sandbox-bucket/models/jax_model_prebuilt/output/model/saved_model.pb\n",
      "gs://dsparing-sandbox-bucket/models/jax_model_prebuilt/output/model/assets/\n",
      "gs://dsparing-sandbox-bucket/models/jax_model_prebuilt/output/model/variables/\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gsutil ls $SAVEDMODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "158efe53",
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p $SERVING_MODELNAME/$MODEL_VERSION # tf serving wants model versions in numbered directories.\n",
    "gsutil -q cp -r $SAVEDMODEL_DIR/* $SERVING_MODELNAME/$MODEL_VERSION/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22d0908",
   "metadata": {},
   "source": [
    "following [TF Serving tutorial](https://www.tensorflow.org/tfx/serving/docker#creating_your_own_serving_image) below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9e27735",
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4d92b24d393e3559bdf3fcb669145ed049430837892f6562100cbcd5c5bef563\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker run -d --name serving_base tensorflow/serving:$TFSERVING_TAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "935d6060",
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sha256:65a3cabbd278f35a19f0be65447ee808fbc23eeb996f62aa7e00818009a61245\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker cp $SERVING_MODELNAME serving_base:/models/$SERVING_MODELNAME\n",
    "docker commit --change \"ENV MODEL_NAME $SERVING_MODELNAME\" serving_base $IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20bcd4b4",
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serving_base\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker rm -f serving_base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923c4810",
   "metadata": {},
   "source": [
    "## Try serving locally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a30d927",
   "metadata": {},
   "source": [
    "Get image to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e6a0884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:absl:Load dataset info from /home/jupyter/tensorflow_datasets/mnist/3.0.1\n",
      "INFO:absl:Field info.citation from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.splits from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.module_name from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Reusing dataset mnist (/home/jupyter/tensorflow_datasets/mnist/3.0.1)\n",
      "INFO:absl:Constructing tf.data.Dataset mnist for split test, from /home/jupyter/tensorflow_datasets/mnist/3.0.1\n"
     ]
    }
   ],
   "source": [
    "# need to initialize flags somehow to avoid errors in load_mnist\n",
    "flags.FLAGS(['e'])\n",
    "\n",
    "image_to_predict, _ = next(iter(load_mnist(tfds.Split.TEST, batch_size=1)))\n",
    "instances = image_to_predict.numpy().tolist()\n",
    "image_json = json.dumps(instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e51c61d",
   "metadata": {},
   "source": [
    "Start up container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a2bce3a",
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d580fb0664eecbd54ecc4e23282b51940f3df052634d03d80bf1ca16484274a5\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker run -d -p 8501:8501 -e MODEL_NAME=$SERVING_MODELNAME --name serving_jax $IMAGE_URI --xla_cpu_compilation_enabled=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f001ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-26 20:23:36.608829: I tensorflow_serving/model_servers/server.cc:89] Building single TensorFlow model file config:  model_name: jax_model model_base_path: /models/jax_model\n",
      "2021-06-26 20:23:36.609255: I tensorflow_serving/model_servers/server_core.cc:465] Adding/updating models.\n",
      "2021-06-26 20:23:36.609286: I tensorflow_serving/model_servers/server_core.cc:591]  (Re-)adding model: jax_model\n",
      "2021-06-26 20:23:36.709703: I tensorflow_serving/core/basic_manager.cc:740] Successfully reserved resources to load servable {name: jax_model version: 1}\n",
      "2021-06-26 20:23:36.709743: I tensorflow_serving/core/loader_harness.cc:66] Approving load for servable version {name: jax_model version: 1}\n",
      "2021-06-26 20:23:36.709757: I tensorflow_serving/core/loader_harness.cc:74] Loading servable version {name: jax_model version: 1}\n",
      "2021-06-26 20:23:36.709812: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:38] Reading SavedModel from: /models/jax_model/1\n",
      "2021-06-26 20:23:36.711456: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:90] Reading meta graph with tags { serve }\n",
      "2021-06-26 20:23:36.711520: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /models/jax_model/1\n",
      "2021-06-26 20:23:36.711630: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-06-26 20:23:36.763565: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:206] Restoring SavedModel bundle.\n",
      "2021-06-26 20:23:36.764715: I external/org_tensorflow/tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2299995000 Hz\n",
      "2021-06-26 20:23:36.795606: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:190] Running initialization op on SavedModel bundle at path: /models/jax_model/1\n",
      "2021-06-26 20:23:36.802788: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:277] SavedModel load for tags { serve }; Status: success: OK. Took 92974 microseconds.\n",
      "2021-06-26 20:23:36.803413: I tensorflow_serving/servables/tensorflow/saved_model_warmup_util.cc:59] No warmup data file found at /models/jax_model/1/assets.extra/tf_serving_warmup_requests\n",
      "2021-06-26 20:23:36.803863: I tensorflow_serving/core/loader_harness.cc:87] Successfully loaded servable version {name: jax_model version: 1}\n",
      "2021-06-26 20:23:36.804870: I tensorflow_serving/model_servers/server_core.cc:486] Finished adding/updating models\n",
      "2021-06-26 20:23:36.804984: I tensorflow_serving/model_servers/server.cc:367] Profiler service is enabled\n",
      "2021-06-26 20:23:36.806879: I tensorflow_serving/model_servers/server.cc:393] Running gRPC ModelServer at 0.0.0.0:8500 ...\n",
      "[warn] getaddrinfo: address family for nodename not supported\n",
      "2021-06-26 20:23:36.809154: I tensorflow_serving/model_servers/server.cc:414] Exporting HTTP/REST API at:localhost:8501 ...\n",
      "[evhttp_server.cc : 245] NET_LOG: Entering the event loop ...\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "sleep 2\n",
    "docker logs serving_jax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9cae73",
   "metadata": {},
   "source": [
    "Send prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55a97bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-12.7403393, -3.94832897, -6.22885, -0.0298643112, -13.4114065, -7.25624228, -17.2837029, -8.350564, -5.00295305, -7.60113287]]\n"
     ]
    }
   ],
   "source": [
    "data = json.dumps({\"instances\": image_to_predict.numpy().tolist()})\n",
    "json_response = requests.post(\n",
    "    f'http://localhost:8501/v1/models/{SERVING_MODELNAME}:predict',\n",
    "    data=data,\n",
    ")\n",
    "predictions = json.loads(json_response.text)['predictions']\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfe82149",
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serving_jax\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker rm -f serving_jax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d9a25e",
   "metadata": {},
   "source": [
    "## Push image to Container Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c1bef6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The push refers to repository [gcr.io/dsparing-sandbox/jax_tfserving_image]\n",
      "33caea58d11f: Preparing\n",
      "bb4423850a27: Preparing\n",
      "b60ba33781cd: Preparing\n",
      "547f89523b17: Preparing\n",
      "bd91f28d5f3c: Preparing\n",
      "8cafc6d2db45: Preparing\n",
      "a5d4bacb0351: Preparing\n",
      "5153e1acaabc: Preparing\n",
      "8cafc6d2db45: Waiting\n",
      "a5d4bacb0351: Waiting\n",
      "5153e1acaabc: Waiting\n",
      "bd91f28d5f3c: Layer already exists\n",
      "b60ba33781cd: Layer already exists\n",
      "bb4423850a27: Layer already exists\n",
      "547f89523b17: Layer already exists\n",
      "5153e1acaabc: Layer already exists\n",
      "8cafc6d2db45: Layer already exists\n",
      "a5d4bacb0351: Layer already exists\n",
      "33caea58d11f: Pushed\n",
      "latest-cpu: digest: sha256:53cf8d3dbf24827136bba0b077edbf46ba0591d28c5f690b7c60bc43d051466b size: 1991\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker push $IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eef3976",
   "metadata": {},
   "source": [
    "## Upload prediction container to Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fa6ca6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = \"us-central1\"\n",
    "\n",
    "MACHINE_TYPE = \"n1-standard-2\"\n",
    "\n",
    "if USE_GPU:\n",
    "    MODEL_DISPLAYNAME = f\"{SERVING_MODELNAME}-gpu\"\n",
    "    ACCELERATOR_TYPE = \"NVIDIA_TESLA_T4\"\n",
    "    ACCELERATOR_COUNT = 1\n",
    "else:\n",
    "    MODEL_DISPLAYNAME = f\"{SERVING_MODELNAME}-cpu\"\n",
    "    ACCELERATOR_TYPE = None\n",
    "    ACCELERATOR_COUNT = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c9f4e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0fbc9ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Creating Model\n",
      "INFO:google.cloud.aiplatform.models:Create Model backing LRO: projects/654544512569/locations/us-central1/models/5315522993785405440/operations/8645506383568961536\n",
      "INFO:google.cloud.aiplatform.models:Model created. Resource name: projects/654544512569/locations/us-central1/models/5315522993785405440\n",
      "INFO:google.cloud.aiplatform.models:To use this Model in another session:\n",
      "INFO:google.cloud.aiplatform.models:model = aiplatform.Model('projects/654544512569/locations/us-central1/models/5315522993785405440')\n",
      "jax_model-cpu\n",
      "projects/654544512569/locations/us-central1/models/5315522993785405440\n"
     ]
    }
   ],
   "source": [
    "model = aiplatform.Model.upload(\n",
    "    display_name=MODEL_DISPLAYNAME,\n",
    "    serving_container_image_uri=IMAGE_URI,\n",
    "    serving_container_predict_route=f\"/v1/models/{SERVING_MODELNAME}:predict\",\n",
    "    serving_container_health_route=f\"/v1/models/{SERVING_MODELNAME}\",\n",
    "    serving_container_args=['--xla_cpu_compilation_enabled=true'],\n",
    "    serving_container_ports=[8501],\n",
    ")\n",
    "\n",
    "print(model.display_name)\n",
    "print(model.resource_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "816d15cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Creating Endpoint\n",
      "INFO:google.cloud.aiplatform.models:Create Endpoint backing LRO: projects/654544512569/locations/us-central1/endpoints/2382175504460414976/operations/7073750113616658432\n",
      "INFO:google.cloud.aiplatform.models:Endpoint created. Resource name: projects/654544512569/locations/us-central1/endpoints/2382175504460414976\n",
      "INFO:google.cloud.aiplatform.models:To use this Endpoint in another session:\n",
      "INFO:google.cloud.aiplatform.models:endpoint = aiplatform.Endpoint('projects/654544512569/locations/us-central1/endpoints/2382175504460414976')\n",
      "INFO:google.cloud.aiplatform.models:Deploying model to Endpoint : projects/654544512569/locations/us-central1/endpoints/2382175504460414976\n",
      "INFO:google.cloud.aiplatform.models:Deploy Endpoint model backing LRO: projects/654544512569/locations/us-central1/endpoints/2382175504460414976/operations/5712537126243926016\n",
      "INFO:google.cloud.aiplatform.models:Endpoint model deployed. Resource name: projects/654544512569/locations/us-central1/endpoints/2382175504460414976\n"
     ]
    }
   ],
   "source": [
    "endpoint = model.deploy(\n",
    "    machine_type=MACHINE_TYPE,\n",
    "    accelerator_type=ACCELERATOR_TYPE,\n",
    "    accelerator_count=ACCELERATOR_COUNT,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101fab58",
   "metadata": {},
   "source": [
    "## Get Online Predictions from Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7beb6d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-18.9266491, -21.4380112, -11.6358833, -1.71661377e-05, -25.4557629, -12.2982159, -26.5184898, -27.9774284, -12.2741051, -19.7286148]]\n"
     ]
    }
   ],
   "source": [
    "prediction = endpoint.predict(instances)\n",
    "print(prediction.predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3fada3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-5.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m74"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
